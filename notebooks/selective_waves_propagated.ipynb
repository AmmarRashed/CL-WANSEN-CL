{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle, pandas as pd, re, numpy as np, ast, warnings\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import chain, starmap\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>i love science fiction and i hate superheroes ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>the movie is absolutely incredible all the per...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>in a cinematic era dominated by reboots and mi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>movie review on rise of the planet of the apes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>during experiments to find a cure for alzheime...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language    Movie_ID                                             Review  \\\n",
       "0       en  -800777728  i love science fiction and i hate superheroes ...   \n",
       "1       en  -800777728  the movie is absolutely incredible all the per...   \n",
       "2       en -1018312192  in a cinematic era dominated by reboots and mi...   \n",
       "3       en -1018312192  movie review on rise of the planet of the apes...   \n",
       "4       en -1018312192  during experiments to find a cure for alzheime...   \n",
       "\n",
       "   Score  \n",
       "0      9  \n",
       "1     10  \n",
       "2      8  \n",
       "3      4  \n",
       "4      7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/movie_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language  Movie_ID  Review\n",
       "Score                            \n",
       "1            29        29      29\n",
       "2            21        21      21\n",
       "3            14        14      14\n",
       "4            23        23      23\n",
       "5            83        83      83\n",
       "6            43        43      43\n",
       "7            71        71      71\n",
       "8           207       207     207\n",
       "9           175       175     175\n",
       "10          334       334     334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Score\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../NLP_data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../NLP_data/wiki.tr/wiki.tr.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_stemmer = TurkishStemmer()\n",
    "def clean(text, language=\"en\", stem=True):\n",
    "    global turkish_stemmer\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "    \n",
    "    if language == \"tr\":\n",
    "        if stem:\n",
    "            text= ' '.join([turkish_stemmer.stem(w) for w in text.split()])\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r'[0-9]', '#', text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    return TextBlob(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 300\n",
    "def vectorize(text, language):\n",
    "    global VECTOR_SIZE            \n",
    "    blob = clean(text, language)\n",
    "    vector = np.zeros(VECTOR_SIZE)\n",
    "    if len(blob.words) < 1:\n",
    "        return None\n",
    "\n",
    "    for word in blob.words:\n",
    "        try:\n",
    "            if language == \"en\":\n",
    "                vector += globals()[\"en_vects\"][word]\n",
    "            else:\n",
    "                vector += globals()[\"tr_vects\"][word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vector /= len(blob.words)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvec(x):\n",
    "    lang, rev = x.split(\":::::\")\n",
    "    return vectorize(rev, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMSR\n",
    "def preprocess_data(df, language_column=\"Language\", review_column=\"Review\"):\n",
    "    LMSR_df = df.copy()\n",
    "    LMSR_df[\"lang_rev\"] = LMSR_df[[language_column, review_column]].apply(lambda x: x[0]+\":::::\"+x[1], axis=1)\n",
    "    LMSR_df[\"rev_vec\"] = LMSR_df[\"lang_rev\"].apply(lambda x:getvec(x))\n",
    "    LMSR_df.drop([\"lang_rev\", \"Review\"], axis=1, inplace=True)\n",
    "    return LMSR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_accuracy(y_true, y_predict):\n",
    "    res = 0\n",
    "    for i in range(len(y_true)):\n",
    "        res += abs(y_true[i]-y_predict[i])\n",
    "    return 1-res/(len(y_true)*len(set(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XYy(LMSR):\n",
    "    X = np.zeros((len(LMSR), VECTOR_SIZE))\n",
    "    Y = np.zeros((len(LMSR), VECTOR_SIZE))\n",
    "    y = np.zeros((len(LMSR)))\n",
    "    i = 0\n",
    "    for rev in LMSR.iterrows():\n",
    "        score = rev[1][2]\n",
    "        rev_vec = rev[1][3]\n",
    "        score_vec = rev[1][4]\n",
    "\n",
    "        X[i] = rev_vec\n",
    "        Y[i] = score_vec\n",
    "        y[i] = score\n",
    "\n",
    "        i += 1\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derive=False):\n",
    "    if derive:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(df, get_L2and3=False):\n",
    "    data_dict = dict() #{language:{score: {movie_id: [rev1, rev2, ..., revn]}}}\n",
    "    L1 = dict()  # {(languge, score, movie_id): list of reviews with the same score with the same language}\n",
    "    L2 = dict()  # {(language, score): None}\n",
    "    L3 = dict()  # {score: None}\n",
    "    for _, row in df.iterrows():\n",
    "        lang = row[\"Language\"]\n",
    "        movie_id = row[\"Movie_ID\"]\n",
    "        score = row[\"Score\"]\n",
    "        review = row[\"rev_vec\"]\n",
    "\n",
    "        data_dict.setdefault(lang, {})\n",
    "        data_dict[lang].setdefault(score, {})\n",
    "        data_dict[lang][score].setdefault(movie_id, [])\n",
    "        data_dict[lang][score][movie_id].append(review)\n",
    "        \n",
    "        L1.setdefault((lang, score, movie_id), list())\n",
    "        L1[(lang, score, movie_id)].append(review)\n",
    "        if get_L2and3:    \n",
    "            L2[(lang, score)] = None\n",
    "            L3[score] = None\n",
    "    if get_L2and3:\n",
    "        return data_dict, L1, L2, L3\n",
    "    return data_dict, L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L2(LSM_R, data_dict):\n",
    "    L2 = dict()  # {(language, score): list of movies vectors}\n",
    "    for language in data_dict:\n",
    "        for score in data_dict[language]:\n",
    "            for movie_id in data_dict[language][score]:\n",
    "                L2.setdefault((language, score), list())\n",
    "                L2[(language, score)].append(LSM_R[(language, score, movie_id)])\n",
    "    return L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L3(LS_MR, data_dict):\n",
    "    L3 = dict()  # {score: vector of merged languages for that score}\n",
    "    for language in data_dict:\n",
    "        for score in data_dict[language]:\n",
    "            L3.setdefault(score, list())\n",
    "            L3[score].append(LS_MR[(language, score)])\n",
    "    return L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(L, W):\n",
    "    merged = dict()  # {item: vector of merged subitems}\n",
    "    for i, item in enumerate(sorted(L)):\n",
    "        for subitem in L[item]:\n",
    "            merged.setdefault(item, [np.zeros(VECTOR_SIZE),0])\n",
    "            merged[item][0] += sigmoid(subitem.dot(W[i]))\n",
    "            merged[item][1] += 1\n",
    "    for item in merged:\n",
    "        merged[item] = merged[item][0]/ merged[item][1]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(L, delta, W, alpha=0.1):\n",
    "    for i, k in enumerate(sorted(L)):\n",
    "        for l in L[k]:\n",
    "            W[i] += l.T.dot(delta[i]) *alpha\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_error(delta, W):\n",
    "    error = 0\n",
    "    for i in range(len(delta)):\n",
    "        error += delta[i].dot(W[i].T)\n",
    "    return error/len(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_delta(error, layer, size):\n",
    "    delta = np.zeros((size, VECTOR_SIZE))\n",
    "    j = 0\n",
    "    for i,k in enumerate(sorted(layer)):\n",
    "        for l in layer[k]:\n",
    "            delta[j] = error[i]*sigmoid(l, True)\n",
    "            j += 1\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_vects(df, iterations=100, alpha=0.1, random_state=42, W1=None, W2=None, W3=None, W4=None):\n",
    "    LSMR = preprocess_data(df)\n",
    "    data_dict, L1 = get_data_dict(LSMR)\n",
    "    y = softmax(list(LSMR.Score))\n",
    "#     np.random.seed(random_state)\n",
    "    learning_curve = dict()\n",
    "    for i in range(iterations+1):\n",
    "        # forward propagation\n",
    "        if W1 is None:\n",
    "            W1 = 2*np.random.random((len(L1), 300, 300))-1\n",
    "\n",
    "        LSM_R = merge(L1, W1)\n",
    "        L2 = get_L2(LSM_R, data_dict)\n",
    "        if W2 is None:\n",
    "            W2 = 2*np.random.random((len(L2), 300, 300))-1\n",
    "\n",
    "        LS_MR = merge(L2, W2)\n",
    "        L3 = get_L3(LS_MR, data_dict)\n",
    "        if W3 is None:\n",
    "            W3 = 2*np.random.random((len(L3), 300, 300))-1\n",
    "\n",
    "        score_vectors_dict = merge(L3, W3)\n",
    "        l4 = sigmoid(np.array([v for k, v in sorted(score_vectors_dict.items())]))\n",
    "        if W4 is None:\n",
    "            W4 = 2*np.random.random((300, len(LSMR)))-1\n",
    "        \n",
    "        l5 = softmax(l4.dot(W4))  # predicted scores\n",
    "        \n",
    "        # Calculate the error\n",
    "        l5_error = np.mean(np.dot(np.log(l5), y))\n",
    "        \n",
    "        # Back propagation\n",
    "        l5_delta = l5_error * sigmoid(l5, True)\n",
    "        W4 += l4.T.dot(l5_delta)*alpha\n",
    "        \n",
    "        l4_error = l5_delta.dot(W4.T)\n",
    "        l4_delta = l4_error * sigmoid(l4, True)\n",
    "        \n",
    "        W3 = update_weights(L3, l4_delta, W3, alpha)\n",
    "        \n",
    "        l3_error = get_layer_error(l4_delta, W3)\n",
    "        l3_delta = get_layer_delta(l3_error, L3, len(L2))\n",
    "        \n",
    "        W2 = update_weights(L2, l3_delta, W2, alpha)\n",
    "        \n",
    "        l2_error = get_layer_error(l3_delta, W2)\n",
    "        l2_delta = get_layer_delta(l2_error, L2, len(LSMR))\n",
    "        \n",
    "        W1 = update_weights(L1, l2_delta, W1, alpha)\n",
    "        learning_curve[i] = l5_error\n",
    "        if i%10 == 0:\n",
    "            print(\"epoch {}:\\t{}\".format(i, np.abs(l5_error)))\n",
    "        if i%100 == 0:\n",
    "            alpha *= 0.9\n",
    "    return LSMR, score_vectors_dict, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(LSMR, score_vect_dicts,random_state=42, regressor=MLPRegressor(), classifier=MLPClassifier()):\n",
    "    LSMR[\"score_vec\"] = LSMR[\"Score\"].apply(lambda x: score_vect_dicts[x] if x in score_vect_dicts else np.NaN)\n",
    "    LSMR.dropna(inplace=True)\n",
    "    \n",
    "    X, Y, y = get_XYy(LSMR)\n",
    "    \n",
    "    regressor.random_state = random_state\n",
    "    classifier.random_state = random_state\n",
    "        \n",
    "    regressor.fit(X, Y)\n",
    "    classifier.fit(Y, y)\n",
    "    return regressor, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(LSMR, score_vect_dicts, regressor, classifier):\n",
    "    LSMR[\"score_vec\"] = LSMR[\"Score\"].apply(lambda x: score_vect_dicts[x] if x in score_vect_dicts else np.NaN)\n",
    "    LSMR.dropna(inplace=True)\n",
    "    \n",
    "    X, Y, y = get_XYy(LSMR)\n",
    "    \n",
    "    preds_score_vecs = regressor.predict(X)\n",
    "    pred_scores = classifier.predict(preds_score_vecs)\n",
    "    \n",
    "    return pred_scores, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "epoch 0:\t21.857929193952334\n",
      "epoch 10:\t11.78191135546589\n",
      "epoch 20:\t10.743861036946399\n",
      "epoch 30:\t10.564884861268293\n",
      "epoch 40:\t10.082234042025572\n",
      "epoch 50:\t9.801125166103542\n",
      "epoch 60:\t9.627557759616256\n",
      "epoch 70:\t9.516972934779547\n",
      "epoch 80:\t9.444329098905214\n",
      "epoch 90:\t9.394712575639238\n",
      "epoch 100:\t9.360967767028788\n",
      "K:\t1\n",
      "Score:\t0.7822222222222223\n",
      "took: 24.64276146888733 seconds\n",
      "epoch 0:\t21.532691590684184\n",
      "epoch 10:\t11.766409764075963\n",
      "epoch 20:\t10.74274440048058\n",
      "epoch 30:\t10.179095377054258\n",
      "epoch 40:\t9.724601032603228\n",
      "epoch 50:\t9.472044163694722\n",
      "epoch 60:\t9.328757871814979\n",
      "epoch 70:\t9.24747872271433\n",
      "epoch 80:\t9.203535284813588\n",
      "epoch 90:\t9.181422301413708\n",
      "epoch 100:\t9.170314841048413\n",
      "K:\t2\n",
      "Score:\t0.745\n",
      "took: 24.919804573059082 seconds\n",
      "epoch 0:\t22.65762101851372\n",
      "epoch 10:\t12.334990193081541\n",
      "epoch 20:\t10.89924189676108\n",
      "epoch 30:\t10.220339226671955\n",
      "epoch 40:\t9.858984762482653\n",
      "epoch 50:\t9.66579546814998\n",
      "epoch 60:\t9.562490542218395\n",
      "epoch 70:\t9.50355247016409\n",
      "epoch 80:\t9.464500105346442\n",
      "epoch 90:\t9.434720113796626\n",
      "epoch 100:\t9.411690038921577\n",
      "K:\t3\n",
      "Score:\t0.7755555555555556\n",
      "took: 24.624197006225586 seconds\n",
      "epoch 0:\t22.624056059805746\n",
      "epoch 10:\t12.504636712363403\n",
      "epoch 20:\t11.00929300577866\n",
      "epoch 30:\t10.315025213467429\n",
      "epoch 40:\t9.917903423113184\n",
      "epoch 50:\t9.66434998027311\n",
      "epoch 60:\t9.499469388432123\n",
      "epoch 70:\t9.393706946537918\n",
      "epoch 80:\t9.327903455434313\n",
      "epoch 90:\t9.2859093412768\n",
      "epoch 100:\t9.255126549728894\n",
      "K:\t4\n",
      "Score:\t0.783\n",
      "took: 23.716923236846924 seconds\n",
      "epoch 0:\t22.86878292826082\n",
      "epoch 10:\t12.526981735239241\n",
      "epoch 20:\t10.95958402672816\n",
      "epoch 30:\t10.225917351802547\n",
      "epoch 40:\t9.823372755448565\n",
      "epoch 50:\t9.588087140263013\n",
      "epoch 60:\t9.443985552968432\n",
      "epoch 70:\t9.350873617228126\n",
      "epoch 80:\t9.287668451990337\n",
      "epoch 90:\t9.248479852535867\n",
      "epoch 100:\t9.22630630274656\n",
      "K:\t5\n",
      "Score:\t0.793\n",
      "took: 22.556773900985718 seconds\n",
      "epoch 0:\t22.090386634895744\n",
      "epoch 10:\t11.282093191069235\n",
      "epoch 20:\t10.611744314620646\n",
      "epoch 30:\t10.808247957865014\n",
      "epoch 40:\t10.202110301844886\n",
      "epoch 50:\t9.828321635301037\n",
      "epoch 60:\t9.607396658118885\n",
      "epoch 70:\t9.471665724715873\n",
      "epoch 80:\t9.384341064133864\n",
      "epoch 90:\t9.32506722358654\n",
      "epoch 100:\t9.284016082109446\n",
      "K:\t6\n",
      "Score:\t0.778\n",
      "took: 22.628109455108643 seconds\n",
      "epoch 0:\t23.1421106770598\n",
      "epoch 10:\t12.484106906050012\n",
      "epoch 20:\t10.911373739432063\n",
      "epoch 30:\t10.17731685872496\n",
      "epoch 40:\t9.760278764541294\n",
      "epoch 50:\t9.506311604961267\n",
      "epoch 60:\t9.346581143775278\n",
      "epoch 70:\t9.240594373047246\n",
      "epoch 80:\t9.174588404584378\n",
      "epoch 90:\t9.139897391334618\n",
      "epoch 100:\t9.123531434403517\n",
      "K:\t7\n",
      "Score:\t0.7922222222222222\n",
      "took: 22.773515939712524 seconds\n",
      "epoch 0:\t25.344958365755044\n",
      "epoch 10:\t12.365141213071917\n",
      "epoch 20:\t10.930085017203655\n",
      "epoch 30:\t10.31151364669532\n",
      "epoch 40:\t9.977976680855322\n",
      "epoch 50:\t9.77789449314256\n",
      "epoch 60:\t9.642395467406397\n",
      "epoch 70:\t9.550498318928621\n",
      "epoch 80:\t9.48945915125637\n",
      "epoch 90:\t9.446254886751934\n",
      "epoch 100:\t9.415041669414922\n",
      "K:\t8\n",
      "Score:\t0.7975\n",
      "took: 22.510948657989502 seconds\n",
      "epoch 0:\t23.636462672847763\n",
      "epoch 10:\t12.246376861709225\n",
      "epoch 20:\t10.763858795827517\n",
      "epoch 30:\t10.069085253514254\n",
      "epoch 40:\t9.680590987145674\n",
      "epoch 50:\t9.452882891304336\n",
      "epoch 60:\t9.313318580855077\n",
      "epoch 70:\t9.226911744425944\n",
      "epoch 80:\t9.175939646749411\n",
      "epoch 90:\t9.145019625951084\n",
      "epoch 100:\t9.125958102141164\n",
      "K:\t9\n",
      "Score:\t0.772\n",
      "took: 22.36478877067566 seconds\n",
      "epoch 0:\t24.486866543661268\n",
      "epoch 10:\t12.755846588857029\n",
      "epoch 20:\t11.372048844746185\n",
      "epoch 30:\t10.686526290013564\n",
      "epoch 40:\t10.300908161034153\n",
      "epoch 50:\t10.065091622960358\n",
      "epoch 60:\t9.922168540463996\n",
      "epoch 70:\t9.837188262733317\n",
      "epoch 80:\t9.78392280504176\n",
      "epoch 90:\t9.747356077366351\n",
      "epoch 100:\t9.720311667061997\n",
      "K:\t10\n",
      "Score:\t0.8022222222222222\n",
      "took: 22.4143545627594 seconds\n",
      "**********\n",
      "Trial 1 avg score:\t 0.7820722222222222\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 1\n",
    "scores = dict()\n",
    "learning_curves = dict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    scores[i] = dict()\n",
    "    learning_curves[i] = dict()\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        start = time.time()\n",
    "        LSMR, score_vect_dicts, training_curve = get_score_vects(df.loc[train_index], random_state=i)\n",
    "        regressor, classifier = fit(LSMR, score_vect_dicts, random_state=i)\n",
    "        preds, true = predict(preprocess_data(df.loc[test_index]), score_vect_dicts, regressor, classifier)\n",
    "        s = distance_accuracy(true, preds)\n",
    "        k += 1\n",
    "        print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "        print(\"took:\", time.time()-start, \"seconds\")\n",
    "        scores[i][k] = s\n",
    "        learning_curves[i][k] = training_curve\n",
    "    print(\"*\"*10)\n",
    "    try:\n",
    "        print(\"Trial {} avg score:\\t {}\".format(i+1, np.mean(list(scores[i].values()))))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([scores, learning_curves], open(\"batch_no_tf.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(LSMR):\n",
    "    X = dict()\n",
    "    y = dict()\n",
    "    for _, row in LSMR.iterrows():\n",
    "        score = row[\"Score\"]\n",
    "        y_ = np.zeros(10)\n",
    "        y_[score-1] = 1\n",
    "        y[len(y)] = y_\n",
    "        X[len(X)] = row[\"rev_vec\"]\n",
    "    return np.array(list(X.values())), np.array(list(y.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_selective(df_train,epochs=100, learning_rate = 0.1, random_state=42):\n",
    "    LSMR_train = preprocess_data(df_train)\n",
    "    np.random.seed(random_state)\n",
    "    data_dict, L1, L2, L3 = get_data_dict(LSMR_train, get_L2and3=True)\n",
    "    init_weights = lambda layer, i, o: {k:2*np.random.random((i, o))-1 for k in layer}\n",
    "    W1 = init_weights(L1, 300, 300)  # (languge, score, movie_id)\n",
    "    W2 = init_weights(L2, 300, 300)  # (languge, score):\n",
    "    W3 = init_weights(L3, 300, 10)  # score:\n",
    "    \n",
    "    \n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    training_curve = dict()\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for e in range(epochs+1):\n",
    "                avg_cost = 0.\n",
    "                for _, row in LSMR_train.iterrows():\n",
    "                    lang = row[\"Language\"]\n",
    "                    movie_id = row[\"Movie_ID\"]\n",
    "                    score = row[\"Score\"]\n",
    "                    y_ = np.zeros(10)\n",
    "                    y_[score-1] = 1\n",
    "                    y_ = np.atleast_2d(y_)\n",
    "                    x_ = np.atleast_2d(row[\"rev_vec\"])\n",
    "                    w1_,w2_,w3_,_, c = sess.run([w1, w2, w3, optimizer, cost],\n",
    "                                             feed_dict={x: x_,\n",
    "                                                        y: y_,\n",
    "                                                        w1:W1[(lang, score, movie_id)],\n",
    "                                                        w2:W2[(lang, score)],\n",
    "                                                        w3:W3[score]})\n",
    "                    W1[(lang, score, movie_id)] = w1_\n",
    "                    W2[(lang, score)] = w2_\n",
    "                    W3[score] = w3_\n",
    "\n",
    "                    avg_cost += c\n",
    "                training_curve[e] = avg_cost\n",
    "                if e%10==0:\n",
    "                    print(\"Epoch {}: {}\".format(e, avg_cost/len(LSMR_train)))\n",
    "\n",
    "            return W1, W2, W3, training_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(array):\n",
    "    indx = None\n",
    "    max_ = float(\"-inf\")\n",
    "    for i, e in enumerate(array):\n",
    "        if e > max_:\n",
    "            max_ = e\n",
    "            indx = i\n",
    "    return indx, max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_selective(df, W1, W2, W3):\n",
    "    LSMR = preprocess_data(df)\n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction = tf.argmax(pred, 1)\n",
    "    preds = np.zeros(len(LSMR))\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            j = 0\n",
    "            for _, row in LSMR.iterrows():\n",
    "                v = row[\"rev_vec\"]\n",
    "                predicted_scores = np.zeros(len(W1))\n",
    "                for i, info in enumerate(W1):\n",
    "                    language, score, movie_id = info\n",
    "                    w_1 = W1[(language, score, movie_id)]\n",
    "                    w_2 = W2[(language, score)]\n",
    "                    w_3 = W3[score]\n",
    "\n",
    "                    predicted_scores[i] = prediction.eval({x: np.atleast_2d(v),\n",
    "                                                           w1:w_1,w2:w_2,w3:w_3})\n",
    "\n",
    "                max_index, probability = get_max_index(softmax(predicted_scores))\n",
    "                predicted_score = predicted_scores[max_index]\n",
    "\n",
    "                preds[j] = predicted_score\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    return preds, np.array(list(LSMR.Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "Epoch 0: 1.6382539035607544\n",
      "Epoch 10: 0.00798041373439547\n",
      "Epoch 20: 0.00414424662976267\n",
      "Epoch 30: 0.002863710963413016\n",
      "Epoch 40: 0.002210395185328606\n",
      "Epoch 50: 0.0018101873358439965\n",
      "Epoch 60: 0.001538219162547547\n",
      "Epoch 70: 0.001340616424115271\n",
      "Epoch 80: 0.0011901179336645227\n",
      "Epoch 90: 0.0010714582095129623\n",
      "Epoch 100: 0.0009753979668150552\n",
      "Epoch 110: 0.000895884656217984\n",
      "Epoch 120: 0.0008289639292504741\n",
      "Epoch 130: 0.0007718336999662117\n",
      "Epoch 140: 0.0007223806926049293\n",
      "Epoch 150: 0.0006791661756869871\n",
      "Epoch 160: 0.0006410830465675745\n",
      "Epoch 170: 0.0006072232363415727\n",
      "Epoch 180: 0.0005769201475130911\n",
      "Epoch 190: 0.0005496473716306759\n",
      "Epoch 200: 0.0005249699332408555\n",
      "K:\t1\n",
      "Score:\t0.8311111111111111\n",
      "took: 254.67744493484497\n",
      "Epoch 0: 1.1594848213867388\n",
      "Epoch 10: 0.005971914769227927\n",
      "Epoch 20: 0.0031005067735289534\n",
      "Epoch 30: 0.0021304092739592305\n",
      "Epoch 40: 0.0016353528789800798\n",
      "Epoch 50: 0.0013328843151167449\n",
      "Epoch 60: 0.0011280476619948685\n",
      "Epoch 70: 0.0009797295794932224\n",
      "Epoch 80: 0.0008671807002812986\n",
      "Epoch 90: 0.0007787500059122168\n",
      "Epoch 100: 0.0007073532712658764\n",
      "Epoch 110: 0.0006484326457818194\n",
      "Epoch 120: 0.0005989645420049782\n",
      "Epoch 130: 0.0005568126516138566\n",
      "Epoch 140: 0.0005204455872768045\n",
      "Epoch 150: 0.0004887657376174401\n",
      "Epoch 160: 0.0004608892765099881\n",
      "Epoch 170: 0.0004361640494284479\n",
      "Epoch 180: 0.00041407682522225916\n",
      "Epoch 190: 0.00039424952776850354\n",
      "Epoch 200: 0.00037633229134371505\n",
      "K:\t2\n",
      "Score:\t0.789\n",
      "took: 262.09271001815796\n",
      "Epoch 0: 0.6120416745989334\n",
      "Epoch 10: 0.004075438038344145\n",
      "Epoch 20: 0.002176466930080399\n",
      "Epoch 30: 0.0015145005052747567\n",
      "Epoch 40: 0.0011730768415954016\n",
      "Epoch 50: 0.0009631936659651628\n",
      "Epoch 60: 0.0008203978479771952\n",
      "Epoch 70: 0.0007165970878001342\n",
      "Epoch 80: 0.000637499033166225\n",
      "Epoch 90: 0.0005751488446872423\n",
      "Epoch 100: 0.0005245999800253129\n",
      "Epoch 110: 0.0004827817076794519\n",
      "Epoch 120: 0.0004475334763345826\n",
      "Epoch 130: 0.0004174187956191923\n",
      "Epoch 140: 0.0003913305437365327\n",
      "Epoch 150: 0.0003685602068192869\n",
      "Epoch 160: 0.00034843736967256215\n",
      "Epoch 170: 0.00033057264006654605\n",
      "Epoch 180: 0.0003145685471665476\n",
      "Epoch 190: 0.0003001650652974301\n",
      "Epoch 200: 0.0002870939500523188\n",
      "K:\t3\n",
      "Score:\t0.8044444444444444\n",
      "took: 260.09117579460144\n",
      "Epoch 0: 1.320146789300359\n",
      "Epoch 10: 0.007894736596434894\n",
      "Epoch 20: 0.003821273039211519\n",
      "Epoch 30: 0.0025929678931909924\n",
      "Epoch 40: 0.0019875036838736073\n",
      "Epoch 50: 0.0016230492274432132\n",
      "Epoch 60: 0.0013780179299000238\n",
      "Epoch 70: 0.0012011988514698008\n",
      "Epoch 80: 0.0010671869850031929\n",
      "Epoch 90: 0.0009619050292045964\n",
      "Epoch 100: 0.0008767938805168443\n",
      "Epoch 110: 0.0008065085103832341\n",
      "Epoch 120: 0.0007474060096946131\n",
      "Epoch 130: 0.0006969276851547571\n",
      "Epoch 140: 0.0006533398467905095\n",
      "Epoch 150: 0.0006152987514603107\n",
      "Epoch 160: 0.0005817538149130996\n",
      "Epoch 170: 0.0005519158408666651\n",
      "Epoch 180: 0.0005252485982863517\n",
      "Epoch 190: 0.0005012266857536613\n",
      "Epoch 200: 0.00047944260339944674\n",
      "K:\t4\n",
      "Score:\t0.815\n",
      "took: 269.78641295433044\n",
      "Epoch 0: 1.0490617420880315\n",
      "Epoch 10: 0.005953050926974457\n",
      "Epoch 20: 0.0028458010501021314\n",
      "Epoch 30: 0.0019067970357718878\n",
      "Epoch 40: 0.0014478877764001177\n",
      "Epoch 50: 0.0011739704003492888\n",
      "Epoch 60: 0.0009911417246783257\n",
      "Epoch 70: 0.0008600498861051165\n",
      "Epoch 80: 0.0007612567578002603\n",
      "Epoch 90: 0.0006839331592588375\n",
      "Epoch 100: 0.000621693210802429\n",
      "Epoch 110: 0.0005705037790661057\n",
      "Epoch 120: 0.0005275997069192171\n",
      "Epoch 130: 0.0004910814499372565\n",
      "Epoch 140: 0.0004595831179904053\n",
      "Epoch 150: 0.00043215176268454847\n",
      "Epoch 160: 0.00040804321644019283\n",
      "Epoch 170: 0.00038663623402600126\n",
      "Epoch 180: 0.000367525347966067\n",
      "Epoch 190: 0.0003503356692494385\n",
      "Epoch 200: 0.0003348216878415163\n",
      "K:\t5\n",
      "Score:\t0.819\n",
      "took: 265.65846014022827\n",
      "Epoch 0: 1.1722658822794134\n",
      "Epoch 10: 0.0059080944598341985\n",
      "Epoch 20: 0.0030892584154046036\n",
      "Epoch 30: 0.0021282435998566346\n",
      "Epoch 40: 0.0016360375188459227\n",
      "Epoch 50: 0.0013346497438679863\n",
      "Epoch 60: 0.0011302759235291484\n",
      "Epoch 70: 0.00098209461090543\n",
      "Epoch 80: 0.000869592491880111\n",
      "Epoch 90: 0.0007811188825063356\n",
      "Epoch 100: 0.0007096362040061245\n",
      "Epoch 110: 0.000650664838613011\n",
      "Epoch 120: 0.0006011116376127271\n",
      "Epoch 130: 0.0005588501121544849\n",
      "Epoch 140: 0.0005224043960333802\n",
      "Epoch 150: 0.000490644936111898\n",
      "Epoch 160: 0.00046269908164200995\n",
      "Epoch 170: 0.0004378957908193115\n",
      "Epoch 180: 0.0004157445530614091\n",
      "Epoch 190: 0.0003958692613943842\n",
      "Epoch 200: 0.0003778709103223971\n",
      "K:\t6\n",
      "Score:\t0.812\n",
      "took: 269.5450427532196\n",
      "Epoch 0: 1.0844716265538914\n",
      "Epoch 10: 0.006051030131574306\n",
      "Epoch 20: 0.0030174847456833553\n",
      "Epoch 30: 0.0020648984809587194\n",
      "Epoch 40: 0.0015885282991156499\n",
      "Epoch 50: 0.0012996277140013667\n",
      "Epoch 60: 0.0011044320845717771\n",
      "Epoch 70: 0.0009630755861710188\n",
      "Epoch 80: 0.000855685044791446\n",
      "Epoch 90: 0.0007711411633095445\n",
      "Epoch 100: 0.0007027090893663828\n",
      "Epoch 110: 0.0006460826011607424\n",
      "Epoch 120: 0.0005984307330476845\n",
      "Epoch 130: 0.0005577108968282118\n",
      "Epoch 140: 0.0005225072979050715\n",
      "Epoch 150: 0.0004917094985204232\n",
      "Epoch 160: 0.0004645538087950424\n",
      "Epoch 170: 0.0004404106851062453\n",
      "Epoch 180: 0.0004187877182913427\n",
      "Epoch 190: 0.0003993143601110205\n",
      "Epoch 200: 0.00038169081025342974\n",
      "K:\t7\n",
      "Score:\t0.8277777777777777\n",
      "took: 273.9381196498871\n",
      "Epoch 0: 1.2060200627013626\n",
      "Epoch 10: 0.005660453100378314\n",
      "Epoch 20: 0.002991677048590241\n",
      "Epoch 30: 0.0020804145952246877\n",
      "Epoch 40: 0.0016114360057205583\n",
      "Epoch 50: 0.0013229220593348147\n",
      "Epoch 60: 0.0011263836459450734\n",
      "Epoch 70: 0.0009833616553482393\n",
      "Epoch 80: 0.0008743245098675188\n",
      "Epoch 90: 0.00078826824988937\n",
      "Epoch 100: 0.0007184470136417077\n",
      "Epoch 110: 0.0006606262865049454\n",
      "Epoch 120: 0.0006119225416558846\n",
      "Epoch 130: 0.000570211410199085\n",
      "Epoch 140: 0.0005341477110535682\n",
      "Epoch 150: 0.0005025880370036854\n",
      "Epoch 160: 0.00047471876180174555\n",
      "Epoch 170: 0.00044993407416364385\n",
      "Epoch 180: 0.00042774027003259917\n",
      "Epoch 190: 0.00040770511262558606\n",
      "Epoch 200: 0.0003895762135572214\n",
      "K:\t8\n",
      "Score:\t0.84\n",
      "took: 262.045916557312\n",
      "Epoch 0: 1.1561860611682964\n",
      "Epoch 10: 0.0059207740680883745\n",
      "Epoch 20: 0.0030844814903361515\n",
      "Epoch 30: 0.00212254353850666\n",
      "Epoch 40: 0.0016307634049897185\n",
      "Epoch 50: 0.0013299411476732025\n",
      "Epoch 60: 0.0011260465946462419\n",
      "Epoch 70: 0.000978286715496021\n",
      "Epoch 80: 0.000866139934530818\n",
      "Epoch 90: 0.0007779366464334695\n",
      "Epoch 100: 0.0007067015891112128\n",
      "Epoch 110: 0.0006478908637654968\n",
      "Epoch 120: 0.0005985077163333901\n",
      "Epoch 130: 0.0005564077251943268\n",
      "Epoch 140: 0.0005200876381827079\n",
      "Epoch 150: 0.0004884190964528696\n",
      "Epoch 160: 0.00046054556897919\n",
      "Epoch 170: 0.00043582089564653064\n",
      "Epoch 180: 0.0004137290618139862\n",
      "Epoch 190: 0.00039387519455986975\n",
      "Epoch 200: 0.0003759354470174811\n",
      "K:\t9\n",
      "Score:\t0.812\n",
      "took: 253.97217631340027\n",
      "Epoch 0: 0.9640095431078225\n",
      "Epoch 10: 0.004729528367622859\n",
      "Epoch 20: 0.0025650803301767965\n",
      "Epoch 30: 0.0018007458111323003\n",
      "Epoch 40: 0.0014034708382678219\n",
      "Epoch 50: 0.0011579019468788627\n",
      "Epoch 60: 0.0009901128450292163\n",
      "Epoch 70: 0.0008676782507973257\n",
      "Epoch 80: 0.0007741572853248929\n",
      "Epoch 90: 0.0007001970477479821\n",
      "Epoch 100: 0.0006401247016846254\n",
      "Epoch 110: 0.0005902956469476016\n",
      "Epoch 120: 0.0005482204101604617\n",
      "Epoch 130: 0.0005122019788703053\n",
      "Epoch 140: 0.0004809696620456331\n",
      "Epoch 150: 0.00045362098472348104\n",
      "Epoch 160: 0.0004294912059817256\n",
      "Epoch 170: 0.00040797644634343064\n",
      "Epoch 180: 0.00038871393304563956\n",
      "Epoch 190: 0.0003712853196985735\n",
      "Epoch 200: 0.0003555061463379793\n",
      "K:\t10\n",
      "Score:\t0.8244444444444444\n",
      "took: 259.59874272346497\n",
      "**********\n",
      "Trial 1 avg score:\t 0.818111111111111\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 1\n",
    "scores_incremental = dict()\n",
    "learning_curves = dict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    scores_incremental[i] = dict()\n",
    "    learning_curves[i] = dict()\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        start = time.time()\n",
    "        # approx 3 epochs per second\n",
    "        W1, W2, W3, training_curve = train_selective(df.loc[train_index], epochs=200)\n",
    "        \n",
    "        preds, true = predict_selective(df.loc[test_index], W1, W2, W3)\n",
    "        s = distance_accuracy(true, preds)\n",
    "        scores_incremental[i][k] = s\n",
    "        learning_curves[i][k] = training_curve\n",
    "        k += 1\n",
    "        print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "        print(\"took:\", time.time()-start)\n",
    "        scores_incremental[i][k] = s\n",
    "        learning_curves[i][k] = training_curve\n",
    "    print(\"*\"*10)\n",
    "    try:\n",
    "        print(\"Trial {} avg score:\\t {}\".format(i+1, np.mean(list(scores_incremental[i].values()))))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([scores_incremental, learning_curves], open(\"incremental_tf.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO CHEAT LIKE A PRO\n",
    "# \"\"\"\n",
    "# def test_selective(df_test, W1, W2, W3):\n",
    "#     reset_graph()\n",
    "#     x = tf.placeholder(tf.float32, [None, 300])\n",
    "#     y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "#     w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "#     b1 = tf.Variable(tf.zeros([300]))\n",
    "#     b2 = tf.Variable(tf.zeros([300]))\n",
    "#     b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#     l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "#     l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "#     pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "    \n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#     instance_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#         # Testing the model\n",
    "#         LSMR_test = preprocess_data(df_test)\n",
    "#         X_test, y_test = get_test(LSMR_test)\n",
    "#         accuracy = 0.\n",
    "#         for i in range(len(X_test)):\n",
    "#             best_instance_accuracy = float(\"-inf\")\n",
    "#             for language, score, movie_id in W1:\n",
    "#                 w_1 = W1[(language, score, movie_id)]\n",
    "#                 w_2 = W2[(language, score)]\n",
    "#                 w_3 = W3[score]\n",
    "#                 a = instance_accuracy.eval({x: np.atleast_2d(X_test[i]), y: np.atleast_2d(y_test[i]),\n",
    "#                                    w1:w_1,\n",
    "#                                    w2:w_2,\n",
    "#                                    w3:w_3})\n",
    "#                 if a > best_instance_accuracy:\n",
    "#                     best_instance_accuracy = a\n",
    "#             accuracy += best_instance_accuracy\n",
    "\n",
    "#     return accuracy/len(X_test)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-layer NN > needs at least 3 days for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu is a must\n",
    "def train_deep(df_train, epochs=100, learning_rate=0.1, random_state=42):\n",
    "    LSMR_train = preprocess_data(df_train)\n",
    "    np.random.seed(random_state)\n",
    "    data_dict, L1, L2, L3 = get_data_dict(LSMR_train, get_L2and3=True)\n",
    "    init_weights = lambda layer, i, o: {k:2*np.random.random((i, o))-1 for k in layer}\n",
    "    W1 = init_weights(L1, 300, 300)  # (languge, score, movie_id)\n",
    "    W2 = init_weights(L2, 300, 300)  # (languge, score):\n",
    "    W3 = init_weights(L3, 300, 10)  # score:\n",
    "    \n",
    "    \n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.Variable(tf.zeros([300, 300]))\n",
    "    w2 = tf.Variable(tf.zeros([300, 300]))\n",
    "    w3 = tf.Variable(tf.zeros([300, 10]))\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    training_curve = dict()\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for e in range(epochs+1):\n",
    "                start = time.time()\n",
    "                avg_cost = 0.\n",
    "                for _, row in LSMR_train.iterrows():\n",
    "                    score = row[\"Score\"]\n",
    "                    y_ = np.zeros(10)\n",
    "                    y_[score-1] = 1\n",
    "                    y_ = np.atleast_2d(y_)\n",
    "                    x_ = np.atleast_2d(row[\"rev_vec\"])\n",
    "                    w_1, w_2, w_3 , _, c = sess.run([w1, w2, w3, optimizer, cost], feed_dict={x: x_,y: y_})               \n",
    "                    avg_cost += c\n",
    "                avg_cost /= len(LSMR_train)\n",
    "                training_curve[e] = (avg_cost, time.time()-start)\n",
    "                if e%10==0:\n",
    "                    print(\"Epoch {}: {}\".format(e, avg_cost))\n",
    "\n",
    "    return w_1, w_2, w_3, training_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_deep(df_test, w_1, w_2, w_3):\n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Testing the model\n",
    "            LSMR_test = preprocess_data(df_test)\n",
    "            X_test, y_test = get_test(LSMR_test)\n",
    "            return accuracy.eval({x: X_test,\n",
    "                                  y: y_test,\n",
    "                                  w1:w_1,w2:w_2,\n",
    "                                  w3:w_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "K:\t1\n",
      "Score:\t0.2800000011920929\n",
      "took: 0.1636667251586914\n",
      "K:\t2\n",
      "Score:\t0.2800000011920929\n",
      "took: 0.18086695671081543\n",
      "K:\t3\n",
      "Score:\t0.3700000047683716\n",
      "took: 0.22888827323913574\n",
      "K:\t4\n",
      "Score:\t0.3400000035762787\n",
      "took: 0.1683807373046875\n",
      "K:\t5\n",
      "Score:\t0.3700000047683716\n",
      "took: 0.16428303718566895\n",
      "K:\t6\n",
      "Score:\t0.33000001311302185\n",
      "took: 0.16387176513671875\n",
      "K:\t7\n",
      "Score:\t0.3400000035762787\n",
      "took: 0.2004718780517578\n",
      "K:\t8\n",
      "Score:\t0.33000001311302185\n",
      "took: 0.18352031707763672\n",
      "K:\t9\n",
      "Score:\t0.30000001192092896\n",
      "took: 0.16056609153747559\n",
      "K:\t10\n",
      "Score:\t0.4000000059604645\n",
      "took: 0.16551804542541504\n",
      "**********\n",
      "Trial 1 avg score:\t 0.33400002121925354\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 1\n",
    "scores_incremental = dict()\n",
    "learning_curves = dict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    scores_incremental[i] = dict()\n",
    "    learning_curves[i] = dict()\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        start = time.time()\n",
    "        w1, w2, w3, learning_curve = train_deep(df.loc[train_index], random_state=i, epochs=10000)\n",
    "        s = test_deep(df.loc[test_index], w1, w2, w3)\n",
    "        k += 1\n",
    "        print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "        print(\"took:\", time.time()-start)\n",
    "        scores_incremental[i][k] = s\n",
    "        learning_curves[i][k] = learning_curve\n",
    "    print(\"*\"*10)\n",
    "    try:\n",
    "        print(\"Trial {} avg score:\\t {}\".format(i+1, np.mean(list(scores_incremental[i].values()))))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
