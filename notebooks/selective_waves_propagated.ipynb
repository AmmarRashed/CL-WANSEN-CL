{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle, pandas as pd, re, numpy as np, ast, warnings\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import chain, starmap\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>i love science fiction and i hate superheroes ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>the movie is absolutely incredible all the per...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>in a cinematic era dominated by reboots and mi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>movie review on rise of the planet of the apes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>during experiments to find a cure for alzheime...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language    Movie_ID                                             Review  \\\n",
       "0       en  -800777728  i love science fiction and i hate superheroes ...   \n",
       "1       en  -800777728  the movie is absolutely incredible all the per...   \n",
       "2       en -1018312192  in a cinematic era dominated by reboots and mi...   \n",
       "3       en -1018312192  movie review on rise of the planet of the apes...   \n",
       "4       en -1018312192  during experiments to find a cure for alzheime...   \n",
       "\n",
       "   Score  \n",
       "0      9  \n",
       "1     10  \n",
       "2      8  \n",
       "3      4  \n",
       "4      7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.read_csv(\"../datasets/movie_data.csv\")\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language  Movie_ID  Review\n",
       "Score                            \n",
       "1            29        29      29\n",
       "2            21        21      21\n",
       "3            14        14      14\n",
       "4            23        23      23\n",
       "5            83        83      83\n",
       "6            43        43      43\n",
       "7            71        71      71\n",
       "8           207       207     207\n",
       "9           175       175     175\n",
       "10          334       334     334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.groupby(\"Score\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../../NLP_data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../../NLP_data/wiki.tr/wiki.tr.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_stemmer = TurkishStemmer()\n",
    "def clean(text, language=\"en\", stem=True):\n",
    "    global turkish_stemmer\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "    \n",
    "    if language == \"tr\":\n",
    "        if stem:\n",
    "            text= ' '.join([turkish_stemmer.stem(w) for w in text.split()])\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r'[0-9]', '#', text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    return TextBlob(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 300\n",
    "def vectorize(text, language):\n",
    "    global VECTOR_SIZE            \n",
    "    blob = clean(text, language)\n",
    "    vector = np.zeros(VECTOR_SIZE)\n",
    "    if len(blob.words) < 1:\n",
    "        return None\n",
    "\n",
    "    for word in blob.words:\n",
    "        try:\n",
    "            if language == \"en\":\n",
    "                vector += globals()[\"en_vects\"][word]\n",
    "            else:\n",
    "                vector += globals()[\"tr_vects\"][word]\n",
    "        except KeyError as e:\n",
    "#             warnings.warn(str(e))\n",
    "            continue\n",
    "    vector /= max(len(blob.words),1)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvec(x):\n",
    "    lang, rev = x.split(\":::::\")\n",
    "    return vectorize(rev, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMSR\n",
    "def preprocess_data(df, language_column=\"Language\", review_column=\"Review\"):\n",
    "    LMSR_df = df.copy()\n",
    "    LMSR_df[\"lang_rev\"] = LMSR_df[[language_column, review_column]].apply(lambda x: x[0]+\":::::\"+x[1], axis=1)\n",
    "    LMSR_df[\"rev_vec\"] = LMSR_df[\"lang_rev\"].apply(lambda x:getvec(x))\n",
    "    LMSR_df.drop([\"lang_rev\", \"Review\"], axis=1, inplace=True)\n",
    "    return LMSR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_accuracy(y_true, y_predict):\n",
    "    res = 0\n",
    "    for i in range(len(y_true)):\n",
    "        res += abs(y_true[i]-y_predict[i])\n",
    "    return 1-res/(len(y_true)*len(set(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XYy(LMSR):\n",
    "    X = np.zeros((len(LMSR), VECTOR_SIZE))\n",
    "    Y = np.zeros((len(LMSR), VECTOR_SIZE))\n",
    "    y = np.zeros((len(LMSR)))\n",
    "    i = 0\n",
    "    for rev in LMSR.iterrows():\n",
    "        score = rev[1][2]\n",
    "        rev_vec = rev[1][3]\n",
    "        score_vec = rev[1][4]\n",
    "\n",
    "        X[i] = rev_vec\n",
    "        Y[i] = score_vec\n",
    "        y[i] = score\n",
    "\n",
    "        i += 1\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derive=False):\n",
    "    if derive:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(df, get_L2and3=False):\n",
    "    data_dict = dict() #{language:{score: {movie_id: [rev1, rev2, ..., revn]}}}\n",
    "    L1 = dict()  # {(languge, score, movie_id): list of reviews with the same score with the same language}\n",
    "    L2 = dict()  # {(language, score): None}\n",
    "    L3 = dict()  # {score: None}\n",
    "    for _, row in df.iterrows():\n",
    "        lang = row[\"Language\"]\n",
    "        movie_id = row[\"Movie_ID\"]\n",
    "        score = row[\"Score\"]\n",
    "        review = row[\"rev_vec\"]\n",
    "\n",
    "        data_dict.setdefault(lang, {})\n",
    "        data_dict[lang].setdefault(score, {})\n",
    "        data_dict[lang][score].setdefault(movie_id, [])\n",
    "        data_dict[lang][score][movie_id].append(review)\n",
    "        \n",
    "        L1.setdefault((lang, score, movie_id), list())\n",
    "        L1[(lang, score, movie_id)].append(review)\n",
    "        if get_L2and3:    \n",
    "            L2[(lang, score)] = None\n",
    "            L3[score] = None\n",
    "    if get_L2and3:\n",
    "        return data_dict, L1, L2, L3\n",
    "    return data_dict, L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L2(LSM_R, data_dict):\n",
    "    L2 = dict()  # {(language, score): list of movies vectors}\n",
    "    for language in data_dict:\n",
    "        for score in data_dict[language]:\n",
    "            for movie_id in data_dict[language][score]:\n",
    "                L2.setdefault((language, score), list())\n",
    "                L2[(language, score)].append(LSM_R[(language, score, movie_id)])\n",
    "    return L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L3(LS_MR, data_dict):\n",
    "    L3 = dict()  # {score: vector of merged languages for that score}\n",
    "    for language in data_dict:\n",
    "        for score in data_dict[language]:\n",
    "            L3.setdefault(score, list())\n",
    "            L3[score].append(LS_MR[(language, score)])\n",
    "    return L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(L, W):\n",
    "    merged = dict()  # {item: vector of merged subitems}\n",
    "    for i, item in enumerate(sorted(L)):\n",
    "        for subitem in L[item]:\n",
    "            merged.setdefault(item, [np.zeros(VECTOR_SIZE),0])\n",
    "            merged[item][0] += sigmoid(subitem.dot(W[i]))\n",
    "            merged[item][1] += 1\n",
    "    for item in merged:\n",
    "        merged[item] = merged[item][0]/ merged[item][1]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(L, delta, W, alpha=0.1):\n",
    "    for i, k in enumerate(sorted(L)):\n",
    "        for l in L[k]:\n",
    "            W[i] += l.T.dot(delta[i]) *alpha\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_error(delta, W):\n",
    "    error = 0\n",
    "    for i in range(len(delta)):\n",
    "        error += delta[i].dot(W[i].T)\n",
    "    return error/len(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_delta(error, layer, size):\n",
    "    delta = np.zeros((size, VECTOR_SIZE))\n",
    "    j = 0\n",
    "    for i,k in enumerate(sorted(layer)):\n",
    "        for l in layer[k]:\n",
    "            delta[j] = error[i]*sigmoid(l, True)\n",
    "            j += 1\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_vects(df, iterations=100, alpha=0.1, random_state=42, W1=None, W2=None, W3=None, W4=None):\n",
    "    LSMR = preprocess_data(df)\n",
    "    data_dict, L1 = get_data_dict(LSMR)\n",
    "    y = softmax(list(LSMR.Score))\n",
    "#     np.random.seed(random_state)\n",
    "    learning_curve = dict()\n",
    "    for i in range(iterations+1):\n",
    "        # forward propagation\n",
    "        if W1 is None:\n",
    "            W1 = 2*np.random.random((len(L1), 300, 300))-1\n",
    "\n",
    "        LSM_R = merge(L1, W1)\n",
    "        L2 = get_L2(LSM_R, data_dict)\n",
    "        if W2 is None:\n",
    "            W2 = 2*np.random.random((len(L2), 300, 300))-1\n",
    "\n",
    "        LS_MR = merge(L2, W2)\n",
    "        L3 = get_L3(LS_MR, data_dict)\n",
    "        if W3 is None:\n",
    "            W3 = 2*np.random.random((len(L3), 300, 300))-1\n",
    "\n",
    "        score_vectors_dict = merge(L3, W3)\n",
    "        l4 = sigmoid(np.array([v for k, v in sorted(score_vectors_dict.items())]))\n",
    "        if W4 is None:\n",
    "            W4 = 2*np.random.random((300, len(LSMR)))-1\n",
    "        \n",
    "        l5 = softmax(l4.dot(W4))  # predicted scores\n",
    "        \n",
    "        # Calculate the error\n",
    "        l5_error = np.mean(np.dot(np.log(l5), y))\n",
    "        \n",
    "        # Back propagation\n",
    "        l5_delta = l5_error * sigmoid(l5, True)\n",
    "        W4 += l4.T.dot(l5_delta)*alpha\n",
    "        \n",
    "        l4_error = l5_delta.dot(W4.T)\n",
    "        l4_delta = l4_error * sigmoid(l4, True)\n",
    "        \n",
    "        W3 = update_weights(L3, l4_delta, W3, alpha)\n",
    "        \n",
    "        l3_error = get_layer_error(l4_delta, W3)\n",
    "        l3_delta = get_layer_delta(l3_error, L3, len(L2))\n",
    "        \n",
    "        W2 = update_weights(L2, l3_delta, W2, alpha)\n",
    "        \n",
    "        l2_error = get_layer_error(l3_delta, W2)\n",
    "        l2_delta = get_layer_delta(l2_error, L2, len(LSMR))\n",
    "        \n",
    "        W1 = update_weights(L1, l2_delta, W1, alpha)\n",
    "        learning_curve[i] = l5_error\n",
    "        if i%10 == 0:\n",
    "            print(\"epoch {}:\\t{}\".format(i, np.abs(l5_error)))\n",
    "        if i%100 == 0:\n",
    "            alpha *= 0.9\n",
    "    return LSMR, score_vectors_dict, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(LSMR, score_vect_dicts,random_state=42, regressor=MLPRegressor(), classifier=MLPClassifier()):\n",
    "    LSMR[\"score_vec\"] = LSMR[\"Score\"].apply(lambda x: score_vect_dicts[x] if x in score_vect_dicts else np.NaN)\n",
    "    LSMR.dropna(inplace=True)\n",
    "    \n",
    "    X, Y, y = get_XYy(LSMR)\n",
    "    \n",
    "    regressor.random_state = random_state\n",
    "    classifier.random_state = random_state\n",
    "        \n",
    "    regressor.fit(X, Y)\n",
    "    classifier.fit(Y, y)\n",
    "    return regressor, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(LSMR, score_vect_dicts, regressor, classifier):\n",
    "    LSMR[\"score_vec\"] = LSMR[\"Score\"].apply(lambda x: score_vect_dicts[x] if x in score_vect_dicts else np.NaN)\n",
    "    LSMR.dropna(inplace=True)\n",
    "    \n",
    "    X, Y, y = get_XYy(LSMR)\n",
    "    \n",
    "    preds_score_vecs = regressor.predict(X)\n",
    "    pred_scores = classifier.predict(preds_score_vecs)\n",
    "    \n",
    "    return pred_scores, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language    0\n",
       "Movie_ID    0\n",
       "Score       0\n",
       "rev_vec     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tronly_test_raw = df_full[-100:]\n",
    "tronly_test = preprocess_data(tronly_test_raw)\n",
    "df = df_full[:-100]\n",
    "tronly_test[tronly_test.Language==\"en\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(model, train, test, tronly, ytrain, ytest, ytronly):\n",
    "    _ = time.time()\n",
    "    model.fit(train, ytrain)\n",
    "    predtra = time.time()-_\n",
    "    \n",
    "    _ = time.time()\n",
    "    s_train = distance_accuracy(ytrain, model.predict(train))\n",
    "    trat = time.time()-_\n",
    "    _ = time.time()\n",
    "    s_test = distance_accuracy(ytest, model.predict(test))\n",
    "    tet = time.time()-_\n",
    "    _ = time.time()\n",
    "    s_tr = distance_accuracy(ytronly, model.predict(tronly))\n",
    "    trt = time.time()-_\n",
    "    evals = OrderedDict()\n",
    "    evals[\"Train\"] = s_train\n",
    "    evals[\"Test\"] = s_test\n",
    "    evals[\"Tr. Only\"] = s_tr\n",
    "    evals[\"Training Time\"] = trat\n",
    "    evals[\"Pred.Tra. Time\"] = predtra\n",
    "    evals[\"Testing Time\"] = tet\n",
    "    evals[\"Tr.Test Time\"] = trt\n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_average(scores_tables):\n",
    "#     scores_tables: {i_th trial:\n",
    "#                     {k_th fold:\n",
    "#                         {'Model': {'Test': 0.8090301003344482,\n",
    "#                                    'Train': 0.783361064891847,\n",
    "#                                    'Turkish only': 0.7414285714285714}}}\n",
    "    avgs = dict()\n",
    "    for trial in scores_tables:\n",
    "        for table in scores_tables[trial]:\n",
    "            for model in scores_tables[trial][table]:\n",
    "                avgs.setdefault(model, dict())\n",
    "                for metric, score in scores_tables[trial][table][model].items():\n",
    "                    avgs[model].setdefault(metric, list())\n",
    "                    avgs[model][metric].append(score)\n",
    "    for model in avgs:\n",
    "        for metric in avgs[model]:\n",
    "            avgs[model][metric] = np.mean(avgs[model][metric])\n",
    "    return pd.DataFrame(avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_score(trial_scores_tables):\n",
    "#  trial_scores_tables: {k_th fold:\n",
    "#                             {'Model': {'Test': 0.8090301003344482,\n",
    "#                                        'Train': 0.783361064891847,\n",
    "#                                        'Turkish only': 0.7414285714285714}}}\n",
    "    avgs = dict()\n",
    "    for table in trial_scores_tables:\n",
    "        for model in trial_scores_tables[table]:\n",
    "            avgs.setdefault(model, dict())\n",
    "            for metric, score in trial_scores_tables[table][model].items():\n",
    "                avgs[model].setdefault(metric, list())\n",
    "                avgs[model][metric].append(score)\n",
    "    for model in avgs:\n",
    "        for metric in avgs[model]:\n",
    "            avgs[model][metric] = np.mean(avgs[model][metric])\n",
    "    return pd.DataFrame(avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "K:\t1\n",
      "epoch 0:\t24.45730175287146\n",
      "epoch 10:\t23.96708852815955\n",
      "epoch 20:\t23.713081453867304\n",
      "epoch 30:\t23.527294032263164\n",
      "epoch 40:\t23.380831185653854\n",
      "epoch 50:\t23.235479365133063\n",
      "\n",
      "K:\t1\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.627098  0.294413             0.150325      0.107511\n",
      "Test              0.807407  0.811111             0.802469      0.798765\n",
      "Testing Time      0.008493  0.000366             0.000142      0.105365\n",
      "Tr. Only          0.784286  0.798571             0.781429      0.714286\n",
      "Tr.Test Time      0.013459  0.000374             0.000141      0.105668\n",
      "Train             0.809753  0.823333             0.823086      0.999259\n",
      "Training Time    12.665944  0.002030             0.001024      0.104978\n",
      "\n",
      "This fold took: 14.508926153182983 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t2\n",
      "epoch 0:\t22.457844475004883\n",
      "epoch 10:\t22.26013491539242\n",
      "epoch 20:\t22.148090851114496\n",
      "epoch 30:\t21.963549969361317\n",
      "epoch 40:\t21.784430638633225\n",
      "epoch 50:\t21.611525932121037\n",
      "\n",
      "K:\t2\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.618378  0.299228             0.148233      0.107587\n",
      "Test              0.762222  0.772222             0.773333      0.793333\n",
      "Testing Time      0.008490  0.000369             0.000170      0.104524\n",
      "Tr. Only          0.755714  0.817143             0.782857      0.748571\n",
      "Tr.Test Time      0.013374  0.000363             0.000131      0.105473\n",
      "Train             0.807778  0.829877             0.829506      0.998889\n",
      "Training Time    13.121943  0.002048             0.000866      0.104532\n",
      "\n",
      "This fold took: 14.946085214614868 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t3\n",
      "epoch 0:\t23.791010699703612\n",
      "epoch 10:\t23.64954735547725\n",
      "epoch 20:\t23.486086782192967\n",
      "epoch 30:\t23.295310492026786\n",
      "epoch 40:\t23.130927991911705\n",
      "epoch 50:\t22.9575868203793\n",
      "\n",
      "K:\t3\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.538740  0.319301             0.151807      0.107782\n",
      "Test              0.759259  0.771605             0.756790      0.770370\n",
      "Testing Time      0.008493  0.000366             0.000131      0.105725\n",
      "Tr. Only          0.767143  0.797143             0.780000      0.754286\n",
      "Tr.Test Time      0.013274  0.000357             0.000128      0.105592\n",
      "Train             0.811605  0.826420             0.827531      0.998395\n",
      "Training Time    12.948300  0.002012             0.000891      0.104757\n",
      "\n",
      "This fold took: 14.73786211013794 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t4\n",
      "epoch 0:\t22.349399639962066\n",
      "epoch 10:\t22.205318743547327\n",
      "epoch 20:\t22.053881696053764\n",
      "epoch 30:\t21.933073916780756\n",
      "epoch 40:\t21.831599599115222\n",
      "epoch 50:\t21.734845866805877\n",
      "\n",
      "K:\t4\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.565330  0.290623             0.149033      0.107655\n",
      "Test              0.815556  0.807778             0.805556      0.785556\n",
      "Testing Time      0.008304  0.000366             0.000131      0.105313\n",
      "Tr. Only          0.798571  0.787143             0.801429      0.790000\n",
      "Tr.Test Time      0.018623  0.000360             0.000129      0.105412\n",
      "Train             0.813704  0.821111             0.823457      0.999259\n",
      "Training Time    12.695321  0.002023             0.000836      0.105002\n",
      "\n",
      "This fold took: 14.492758512496948 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t5\n",
      "epoch 0:\t22.671637846686494\n",
      "epoch 10:\t22.512312196629725\n",
      "epoch 20:\t22.259457882497188\n",
      "epoch 30:\t22.06053191758301\n",
      "epoch 40:\t21.9032279134957\n",
      "epoch 50:\t21.761824191269966\n",
      "\n",
      "K:\t5\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.894274  0.487162             0.148183      0.107556\n",
      "Test              0.806173  0.809877             0.792593      0.746914\n",
      "Testing Time      0.010305  0.000357             0.000136      0.102734\n",
      "Tr. Only          0.790000  0.804286             0.775714      0.742857\n",
      "Tr.Test Time      0.013393  0.000364             0.000131      0.101820\n",
      "Train             0.811975  0.823827             0.820617      0.999259\n",
      "Training Time    14.851061  0.001987             0.001038      0.105739\n",
      "\n",
      "This fold took: 16.989753246307373 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t6\n",
      "epoch 0:\t23.85167939745424\n",
      "epoch 10:\t23.668419695828238\n",
      "epoch 20:\t23.34289907110695\n",
      "epoch 30:\t23.184065162076983\n",
      "epoch 40:\t23.01669872213682\n",
      "epoch 50:\t22.844082817454442\n",
      "\n",
      "K:\t6\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.666926  0.398428             0.148934      0.107682\n",
      "Test              0.801111  0.790000             0.793333      0.805556\n",
      "Testing Time      0.008539  0.000360             0.000137      0.105636\n",
      "Tr. Only          0.804286  0.810000             0.767143      0.747143\n",
      "Tr.Test Time      0.013505  0.000355             0.000133      0.105483\n",
      "Train             0.815802  0.829630             0.827901      0.998025\n",
      "Training Time    14.258187  0.002019             0.000898      0.105091\n",
      "\n",
      "This fold took: 16.12593698501587 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t7\n",
      "epoch 0:\t20.497842806838413\n",
      "epoch 10:\t20.40651383971842\n",
      "epoch 20:\t20.302368851476214\n",
      "epoch 30:\t20.218933912775068\n",
      "epoch 40:\t20.1609307367686\n",
      "epoch 50:\t20.111180170162314\n",
      "\n",
      "K:\t7\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.618160  0.760895             0.151523      0.107647\n",
      "Test              0.760000  0.787778             0.783333      0.777778\n",
      "Testing Time      0.008525  0.000366             0.000135      0.106155\n",
      "Tr. Only          0.741429  0.814286             0.788571      0.771429\n",
      "Tr.Test Time      0.013538  0.000355             0.000130      0.105608\n",
      "Train             0.795432  0.834198             0.827160      0.998148\n",
      "Training Time    12.737924  0.002017             0.000948      0.105148\n",
      "\n",
      "This fold took: 14.577642440795898 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t8\n",
      "epoch 0:\t24.199896094625547\n",
      "epoch 10:\t23.931727471424523\n",
      "epoch 20:\t23.656750659852303\n",
      "epoch 30:\t23.30894172779529\n",
      "epoch 40:\t23.073243735369477\n",
      "epoch 50:\t22.850634906944755\n",
      "\n",
      "K:\t8\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.580773  0.394333             0.150626      0.107975\n",
      "Test              0.813889  0.816667             0.825000      0.775000\n",
      "Testing Time      0.008548  0.000807             0.000135      0.105455\n",
      "Tr. Only          0.790000  0.821429             0.800000      0.688571\n",
      "Tr.Test Time      0.014024  0.000375             0.000131      0.105575\n",
      "Train             0.810123  0.819383             0.821235      0.998025\n",
      "Training Time    12.821843  0.001973             0.000888      0.104665\n",
      "\n",
      "This fold took: 14.627867937088013 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t9\n",
      "epoch 0:\t24.740026578197277\n",
      "epoch 10:\t24.491503795674028\n",
      "epoch 20:\t24.089744729543504\n",
      "epoch 30:\t23.737851843655164\n",
      "epoch 40:\t23.46463401060526\n",
      "epoch 50:\t23.24850338499321\n",
      "\n",
      "K:\t9\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.622314  0.857046             0.153536      0.107918\n",
      "Test              0.770000  0.768889             0.757778      0.754444\n",
      "Testing Time      0.008403  0.000382             0.000136      0.105525\n",
      "Tr. Only          0.775714  0.800000             0.781429      0.737143\n",
      "Tr.Test Time      0.013433  0.000360             0.000133      0.105456\n",
      "Train             0.820494  0.845679             0.828148      0.998642\n",
      "Training Time    13.458017  0.002009             0.000952      0.104709\n",
      "\n",
      "This fold took: 15.341447114944458 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t10\n",
      "epoch 0:\t24.85893685449479\n",
      "epoch 10:\t24.480369868925795\n",
      "epoch 20:\t24.130759888085198\n",
      "epoch 30:\t23.866146260201383\n",
      "epoch 40:\t23.616477710416184\n",
      "epoch 50:\t23.39680318533207\n",
      "\n",
      "K:\t10\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "Pred.Tra. Time    1.605192  0.909685             0.147015      0.107575\n",
      "Test              0.822222  0.793056             0.806944      0.773611\n",
      "Testing Time      0.008555  0.000363             0.000138      0.105346\n",
      "Tr. Only          0.801429  0.735714             0.771429      0.760000\n",
      "Tr.Test Time      0.013474  0.000359             0.000132      0.105412\n",
      "Train             0.811975  0.837284             0.816173      0.998889\n",
      "Training Time    13.705395  0.002027             0.000977      0.104884\n",
      "\n",
      "This fold took: 15.522321462631226 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "Average scores for trial 0\n",
      "                DeepSelect  Logistic Regression       MLP  RandomForest\n",
      "Pred.Tra. Time    1.633719             0.149921  0.501111      0.107689\n",
      "Test              0.791784             0.789713  0.792898      0.778133\n",
      "Testing Time      0.008665             0.000139  0.000410      0.105178\n",
      "Tr. Only          0.780857             0.783000  0.798571      0.745429\n",
      "Tr.Test Time      0.014010             0.000132  0.000362      0.105150\n",
      "Train             0.810864             0.824481  0.829074      0.998679\n",
      "Training Time    13.326394             0.000932  0.002015      0.104951\n",
      "------------------------------\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Average of 1 trials\n",
      "                DeepSelect  Logistic Regression       MLP  RandomForest\n",
      "Pred.Tra. Time    1.633719             0.149921  0.501111      0.107689\n",
      "Test              0.791784             0.789713  0.792898      0.778133\n",
      "Testing Time      0.008665             0.000139  0.000410      0.105178\n",
      "Tr. Only          0.780857             0.783000  0.798571      0.745429\n",
      "Tr.Test Time      0.014010             0.000132  0.000362      0.105150\n",
      "Train             0.810864             0.824481  0.829074      0.998679\n",
      "Training Time    13.326394             0.000932  0.002015      0.104951\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 1\n",
    "learning_curves = dict()\n",
    "scores_tables = OrderedDict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    scores_tables[i] = OrderedDict()\n",
    "    learning_curves[i] = OrderedDict()\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        print(\"K:\\t{}\".format(k+1))\n",
    "        scores_tables[i][k] = OrderedDict()\n",
    "        start = time.time()\n",
    "        LSMR, score_vect_dicts, training_curve = get_score_vects(\n",
    "            df.loc[train_index], random_state=i, alpha=1e-5, iterations=50)\n",
    "        regressor, classifier = fit(LSMR, score_vect_dicts, random_state=i)\n",
    "        trat = time.time()- start\n",
    "        \n",
    "        test_data = preprocess_data(df.loc[test_index])\n",
    "        _ = time.time()\n",
    "        preds, true = predict(test_data, score_vect_dicts, regressor, classifier)\n",
    "        tet = time.time()-_\n",
    "        \n",
    "        _ = time.time()\n",
    "        preds_train, true_train = predict(preprocess_data(df.loc[train_index]),\n",
    "                                          score_vect_dicts,\n",
    "                                          regressor, classifier)\n",
    "        predtra = time.time()-_\n",
    "        \n",
    "        _ = time.time()\n",
    "        preds_tr, true_tr = predict(tronly_test, score_vect_dicts, regressor, classifier)\n",
    "        trt = time.time()-_\n",
    "        \n",
    "        elapsed = time.time()-start\n",
    "        \n",
    "        s = distance_accuracy(true, preds)\n",
    "        s_train = distance_accuracy(true_train, preds_train)\n",
    "        s_tr = distance_accuracy(true_tr, preds_tr)\n",
    "        \n",
    "        \n",
    "        lr = LogisticRegression(random_state=i)\n",
    "        mlp = MLPClassifier(random_state=i)\n",
    "        rf = RandomForestClassifier(random_state=i,n_jobs=-1)\n",
    "        train_mat = np.array(list(LSMR[\"rev_vec\"]))\n",
    "        test_mat = np.array(list(test_data[\"rev_vec\"]))\n",
    "        tronly_mat = np.array(list(tronly_test[\"rev_vec\"]))\n",
    "        \n",
    "        evals = OrderedDict()\n",
    "        evals[\"Train\"] = s_train\n",
    "        evals[\"Test\"] = s\n",
    "        evals[\"Tr. Only\"] = s_tr\n",
    "        evals[\"Training Time\"] = trat\n",
    "        evals[\"Pred.Tra. Time\"] = predtra\n",
    "        evals[\"Testing Time\"] = tet\n",
    "        evals[\"Tr.Test Time\"] = trt\n",
    "        scores_tables[i][k][\"DeepSelect\"] = evals\n",
    "        scores_tables[i][k][\"MLP\"] = eval_models(\n",
    "            mlp, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "        scores_tables[i][k][\"Logistic Regression\"] = eval_models(\n",
    "            lr, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "        scores_tables[i][k][\"RandomForest\"] = eval_models(\n",
    "            rf, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "        \n",
    "        print()\n",
    "        print(\"K:\\t{}\".format(k+1))\n",
    "        print(pd.DataFrame(scores_tables[i][k]))\n",
    "        print(\"\\nThis fold took:\", elapsed, \"seconds\\n\")\n",
    "        learning_curves[i][k] = training_curve\n",
    "        k += 1\n",
    "        print(\"*\"*10+\"\\n\")\n",
    "    print(\"Average scores for trial {}\".format(i))\n",
    "    print(get_trial_score(scores_tables[i]))\n",
    "    print(\"-\"*30)\n",
    "print(\"%%\"*20)\n",
    "print(\"Average of {} trials\".format(NUM_TRIALS))\n",
    "print(get_total_average(scores_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scores_tables, open(\"batch_no_tf_tables.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the full network for prediction\n",
    "### P.S. this variation supports online (incremental) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(LSMR):\n",
    "    X = dict()\n",
    "    y = dict()\n",
    "    for _, row in LSMR.iterrows():\n",
    "        score = row[\"Score\"]\n",
    "        y_ = np.zeros(10)\n",
    "        y_[score-1] = 1\n",
    "        y[len(y)] = y_\n",
    "        X[len(X)] = row[\"rev_vec\"]\n",
    "    return np.array(list(X.values())), np.array(list(y.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_selective(df_train,epochs=100, learning_rate = 0.1, random_state=42, p_every=10):\n",
    "    LSMR_train = preprocess_data(df_train)\n",
    "    np.random.seed(random_state)\n",
    "    data_dict, L1, L2, L3 = get_data_dict(LSMR_train, get_L2and3=True)\n",
    "    init_weights = lambda layer, i, o: {k:2*np.random.random((i, o))-1 for k in layer}\n",
    "    W1 = init_weights(L1, 300, 300)  # (languge, score, movie_id)\n",
    "    W2 = init_weights(L2, 300, 300)  # (languge, score):\n",
    "    W3 = init_weights(L3, 300, 10)  # score:\n",
    "    \n",
    "    \n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    training_curve = dict()\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for e in range(epochs+1):\n",
    "                avg_cost = 0.\n",
    "                for _, row in LSMR_train.iterrows():\n",
    "                    lang = row[\"Language\"]\n",
    "                    movie_id = row[\"Movie_ID\"]\n",
    "                    score = row[\"Score\"]\n",
    "                    y_ = np.zeros(10)\n",
    "                    y_[score-1] = 1\n",
    "                    y_ = np.atleast_2d(y_)\n",
    "                    x_ = np.atleast_2d(row[\"rev_vec\"])\n",
    "                    w1_,w2_,w3_,_, c = sess.run([w1, w2, w3, optimizer, cost],\n",
    "                                             feed_dict={x: x_,\n",
    "                                                        y: y_,\n",
    "                                                        w1:W1[(lang, score, movie_id)],\n",
    "                                                        w2:W2[(lang, score)],\n",
    "                                                        w3:W3[score]})\n",
    "                    W1[(lang, score, movie_id)] = w1_\n",
    "                    W2[(lang, score)] = w2_\n",
    "                    W3[score] = w3_\n",
    "\n",
    "                    avg_cost += c\n",
    "                training_curve[e] = avg_cost\n",
    "                if e%p_every==0:\n",
    "                    print(\"Epoch {}: {}\".format(e, avg_cost/len(LSMR_train)))\n",
    "\n",
    "            return W1, W2, W3, training_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(array):\n",
    "    indx = None\n",
    "    max_ = float(\"-inf\")\n",
    "    for i, e in enumerate(array):\n",
    "        if e > max_:\n",
    "            max_ = e\n",
    "            indx = i\n",
    "    return indx, max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_selective(df, W1, W2, W3):\n",
    "    LSMR = preprocess_data(df)\n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction = tf.argmax(pred, 1)\n",
    "    preds = np.zeros(len(LSMR))\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            j = 0\n",
    "            for _, row in LSMR.iterrows():\n",
    "                v = row[\"rev_vec\"]\n",
    "                predicted_scores = np.zeros(len(W1))\n",
    "                for i, info in enumerate(W1):\n",
    "                    language, score, movie_id = info\n",
    "                    w_1 = W1[(language, score, movie_id)]\n",
    "                    w_2 = W2[(language, score)]\n",
    "                    w_3 = W3[score]\n",
    "\n",
    "                    predicted_scores[i] = prediction.eval({x: np.atleast_2d(v),\n",
    "                                                           w1:w_1,w2:w_2,w3:w_3})\n",
    "\n",
    "                max_index, probability = get_max_index(softmax(predicted_scores))\n",
    "                predicted_score = predicted_scores[max_index]\n",
    "\n",
    "                preds[j] = predicted_score\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    return preds, np.array(list(LSMR.Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "K: \t1\n",
      "Epoch 0: 2.4471188724549733\n",
      "Epoch 25: 0.006596818068616914\n",
      "Epoch 50: 0.0031276120019916014\n",
      "Epoch 75: 0.0021126713735146138\n",
      "Epoch 100: 0.0016174126017723211\n",
      "Epoch 125: 0.0013207504388533715\n",
      "Epoch 150: 0.0011218209131420556\n",
      "Took: 110.28929853439331 for training\n",
      "Took: 284.2523398399353 for predicting 599 training instances\n",
      "Took: 124.47869563102722 for predicting 301 test instances\n",
      "Took: 44.85774111747742 for predicting 100 Turkish test instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t1\n",
      "                DeepSelect  LogisticRegression       MLP  RandomForest\n",
      "Pred.Tra. Time  284.252340            0.108570  1.225991      0.115163\n",
      "Test              0.816279            0.792691  0.791362      0.787708\n",
      "Testing Time    124.478696            0.000349  0.000822      0.105977\n",
      "Tr. Only          0.787143            0.788571  0.767143      0.762857\n",
      "Tr.Test Time     44.857741            0.000164  0.000372      0.105610\n",
      "Train             0.829716            0.831052  0.881970      0.999499\n",
      "Training Time   110.289299            0.000960  0.001627      0.107306\n",
      "took: 564.4611206054688 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K: \t2\n",
      "Epoch 0: 1.9702802376131876\n",
      "Epoch 25: 0.003982249622973389\n",
      "Epoch 50: 0.0020096824128224473\n",
      "Epoch 75: 0.0013711657275957616\n",
      "Epoch 100: 0.0010502235254951605\n",
      "Epoch 125: 0.0008555901006197549\n",
      "Epoch 150: 0.0007243417852619416\n",
      "Took: 112.15304160118103 for training\n",
      "Took: 302.92478942871094 for predicting 600 training instances\n",
      "Took: 150.4370892047882 for predicting 300 test instances\n",
      "Took: 51.68769574165344 for predicting 100 Turkish test instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t2\n",
      "                DeepSelect  LogisticRegression       MLP  RandomForest\n",
      "Pred.Tra. Time  302.924789            0.112297  0.930947      0.110616\n",
      "Test              0.818667            0.806333  0.804333      0.797333\n",
      "Testing Time    150.437089            0.000332  0.000830      0.106021\n",
      "Tr. Only          0.787143            0.774286  0.768571      0.755714\n",
      "Tr.Test Time     51.687696            0.000129  0.000377      0.105570\n",
      "Train             0.828500            0.823667  0.889833      0.997167\n",
      "Training Time   112.153042            0.000661  0.001640      0.103784\n",
      "took: 617.757826089859 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K: \t3\n",
      "Epoch 0: 1.7132777045612988\n",
      "Epoch 25: 0.003901430707642604\n",
      "Epoch 50: 0.001965758791003719\n",
      "Epoch 75: 0.001346215491070591\n",
      "Epoch 100: 0.0010346692861813319\n",
      "Epoch 125: 0.0008454406521583941\n",
      "Epoch 150: 0.0007176313742450183\n",
      "Took: 119.78052258491516 for training\n",
      "Took: 306.9044768810272 for predicting 601 training instances\n",
      "Took: 157.57751441001892 for predicting 299 test instances\n",
      "Took: 50.66577363014221 for predicting 100 Turkish test instances\n",
      "\n",
      "K:\t3\n",
      "                DeepSelect  LogisticRegression       MLP  RandomForest\n",
      "Pred.Tra. Time  306.904477            0.119007  0.217611      0.110547\n",
      "Test              0.840803            0.820736  0.820067      0.812040\n",
      "Testing Time    157.577514            0.000361  0.000802      0.104286\n",
      "Tr. Only          0.787143            0.787143  0.807143      0.770000\n",
      "Tr.Test Time     50.665774            0.000165  0.000372      0.105650\n",
      "Train             0.817471            0.818136  0.812146      0.998835\n",
      "Training Time   119.780523            0.000731  0.001646      0.102493\n",
      "took: 635.5117347240448 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "Average scores for trial 0\n",
      "                DeepSelect  LogisticRegression       MLP  RandomForest\n",
      "Pred.Tra. Time  298.027202            0.113291  0.791516      0.112109\n",
      "Test              0.825249            0.806587  0.805254      0.799027\n",
      "Testing Time    144.164433            0.000348  0.000818      0.105428\n",
      "Tr. Only          0.787143            0.783333  0.780952      0.762857\n",
      "Tr.Test Time     49.070403            0.000153  0.000374      0.105610\n",
      "Train             0.825229            0.824285  0.861317      0.998500\n",
      "Training Time   114.074288            0.000784  0.001638      0.104528\n",
      "------------------------------\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Average of 1 trials\n",
      "                DeepSelect  LogisticRegression       MLP  RandomForest\n",
      "Pred.Tra. Time  298.027202            0.113291  0.791516      0.112109\n",
      "Test              0.825249            0.806587  0.805254      0.799027\n",
      "Testing Time    144.164433            0.000348  0.000818      0.105428\n",
      "Tr. Only          0.787143            0.783333  0.780952      0.762857\n",
      "Tr.Test Time     49.070403            0.000153  0.000374      0.105610\n",
      "Train             0.825229            0.824285  0.861317      0.998500\n",
      "Training Time   114.074288            0.000784  0.001638      0.104528\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 1\n",
    "learning_curves = OrderedDict()\n",
    "scores_tables_nn = OrderedDict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    learning_curves[i] = OrderedDict()\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    scores_tables_nn[i] = dict()\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        print(\"K: \\t{}\".format(k+1))\n",
    "        scores_tables_nn[i][k] = OrderedDict()\n",
    "        start = time.time()\n",
    "        # approx 3 epochs per second\n",
    "        LSMR = preprocess_data(df.loc[train_index])\n",
    "        W1, W2, W3, training_curve = train_selective(df.loc[train_index], epochs=150, p_every=25)\n",
    "        _ = time.time()\n",
    "        trat = _-start\n",
    "        print(\"Took: {} for training\".format(trat))\n",
    "        \n",
    "        _ = time.time()\n",
    "        preds_train, true_train = predict_selective(df.loc[train_index], W1, W2, W3)\n",
    "        predtra = time.time()-_\n",
    "        print(\"Took: {} for predicting {} training instances\".format(predtra, len(train_index)))\n",
    "        \n",
    "        test_data = preprocess_data(df.loc[test_index])\n",
    "        _ = time.time()\n",
    "        preds, true = predict_selective(df.loc[test_index], W1, W2, W3)\n",
    "        tet = time.time()-_\n",
    "        print(\"Took: {} for predicting {} test instances\".format(tet, len(test_index)))\n",
    "        \n",
    "        _ = time.time()\n",
    "        preds_tr, true_tr = predict_selective(tronly_test_raw, W1, W2, W3)\n",
    "        trt = time.time()-_\n",
    "        print(\"Took: {} for predicting {} Turkish test instances\".format(trt, len(tronly_test)))\n",
    "        \n",
    "        elapsed = time.time()-start\n",
    "\n",
    "        s = distance_accuracy(true, preds)\n",
    "        s_train = distance_accuracy(true_train, preds_train)\n",
    "        s_tr = distance_accuracy(true_tr, preds_tr)\n",
    "        mlp = MLPClassifier(random_state=i)\n",
    "        lr = LogisticRegression(random_state=i)\n",
    "        rf = RandomForestClassifier(random_state=i,n_jobs=-1)\n",
    "        train_mat = np.array(list(LSMR[\"rev_vec\"]))\n",
    "        test_mat = np.array(list(test_data[\"rev_vec\"]))\n",
    "        tronly_mat = np.array(list(tronly_test[\"rev_vec\"]))\n",
    "        \n",
    "        evals = OrderedDict()\n",
    "        evals[\"Train\"] = s_train\n",
    "        evals[\"Test\"] = s\n",
    "        evals[\"Tr. Only\"] = s_tr\n",
    "        evals[\"Training Time\"] = trat\n",
    "        evals[\"Pred.Tra. Time\"] = predtra\n",
    "        evals[\"Testing Time\"] = tet\n",
    "        evals[\"Tr.Test Time\"] = trt\n",
    "        scores_tables_nn[i][k][\"DeepSelect\"] = evals\n",
    "        \n",
    "        scores_tables_nn[i][k][\"LogisticRegression\"] = eval_models(lr, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "        scores_tables_nn[i][k][\"MLP\"] = eval_models(mlp, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "        scores_tables_nn[i][k][\"RandomForest\"] = eval_models(rf, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "        \n",
    "        print()\n",
    "        print(pd.DataFrame(scores_tables_nn[i][k]))\n",
    "        print(\"took:\", elapsed, \"seconds\\n\")\n",
    "        learning_curves[i][k] = training_curve\n",
    "        k += 1\n",
    "        print(\"*\"*10+\"\\n\")\n",
    "    print(\"Average scores for trial {}\".format(i))\n",
    "    print(get_trial_score(scores_tables_nn[i]))\n",
    "    print(\"-\"*30)\n",
    "print(\"%%\"*20)\n",
    "print(\"Average of {} trials\".format(NUM_TRIALS))\n",
    "print(get_total_average(scores_tables_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scores_tables_nn, open(\"incremental_tf_tables.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO CHEAT LIKE A PRO\n",
    "# \"\"\"\n",
    "# def test_selective(df_test, W1, W2, W3):\n",
    "#     reset_graph()\n",
    "#     x = tf.placeholder(tf.float32, [None, 300])\n",
    "#     y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "#     w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "#     b1 = tf.Variable(tf.zeros([300]))\n",
    "#     b2 = tf.Variable(tf.zeros([300]))\n",
    "#     b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#     l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "#     l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "#     pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "    \n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#     instance_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#         # Testing the model\n",
    "#         LSMR_test = preprocess_data(df_test)\n",
    "#         X_test, y_test = get_test(LSMR_test)\n",
    "#         accuracy = 0.\n",
    "#         for i in range(len(X_test)):\n",
    "#             best_instance_accuracy = float(\"-inf\")\n",
    "#             for language, score, movie_id in W1:\n",
    "#                 w_1 = W1[(language, score, movie_id)]\n",
    "#                 w_2 = W2[(language, score)]\n",
    "#                 w_3 = W3[score]\n",
    "#                 a = instance_accuracy.eval({x: np.atleast_2d(X_test[i]), y: np.atleast_2d(y_test[i]),\n",
    "#                                    w1:w_1,\n",
    "#                                    w2:w_2,\n",
    "#                                    w3:w_3})\n",
    "#                 if a > best_instance_accuracy:\n",
    "#                     best_instance_accuracy = a\n",
    "#             accuracy += best_instance_accuracy\n",
    "\n",
    "#     return accuracy/len(X_test)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-layer NN > needs at least 3 days for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu is a must\n",
    "def train_deep(df_train, epochs=100, learning_rate=0.1, random_state=42):\n",
    "    LSMR_train = preprocess_data(df_train)\n",
    "    np.random.seed(random_state)\n",
    "    data_dict, L1, L2, L3 = get_data_dict(LSMR_train, get_L2and3=True)\n",
    "    init_weights = lambda layer, i, o: {k:2*np.random.random((i, o))-1 for k in layer}\n",
    "    W1 = init_weights(L1, 300, 300)  # (languge, score, movie_id)\n",
    "    W2 = init_weights(L2, 300, 300)  # (languge, score):\n",
    "    W3 = init_weights(L3, 300, 10)  # score:\n",
    "    \n",
    "    \n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.Variable(tf.zeros([300, 300]))\n",
    "    w2 = tf.Variable(tf.zeros([300, 300]))\n",
    "    w3 = tf.Variable(tf.zeros([300, 10]))\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    training_curve = dict()\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for e in range(epochs+1):\n",
    "                start = time.time()\n",
    "                avg_cost = 0.\n",
    "                for _, row in LSMR_train.iterrows():\n",
    "                    score = row[\"Score\"]\n",
    "                    y_ = np.zeros(10)\n",
    "                    y_[score-1] = 1\n",
    "                    y_ = np.atleast_2d(y_)\n",
    "                    x_ = np.atleast_2d(row[\"rev_vec\"])\n",
    "                    w_1, w_2, w_3 , _, c = sess.run([w1, w2, w3, optimizer, cost], feed_dict={x: x_,y: y_})               \n",
    "                    avg_cost += c\n",
    "                avg_cost /= len(LSMR_train)\n",
    "                training_curve[e] = (avg_cost, time.time()-start)\n",
    "                if e%10==0:\n",
    "                    print(\"Epoch {}: {}\".format(e, avg_cost))\n",
    "\n",
    "    return w_1, w_2, w_3, training_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_deep(df_test, w_1, w_2, w_3):\n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Testing the model\n",
    "            LSMR_test = preprocess_data(df_test)\n",
    "            X_test, y_test = get_test(LSMR_test)\n",
    "            return accuracy.eval({x: X_test,\n",
    "                                  y: y_test,\n",
    "                                  w1:w_1,w2:w_2,\n",
    "                                  w3:w_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 1\n",
    "scores_incremental = dict()\n",
    "learning_curves = dict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    scores_incremental[i] = dict()\n",
    "    learning_curves[i] = dict()\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        start = time.time()\n",
    "        w1, w2, w3, learning_curve = train_deep(df.loc[train_index], random_state=i, epochs=10000)\n",
    "        s = test_deep(df.loc[test_index], w1, w2, w3)\n",
    "        k += 1\n",
    "        print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "        print(\"took:\", time.time()-start)\n",
    "        scores_incremental[i][k] = s\n",
    "        learning_curves[i][k] = learning_curve\n",
    "    print(\"*\"*10)\n",
    "    try:\n",
    "        print(\"Trial {} avg score:\\t {}\".format(i+1, np.mean(list(scores_incremental[i].values()))))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
