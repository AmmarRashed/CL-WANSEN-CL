{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle, pandas as pd, re, numpy as np, ast, warnings\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import chain, starmap\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>i love science fiction and i hate superheroes ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>the movie is absolutely incredible all the per...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>in a cinematic era dominated by reboots and mi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>movie review on rise of the planet of the apes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>during experiments to find a cure for alzheime...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language    Movie_ID                                             Review  \\\n",
       "0       en  -800777728  i love science fiction and i hate superheroes ...   \n",
       "1       en  -800777728  the movie is absolutely incredible all the per...   \n",
       "2       en -1018312192  in a cinematic era dominated by reboots and mi...   \n",
       "3       en -1018312192  movie review on rise of the planet of the apes...   \n",
       "4       en -1018312192  during experiments to find a cure for alzheime...   \n",
       "\n",
       "   Score  \n",
       "0      9  \n",
       "1     10  \n",
       "2      8  \n",
       "3      4  \n",
       "4      7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = pd.read_csv(\"../datasets/movie_data.csv\")\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language  Movie_ID  Review\n",
       "Score                            \n",
       "1            29        29      29\n",
       "2            21        21      21\n",
       "3            14        14      14\n",
       "4            23        23      23\n",
       "5            83        83      83\n",
       "6            43        43      43\n",
       "7            71        71      71\n",
       "8           207       207     207\n",
       "9           175       175     175\n",
       "10          334       334     334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.groupby(\"Score\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../../NLP_data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../../NLP_data/wiki.tr/wiki.tr.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_stemmer = TurkishStemmer()\n",
    "def clean(text, language=\"en\", stem=True):\n",
    "    global turkish_stemmer\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "    \n",
    "    if language == \"tr\":\n",
    "        if stem:\n",
    "            text= ' '.join([turkish_stemmer.stem(w) for w in text.split()])\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r'[0-9]', '#', text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    return TextBlob(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 300\n",
    "def vectorize(text, language):\n",
    "    global VECTOR_SIZE            \n",
    "    blob = clean(text, language)\n",
    "    vector = np.zeros(VECTOR_SIZE)\n",
    "    if len(blob.words) < 1:\n",
    "        return None\n",
    "\n",
    "    for word in blob.words:\n",
    "        try:\n",
    "            if language == \"en\":\n",
    "                vector += globals()[\"en_vects\"][word]\n",
    "            else:\n",
    "                vector += globals()[\"tr_vects\"][word]\n",
    "        except KeyError as e:\n",
    "#             warnings.warn(str(e))\n",
    "            continue\n",
    "    vector /= max(len(blob.words),1)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvec(x):\n",
    "    lang, rev = x.split(\":::::\")\n",
    "    return vectorize(rev, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMSR\n",
    "def preprocess_data(df, language_column=\"Language\", review_column=\"Review\"):\n",
    "    LMSR_df = df.copy()\n",
    "    LMSR_df[\"lang_rev\"] = LMSR_df[[language_column, review_column]].apply(lambda x: x[0]+\":::::\"+x[1], axis=1)\n",
    "    LMSR_df[\"rev_vec\"] = LMSR_df[\"lang_rev\"].apply(lambda x:getvec(x))\n",
    "    LMSR_df.drop([\"lang_rev\", \"Review\"], axis=1, inplace=True)\n",
    "    return LMSR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_accuracy(y_true, y_predict):\n",
    "    res = 0\n",
    "    for i in range(len(y_true)):\n",
    "        res += abs(y_true[i]-y_predict[i])\n",
    "    return 1-res/(len(y_true)*len(set(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XYy(LMSR):\n",
    "    X = np.zeros((len(LMSR), VECTOR_SIZE))\n",
    "    Y = np.zeros((len(LMSR), VECTOR_SIZE))\n",
    "    y = np.zeros((len(LMSR)))\n",
    "    i = 0\n",
    "    for rev in LMSR.iterrows():\n",
    "        score = rev[1][2]\n",
    "        rev_vec = rev[1][3]\n",
    "        score_vec = rev[1][4]\n",
    "\n",
    "        X[i] = rev_vec\n",
    "        Y[i] = score_vec\n",
    "        y[i] = score\n",
    "\n",
    "        i += 1\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derive=False):\n",
    "    if derive:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(df, get_L2and3=False):\n",
    "    data_dict = dict() #{language:{score: {movie_id: [rev1, rev2, ..., revn]}}}\n",
    "    L1 = dict()  # {(languge, score, movie_id): list of reviews with the same score with the same language}\n",
    "    L2 = dict()  # {(language, score): None}\n",
    "    L3 = dict()  # {score: None}\n",
    "    for _, row in df.iterrows():\n",
    "        lang = row[\"Language\"]\n",
    "        movie_id = row[\"Movie_ID\"]\n",
    "        score = row[\"Score\"]\n",
    "        review = row[\"rev_vec\"]\n",
    "\n",
    "        data_dict.setdefault(lang, {})\n",
    "        data_dict[lang].setdefault(score, {})\n",
    "        data_dict[lang][score].setdefault(movie_id, [])\n",
    "        data_dict[lang][score][movie_id].append(review)\n",
    "        \n",
    "        L1.setdefault((lang, score, movie_id), list())\n",
    "        L1[(lang, score, movie_id)].append(review)\n",
    "        if get_L2and3:    \n",
    "            L2[(lang, score)] = None\n",
    "            L3[score] = None\n",
    "    if get_L2and3:\n",
    "        return data_dict, L1, L2, L3\n",
    "    return data_dict, L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L2(LSM_R, data_dict):\n",
    "    L2 = dict()  # {(language, score): list of movies vectors}\n",
    "    for language in data_dict:\n",
    "        for score in data_dict[language]:\n",
    "            for movie_id in data_dict[language][score]:\n",
    "                L2.setdefault((language, score), list())\n",
    "                L2[(language, score)].append(LSM_R[(language, score, movie_id)])\n",
    "    return L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L3(LS_MR, data_dict):\n",
    "    L3 = dict()  # {score: vector of merged languages for that score}\n",
    "    for language in data_dict:\n",
    "        for score in data_dict[language]:\n",
    "            L3.setdefault(score, list())\n",
    "            L3[score].append(LS_MR[(language, score)])\n",
    "    return L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(L, W):\n",
    "    merged = dict()  # {item: vector of merged subitems}\n",
    "    for i, item in enumerate(sorted(L)):\n",
    "        for subitem in L[item]:\n",
    "            merged.setdefault(item, [np.zeros(VECTOR_SIZE),0])\n",
    "            merged[item][0] += sigmoid(subitem.dot(W[i]))\n",
    "            merged[item][1] += 1\n",
    "    for item in merged:\n",
    "        merged[item] = merged[item][0]/ merged[item][1]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(L, delta, W, alpha=0.1):\n",
    "    for i, k in enumerate(sorted(L)):\n",
    "        for l in L[k]:\n",
    "            W[i] += l.T.dot(delta[i]) *alpha\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_error(delta, W):\n",
    "    error = 0\n",
    "    for i in range(len(delta)):\n",
    "        error += delta[i].dot(W[i].T)\n",
    "    return error/len(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_delta(error, layer, size):\n",
    "    delta = np.zeros((size, VECTOR_SIZE))\n",
    "    j = 0\n",
    "    for i,k in enumerate(sorted(layer)):\n",
    "        for l in layer[k]:\n",
    "            delta[j] = error[i]*sigmoid(l, True)\n",
    "            j += 1\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_vects(df, iterations=100, alpha=0.1, random_state=42, W1=None, W2=None, W3=None, W4=None):\n",
    "    LSMR = preprocess_data(df)\n",
    "    data_dict, L1 = get_data_dict(LSMR)\n",
    "    y = softmax(list(LSMR.Score))\n",
    "#     np.random.seed(random_state)\n",
    "    learning_curve = dict()\n",
    "    for i in range(iterations+1):\n",
    "        # forward propagation\n",
    "        if W1 is None:\n",
    "            W1 = 2*np.random.random((len(L1), 300, 300))-1\n",
    "\n",
    "        LSM_R = merge(L1, W1)\n",
    "        L2 = get_L2(LSM_R, data_dict)\n",
    "        if W2 is None:\n",
    "            W2 = 2*np.random.random((len(L2), 300, 300))-1\n",
    "\n",
    "        LS_MR = merge(L2, W2)\n",
    "        L3 = get_L3(LS_MR, data_dict)\n",
    "        if W3 is None:\n",
    "            W3 = 2*np.random.random((len(L3), 300, 300))-1\n",
    "\n",
    "        score_vectors_dict = merge(L3, W3)\n",
    "        l4 = sigmoid(np.array([v for k, v in sorted(score_vectors_dict.items())]))\n",
    "        if W4 is None:\n",
    "            W4 = 2*np.random.random((300, len(LSMR)))-1\n",
    "        \n",
    "        l5 = softmax(l4.dot(W4))  # predicted scores\n",
    "        \n",
    "        # Calculate the error\n",
    "        l5_error = np.mean(np.dot(np.log(l5), y))\n",
    "        \n",
    "        # Back propagation\n",
    "        l5_delta = l5_error * sigmoid(l5, True)\n",
    "        W4 += l4.T.dot(l5_delta)*alpha\n",
    "        \n",
    "        l4_error = l5_delta.dot(W4.T)\n",
    "        l4_delta = l4_error * sigmoid(l4, True)\n",
    "        \n",
    "        W3 = update_weights(L3, l4_delta, W3, alpha)\n",
    "        \n",
    "        l3_error = get_layer_error(l4_delta, W3)\n",
    "        l3_delta = get_layer_delta(l3_error, L3, len(L2))\n",
    "        \n",
    "        W2 = update_weights(L2, l3_delta, W2, alpha)\n",
    "        \n",
    "        l2_error = get_layer_error(l3_delta, W2)\n",
    "        l2_delta = get_layer_delta(l2_error, L2, len(LSMR))\n",
    "        \n",
    "        W1 = update_weights(L1, l2_delta, W1, alpha)\n",
    "        learning_curve[i] = l5_error\n",
    "        if i%10 == 0:\n",
    "            print(\"epoch {}:\\t{}\".format(i, np.abs(l5_error)))\n",
    "        if i%100 == 0:\n",
    "            alpha *= 0.9\n",
    "    return LSMR, score_vectors_dict, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(LSMR, score_vect_dicts,random_state=42, regressor=MLPRegressor(), classifier=MLPClassifier()):\n",
    "    LSMR[\"score_vec\"] = LSMR[\"Score\"].apply(lambda x: score_vect_dicts[x] if x in score_vect_dicts else np.NaN)\n",
    "    LSMR.dropna(inplace=True)\n",
    "    \n",
    "    X, Y, y = get_XYy(LSMR)\n",
    "    \n",
    "    regressor.random_state = random_state\n",
    "    classifier.random_state = random_state\n",
    "        \n",
    "    regressor.fit(X, Y)\n",
    "    classifier.fit(Y, y)\n",
    "    return regressor, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(LSMR, score_vect_dicts, regressor, classifier):\n",
    "    LSMR[\"score_vec\"] = LSMR[\"Score\"].apply(lambda x: score_vect_dicts[x] if x in score_vect_dicts else np.NaN)\n",
    "    LSMR.dropna(inplace=True)\n",
    "    \n",
    "    X, Y, y = get_XYy(LSMR)\n",
    "    \n",
    "    preds_score_vecs = regressor.predict(X)\n",
    "    pred_scores = classifier.predict(preds_score_vecs)\n",
    "    \n",
    "    return pred_scores, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_separate_test_indices(df,size=100):\n",
    "    tr_reviews = df[df.Language==\"tr\"]\n",
    "    classes = list(set(tr_reviews[\"Score\"]))\n",
    "    indices = []\n",
    "    for s in classes:\n",
    "        indices += list(np.random.choice(tr_reviews[tr_reviews.Score==s].index, int(size/len(classes))))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language    0\n",
       "Movie_ID    0\n",
       "Score       0\n",
       "rev_vec     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_test_indices = get_separate_test_indices(df_full)\n",
    "tronly_test_raw = df_full.loc[tr_test_indices]\n",
    "tronly_test = preprocess_data(tronly_test_raw)\n",
    "tronly_test[tronly_test.Language==\"en\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language  Movie_ID  Review\n",
       "Score                            \n",
       "1            11        11      11\n",
       "2            11        11      11\n",
       "4            11        11      11\n",
       "5            11        11      11\n",
       "6            11        11      11\n",
       "7            11        11      11\n",
       "8            11        11      11\n",
       "9            11        11      11\n",
       "10           11        11      11"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tronly_test_raw.groupby(\"Score\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language  Movie_ID  Review\n",
       "Score                            \n",
       "1            22        22      22\n",
       "2            13        13      13\n",
       "3            14        14      14\n",
       "4            18        18      18\n",
       "5            73        73      73\n",
       "6            36        36      36\n",
       "7            62        62      62\n",
       "8           197       197     197\n",
       "9           165       165     165\n",
       "10          323       323     323"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.drop(tr_test_indices).groupby(\"Score\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(model, train, test, tronly, ytrain, ytest, ytronly, LL=False):\n",
    "    _ = time.time()\n",
    "    model.fit(train, ytrain)\n",
    "    predtra = time.time()-_\n",
    "    \n",
    "    _ = time.time()\n",
    "    predtrain = model.predict(train)\n",
    "    trat = time.time()-_\n",
    "    s_train = distance_accuracy(ytrain, predtrain)\n",
    "    f1_train = f1_score(ytrain, predtrain, average='weighted')\n",
    "    \n",
    "    _ = time.time()\n",
    "    predtest= model.predict(test)\n",
    "    tet = time.time()-_\n",
    "    s_test = distance_accuracy(ytest, predtest)\n",
    "    f1_test = f1_score(ytest, predtest, average='weighted')\n",
    "    \n",
    "    _ = time.time()\n",
    "    predtronly = model.predict(tronly)\n",
    "    trt = time.time()-_\n",
    "    s_tr = distance_accuracy(ytronly, predtronly)\n",
    "    f1_tronly = f1_score(ytronly, predtronly, average='weighted')\n",
    "    \n",
    "    evals = OrderedDict()\n",
    "    evals[\"Train\"] = s_train\n",
    "\n",
    "    evals[\"Test\"] = s_test\n",
    "    evals[\"Tr. Only\"] = s_tr\n",
    "    evals[\"Training Time\"] = trat\n",
    "    evals[\"Pred.Tra. Time\"] = predtra\n",
    "    evals[\"Testing Time\"] = tet\n",
    "    evals[\"Tr.Test Time\"] = trt\n",
    "    evals[\"F1 Test\"] = f1_test\n",
    "    evals[\"F1 Train\"] = f1_train\n",
    "    evals[\"F1 Tr. only\"] = f1_tronly\n",
    "    if LL:\n",
    "        evals[\"Train_LL\"] = np.NAN\n",
    "        evals[\"Test_LL\"] = np.NAN\n",
    "        evals[\"Tr. Only_LL\"] = np.NAN\n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_average(scores_tables):\n",
    "#     scores_tables: {i_th trial:\n",
    "#                     {k_th fold:\n",
    "#                         {'Model': {'Test': 0.8090301003344482,\n",
    "#                                    'Train': 0.783361064891847,\n",
    "#                                    'Turkish only': 0.7414285714285714}}}\n",
    "    avgs = dict()\n",
    "    for trial in scores_tables:\n",
    "        for table in scores_tables[trial]:\n",
    "            for model in scores_tables[trial][table]:\n",
    "                avgs.setdefault(model, dict())\n",
    "                for metric, score in scores_tables[trial][table][model].items():\n",
    "                    avgs[model].setdefault(metric, list())\n",
    "                    avgs[model][metric].append(score)\n",
    "    for model in avgs:\n",
    "        for metric in avgs[model]:\n",
    "            avgs[model][metric] = np.mean(avgs[model][metric])\n",
    "    return pd.DataFrame(avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_score(trial_scores_tables):\n",
    "#  trial_scores_tables: {k_th fold:\n",
    "#                             {'Model': {'Test': 0.8090301003344482,\n",
    "#                                        'Train': 0.783361064891847,\n",
    "#                                        'Turkish only': 0.7414285714285714}}}\n",
    "    avgs = dict()\n",
    "    for table in trial_scores_tables:\n",
    "        for model in trial_scores_tables[table]:\n",
    "            avgs.setdefault(model, dict())\n",
    "            for metric, score in trial_scores_tables[table][model].items():\n",
    "                avgs[model].setdefault(metric, list())\n",
    "                avgs[model][metric].append(score)\n",
    "    for model in avgs:\n",
    "        for metric in avgs[model]:\n",
    "            avgs[model][metric] = np.mean(avgs[model][metric])\n",
    "    return pd.DataFrame(avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_selectivewaves_regclass(df, tronly_test_raw, NUM_TRIALS=1, splits=10):  \n",
    "    learning_curves = dict()\n",
    "    scores_tables = OrderedDict()\n",
    "    tronly_test = preprocess_data(tronly_test_raw)\n",
    "    for i in range(NUM_TRIALS):\n",
    "        print(\"Trial:\\t{}\".format(i+1))\n",
    "        scores_tables[i] = OrderedDict()\n",
    "        learning_curves[i] = OrderedDict()\n",
    "        k = 0\n",
    "        skf = StratifiedKFold(n_splits=splits, random_state=i)\n",
    "        for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "            print(\"K:\\t{}\".format(k+1))\n",
    "            scores_tables[i][k] = OrderedDict()\n",
    "            start = time.time()\n",
    "            LSMR, score_vect_dicts, training_curve = get_score_vects(\n",
    "                df.loc[train_index], random_state=i, alpha=1e-5, iterations=50)\n",
    "            regressor, classifier = fit(LSMR, score_vect_dicts, random_state=i)\n",
    "            trat = time.time()- start\n",
    "\n",
    "            test_data = preprocess_data(df.loc[test_index])\n",
    "            _ = time.time()\n",
    "            preds, true = predict(test_data, score_vect_dicts, regressor, classifier)\n",
    "            tet = time.time()-_\n",
    "\n",
    "            _ = time.time()\n",
    "            preds_train, true_train = predict(preprocess_data(df.loc[train_index]),\n",
    "                                              score_vect_dicts,\n",
    "                                              regressor, classifier)\n",
    "            predtra = time.time()-_\n",
    "\n",
    "            _ = time.time()\n",
    "            preds_tr, true_tr = predict(tronly_test, score_vect_dicts, regressor, classifier)\n",
    "            trt = time.time()-_\n",
    "\n",
    "            elapsed = time.time()-start\n",
    "\n",
    "            s = distance_accuracy(true, preds)\n",
    "            f1_test = f1_score(true, preds, average='weighted')\n",
    "\n",
    "            s_train = distance_accuracy(true_train, preds_train)\n",
    "            f1_train = f1_score(true_train, preds_train, average='weighted')\n",
    "\n",
    "            s_tr = distance_accuracy(true_tr, preds_tr)\n",
    "            f1_tronly = f1_test = f1_score(true_tr, preds_tr, average='weighted')        \n",
    "\n",
    "\n",
    "            lr = LogisticRegression(random_state=i)\n",
    "            mlp = MLPClassifier(random_state=i)\n",
    "            rf = RandomForestClassifier(random_state=i,n_jobs=-1)\n",
    "            train_mat = np.array(list(LSMR[\"rev_vec\"]))\n",
    "            test_mat = np.array(list(test_data[\"rev_vec\"]))\n",
    "            tronly_mat = np.array(list(tronly_test[\"rev_vec\"]))\n",
    "\n",
    "            evals = OrderedDict()\n",
    "            evals[\"Train\"] = s_train\n",
    "            evals[\"Test\"] = s\n",
    "            evals[\"Tr. Only\"] = s_tr\n",
    "            evals[\"Training Time\"] = trat\n",
    "            evals[\"Pred.Tra. Time\"] = predtra\n",
    "            evals[\"Testing Time\"] = tet\n",
    "            evals[\"Tr.Test Time\"] = trt\n",
    "            evals[\"F1 Test\"] = f1_test\n",
    "            evals[\"F1 Train\"] = f1_train\n",
    "            evals[\"F1 Tr. only\"] = f1_tronly\n",
    "            scores_tables[i][k][\"DeepSelect\"] = evals\n",
    "            scores_tables[i][k][\"MLP\"] = eval_models(\n",
    "                mlp, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "            scores_tables[i][k][\"Logistic Regression\"] = eval_models(\n",
    "                lr, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "            scores_tables[i][k][\"RandomForest\"] = eval_models(\n",
    "                rf, train_mat, test_mat, tronly_mat, true_train, true, true_tr)\n",
    "\n",
    "            print()\n",
    "            print(\"K:\\t{}\".format(k+1))\n",
    "            print(pd.DataFrame(scores_tables[i][k]))\n",
    "            print(\"\\nThis fold took:\", elapsed, \"seconds\\n\")\n",
    "            learning_curves[i][k] = training_curve\n",
    "            k += 1\n",
    "            print(\"*\"*10+\"\\n\")\n",
    "        print(\"Average scores for trial {}\".format(i))\n",
    "        print(get_trial_score(scores_tables[i]))\n",
    "        print(\"-\"*30)\n",
    "    print(\"%%\"*20)\n",
    "    print(\"Average of {} trials\".format(NUM_TRIALS))\n",
    "    print(get_total_average(scores_tables))\n",
    "    return scores_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "K:\t1\n",
      "epoch 0:\t22.510503812023927\n",
      "epoch 10:\t22.359984764051\n",
      "epoch 20:\t22.178336431334266\n",
      "epoch 30:\t21.98421672483167\n",
      "epoch 40:\t21.81840310737433\n",
      "epoch 50:\t21.679151749629764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t1\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.037876  0.205265             0.196250      0.242046\n",
      "F1 Tr. only       0.037876  0.070707             0.139783      0.793785\n",
      "F1 Train          0.206401  0.331770             0.336056      0.995061\n",
      "Pred.Tra. Time    2.090174  0.596952             0.178907      0.110013\n",
      "Test              0.800000  0.811111             0.802469      0.798765\n",
      "Testing Time      0.010544  0.000305             0.000112      0.104308\n",
      "Tr. Only          0.615039  0.663300             0.645342      0.932660\n",
      "Tr.Test Time      0.016593  0.002251             0.000125      0.102168\n",
      "Train             0.809506  0.823333             0.823086      0.999259\n",
      "Training Time    16.348625  0.002121             0.000747      0.102693\n",
      "\n",
      "This fold took: 18.70770025253296 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t2\n",
      "epoch 0:\t25.23192691134671\n",
      "epoch 10:\t25.111546226023275\n",
      "epoch 20:\t24.94784707635253\n",
      "epoch 30:\t24.77722815051505\n",
      "epoch 40:\t24.616926746149023\n",
      "epoch 50:\t24.46525995631814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t2\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.057656  0.208627             0.217063      0.270889\n",
      "F1 Tr. only       0.057656  0.072192             0.102131      0.757876\n",
      "F1 Train          0.251957  0.334973             0.329319      0.992585\n",
      "Pred.Tra. Time    1.859220  0.369504             0.163771      0.108224\n",
      "Test              0.771111  0.772222             0.773333      0.793333\n",
      "Testing Time      0.010511  0.000385             0.000103      0.105880\n",
      "Tr. Only          0.640853  0.653199             0.618406      0.896745\n",
      "Tr.Test Time      0.014655  0.000328             0.000101      0.105548\n",
      "Train             0.816173  0.829877             0.829506      0.998889\n",
      "Training Time    18.413515  0.001491             0.000696      0.106403\n",
      "\n",
      "This fold took: 20.517911195755005 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t3\n",
      "epoch 0:\t22.436447268536508\n",
      "epoch 10:\t22.235542490165503\n",
      "epoch 20:\t22.039853352697534\n",
      "epoch 30:\t21.87797950755284\n",
      "epoch 40:\t21.769185151509802\n",
      "epoch 50:\t21.627069339758712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t3\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.041579  0.199350             0.178009      0.211884\n",
      "F1 Tr. only       0.041579  0.070113             0.142013      0.774107\n",
      "F1 Train          0.233632  0.318162             0.325155      0.993821\n",
      "Pred.Tra. Time    1.800097  0.398449             0.172492      0.108546\n",
      "Test              0.783951  0.771605             0.756790      0.770370\n",
      "Testing Time      0.009774  0.000307             0.000116      0.104972\n",
      "Tr. Only          0.675645  0.637486             0.654321      0.921437\n",
      "Tr.Test Time      0.013761  0.000316             0.000133      0.105950\n",
      "Train             0.814938  0.826420             0.827531      0.998395\n",
      "Training Time    15.512226  0.001566             0.000720      0.102756\n",
      "\n",
      "This fold took: 17.60788583755493 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t4\n",
      "epoch 0:\t21.072472488204372\n",
      "epoch 10:\t20.857354122296563\n",
      "epoch 20:\t20.783942928783596\n",
      "epoch 30:\t20.724024896437072\n",
      "epoch 40:\t20.66312332324629\n",
      "epoch 50:\t20.602414238381105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t4\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.023061  0.254857             0.255074      0.293663\n",
      "F1 Tr. only       0.023061  0.046172             0.116538      0.710017\n",
      "F1 Train          0.228147  0.299336             0.310901      0.996292\n",
      "Pred.Tra. Time    1.957231  0.729829             0.197230      0.108431\n",
      "Test              0.808889  0.807778             0.805556      0.785556\n",
      "Testing Time      0.010346  0.000301             0.000167      0.101376\n",
      "Tr. Only          0.672278  0.616162             0.634119      0.911336\n",
      "Tr.Test Time      0.030060  0.000313             0.000186      0.101779\n",
      "Train             0.814444  0.821111             0.823457      0.999259\n",
      "Training Time    15.454129  0.001677             0.001490      0.104671\n",
      "\n",
      "This fold took: 17.681356191635132 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t5\n",
      "epoch 0:\t21.422819298272174\n",
      "epoch 10:\t21.32611746487425\n",
      "epoch 20:\t21.20743516078537\n",
      "epoch 30:\t21.11305036905515\n",
      "epoch 40:\t21.01580629289793\n",
      "epoch 50:\t20.919145515622045\n",
      "\n",
      "K:\t5\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.041579  0.289502             0.241477      0.261729\n",
      "F1 Tr. only       0.041579  0.083170             0.110667      0.746682\n",
      "F1 Train          0.227866  0.288964             0.306291      0.996295\n",
      "Pred.Tra. Time    1.890163  0.561500             0.193498      0.108066\n",
      "Test              0.816049  0.809877             0.792593      0.746914\n",
      "Testing Time      0.010593  0.000458             0.000117      0.104885\n",
      "Tr. Only          0.675645  0.658810             0.627385      0.906846\n",
      "Tr.Test Time      0.010783  0.000338             0.000105      0.107012\n",
      "Train             0.812099  0.823827             0.820617      0.999259\n",
      "Training Time    16.245981  0.002217             0.001171      0.103004\n",
      "\n",
      "This fold took: 18.379873037338257 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t6\n",
      "epoch 0:\t24.078565803526825\n",
      "epoch 10:\t23.793053095146483\n",
      "epoch 20:\t23.600216188283955\n",
      "epoch 30:\t23.323714698748848\n",
      "epoch 40:\t23.05575612351863\n",
      "epoch 50:\t22.8027943064411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t6\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.022222  0.228530             0.243285      0.258241\n",
      "F1 Tr. only       0.022222  0.128905             0.151412      0.803979\n",
      "F1 Train          0.172529  0.321539             0.334170      0.991355\n",
      "Pred.Tra. Time    1.820420  0.487417             0.159725      0.107788\n",
      "Test              0.763333  0.790000             0.793333      0.805556\n",
      "Testing Time      0.009893  0.000321             0.000169      0.104748\n",
      "Tr. Only          0.530864  0.654321             0.634119      0.936027\n",
      "Tr.Test Time      0.013615  0.000324             0.000145      0.104308\n",
      "Train             0.795185  0.829630             0.827901      0.998025\n",
      "Training Time    17.281193  0.001624             0.000820      0.102965\n",
      "\n",
      "This fold took: 19.331636667251587 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t7\n",
      "epoch 0:\t23.806425111258786\n",
      "epoch 10:\t23.567025060364045\n",
      "epoch 20:\t23.357586802870028\n",
      "epoch 30:\t23.14264104333667\n",
      "epoch 40:\t22.9793330456179\n",
      "epoch 50:\t22.848342629450606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t7\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.022222  0.261481             0.224081      0.265556\n",
      "F1 Tr. only       0.022222  0.162394             0.146046      0.747278\n",
      "F1 Train          0.173059  0.350501             0.325914      0.991345\n",
      "Pred.Tra. Time    1.768916  0.685073             0.159540      0.107604\n",
      "Test              0.760000  0.787778             0.783333      0.777778\n",
      "Testing Time      0.009205  0.000307             0.000109      0.104600\n",
      "Tr. Only          0.530864  0.666667             0.637486      0.883277\n",
      "Tr.Test Time      0.009610  0.000480             0.000106      0.105215\n",
      "Train             0.795926  0.834198             0.827160      0.998148\n",
      "Training Time    14.111938  0.001530             0.000720      0.103223\n",
      "\n",
      "This fold took: 16.10782241821289 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t8\n",
      "epoch 0:\t21.07345744374721\n",
      "epoch 10:\t20.975878049079437\n",
      "epoch 20:\t20.872030727444887\n",
      "epoch 30:\t20.790724614286646\n",
      "epoch 40:\t20.707588458566455\n",
      "epoch 50:\t20.632080939501886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t8\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.022222  0.219282             0.240992      0.236479\n",
      "F1 Tr. only       0.022222  0.072483             0.147866      0.783121\n",
      "F1 Train          0.159650  0.282132             0.313870      0.993809\n",
      "Pred.Tra. Time    1.693779  0.411098             0.156787      0.108053\n",
      "Test              0.847222  0.816667             0.825000      0.775000\n",
      "Testing Time      0.008955  0.000305             0.000174      0.104977\n",
      "Tr. Only          0.530864  0.676768             0.667789      0.930415\n",
      "Tr.Test Time      0.010864  0.000314             0.000106      0.105683\n",
      "Train             0.782593  0.819383             0.821235      0.998025\n",
      "Training Time    12.893625  0.001531             0.000645      0.104849\n",
      "\n",
      "This fold took: 14.818447351455688 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t9\n",
      "epoch 0:\t20.768665370375896\n",
      "epoch 10:\t20.63657001524127\n",
      "epoch 20:\t20.555603368973525\n",
      "epoch 30:\t20.439006797594548\n",
      "epoch 40:\t20.333181475614595\n",
      "epoch 50:\t20.242126909746645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K:\t9\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.040945  0.185791             0.133072      0.230364\n",
      "F1 Tr. only       0.040945  0.258144             0.139789      0.792474\n",
      "F1 Train          0.178004  0.438538             0.336940      0.995064\n",
      "Pred.Tra. Time    1.699489  0.862772             0.160652      0.107908\n",
      "Test              0.747778  0.768889             0.757778      0.754444\n",
      "Testing Time      0.009321  0.000309             0.000100      0.104976\n",
      "Tr. Only          0.531987  0.698092             0.616162      0.913580\n",
      "Tr.Test Time      0.014431  0.000306             0.000100      0.105034\n",
      "Train             0.796914  0.845679             0.828148      0.998642\n",
      "Training Time    12.881658  0.001490             0.000774      0.103568\n",
      "\n",
      "This fold took: 14.842251062393188 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "K:\t10\n",
      "epoch 0:\t22.1621378865023\n",
      "epoch 10:\t22.07302444164211\n",
      "epoch 20:\t21.963216912790788\n",
      "epoch 30:\t21.905288737613454\n",
      "epoch 40:\t21.85913526233474\n",
      "epoch 50:\t21.822685716264154\n",
      "\n",
      "K:\t10\n",
      "                DeepSelect       MLP  Logistic Regression  RandomForest\n",
      "F1 Test           0.028713  0.338667             0.306383      0.287274\n",
      "F1 Tr. only       0.028713  0.322078             0.106151      0.796376\n",
      "F1 Train          0.190201  0.438964             0.303478      0.995059\n",
      "Pred.Tra. Time    1.701909  1.091351             0.155580      0.107916\n",
      "Test              0.788889  0.793056             0.806944      0.773611\n",
      "Testing Time      0.010212  0.000394             0.000117      0.104171\n",
      "Tr. Only          0.533109  0.762065             0.619529      0.934905\n",
      "Tr.Test Time      0.013329  0.000554             0.000119      0.105046\n",
      "Train             0.790000  0.837284             0.816173      0.998889\n",
      "Training Time    13.164081  0.001519             0.000691      0.103119\n",
      "\n",
      "This fold took: 15.09002423286438 seconds\n",
      "\n",
      "**********\n",
      "\n",
      "Average scores for trial 0\n",
      "                DeepSelect  Logistic Regression       MLP  RandomForest\n",
      "F1 Test           0.033808             0.223569  0.239135      0.255812\n",
      "F1 Tr. only       0.033808             0.130240  0.128636      0.770569\n",
      "F1 Train          0.202145             0.322209  0.340488      0.994069\n",
      "Pred.Tra. Time    1.828140             0.169818  0.619395      0.108255\n",
      "Test              0.788722             0.789713  0.792898      0.778133\n",
      "Testing Time      0.009936             0.000128  0.000339      0.104489\n",
      "Tr. Only          0.593715             0.635466  0.668687      0.916723\n",
      "Tr.Test Time      0.014770             0.000123  0.000553      0.104774\n",
      "Train             0.802778             0.824481  0.829074      0.998679\n",
      "Training Time    15.230697             0.000847  0.001677      0.103725\n",
      "------------------------------\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Average of 1 trials\n",
      "                DeepSelect  Logistic Regression       MLP  RandomForest\n",
      "F1 Test           0.033808             0.223569  0.239135      0.255812\n",
      "F1 Tr. only       0.033808             0.130240  0.128636      0.770569\n",
      "F1 Train          0.202145             0.322209  0.340488      0.994069\n",
      "Pred.Tra. Time    1.828140             0.169818  0.619395      0.108255\n",
      "Test              0.788722             0.789713  0.792898      0.778133\n",
      "Testing Time      0.009936             0.000128  0.000339      0.104489\n",
      "Tr. Only          0.593715             0.635466  0.668687      0.916723\n",
      "Tr.Test Time      0.014770             0.000123  0.000553      0.104774\n",
      "Train             0.802778             0.824481  0.829074      0.998679\n",
      "Training Time    15.230697             0.000847  0.001677      0.103725\n"
     ]
    }
   ],
   "source": [
    "scores_tables = eval_selectivewaves_regclass(df, tronly_test_raw)\n",
    "pickle.dump(scores_tables, open(\"../results/batch_no_tf_tables.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the full network for prediction\n",
    "### P.S. this variation supports online (incremental) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(LSMR):\n",
    "    X = dict()\n",
    "    y = dict()\n",
    "    for _, row in LSMR.iterrows():\n",
    "        score = row[\"Score\"]\n",
    "        y_ = np.zeros(10)\n",
    "        y_[score-1] = 1\n",
    "        y[len(y)] = y_\n",
    "        X[len(X)] = row[\"rev_vec\"]\n",
    "    return np.array(list(X.values())), np.array(list(y.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_selective(df_train,epochs=100, learning_rate = 0.1, random_state=42, p_every=10):\n",
    "    classes = sorted(set(df_train[\"Score\"]))\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    LSMR_train = preprocess_data(df_train)\n",
    "    np.random.seed(random_state)\n",
    "    data_dict, L1, L2, L3 = get_data_dict(LSMR_train, get_L2and3=True)\n",
    "    init_weights = lambda layer, i, o: {k:2*np.random.random((i, o))-1 for k in layer}\n",
    "    W1 = init_weights(L1, 300, 300)  # (languge, score, movie_id)\n",
    "    W2 = init_weights(L2, 300, 300)  # (languge, score):\n",
    "    W3 = init_weights(L3, 300, n_classes)  # score:\n",
    "    \n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, n_classes])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    training_curve = dict()\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for e in range(epochs+1):\n",
    "                avg_cost = 0.\n",
    "                for _, row in LSMR_train.iterrows():\n",
    "                    lang = row[\"Language\"]\n",
    "                    movie_id = row[\"Movie_ID\"]\n",
    "                    score = row[\"Score\"]\n",
    "                    y_ = {i:0 for i in classes}\n",
    "                    y_[score] = 1\n",
    "                    y_ = np.atleast_2d([y_[i] for i in sorted(classes)])\n",
    "                    x_ = np.atleast_2d(row[\"rev_vec\"])\n",
    "                    w1_,w2_,w3_,_, c = sess.run([w1, w2, w3, optimizer, cost],\n",
    "                                             feed_dict={x: x_,\n",
    "                                                        y: y_,\n",
    "                                                        w1:W1[(lang, score, movie_id)],\n",
    "                                                        w2:W2[(lang, score)],\n",
    "                                                        w3:W3[score]})\n",
    "                    W1[(lang, score, movie_id)] = w1_\n",
    "                    W2[(lang, score)] = w2_\n",
    "                    W3[score] = w3_\n",
    "\n",
    "                    avg_cost += c\n",
    "                training_curve[e] = avg_cost\n",
    "                if e%p_every==0:\n",
    "                    print(\"Epoch {}: {}\".format(e, avg_cost/len(LSMR_train)))\n",
    "\n",
    "            return W1, W2, W3, training_curve, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(predicted_scores, classes):\n",
    "    probs = {i:0 for i in classes}\n",
    "    array = softmax(predicted_scores)\n",
    "    indx = None\n",
    "    max_ = float(\"-inf\")\n",
    "    for i, e in enumerate(array):\n",
    "        probs[predicted_scores[i]] = e\n",
    "        if e > max_:\n",
    "            max_ = e\n",
    "            indx = i\n",
    "    return predicted_scores[indx], [probs[i] for i in sorted(classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_selective(df, W1, W2, W3, classes=list(range(1,11))):\n",
    "    LSMR = preprocess_data(df)\n",
    "    reset_graph()\n",
    "    n_classes = len(classes)\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, n_classes])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction = tf.argmax(pred, 1)\n",
    "    preds = np.zeros(len(LSMR))\n",
    "    probs = [None] * len(LSMR)\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            j = 0\n",
    "            for _, row in LSMR.iterrows():\n",
    "                v = row[\"rev_vec\"]\n",
    "                predicted_scores = np.zeros(len(W1))\n",
    "                for i, info in enumerate(W1):\n",
    "                    language, score, movie_id = info\n",
    "                    w_1 = W1[(language, score, movie_id)]\n",
    "                    w_2 = W2[(language, score)]\n",
    "                    w_3 = W3[score]\n",
    "\n",
    "                    predicted_scores[i] = prediction.eval({x: np.atleast_2d(v),\n",
    "                                                           w1:w_1,w2:w_2,w3:w_3})\n",
    "                predicted_score, probabilities = get_max_index(predicted_scores, classes)\n",
    "                preds[j] = predicted_score\n",
    "                probs[j] = probabilities\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    return preds, np.array(list(LSMR.Score)), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_selectivewaves_nn(df, tronly_test_raw, NUM_TRIALS=1, splits=10):  \n",
    "    learning_curves = OrderedDict()\n",
    "    scores_tables_nn = OrderedDict()\n",
    "    tronly_test = preprocess_data(tronly_test_raw)\n",
    "    for i in range(NUM_TRIALS):\n",
    "        print(\"Trial:\\t{}\".format(i+1))\n",
    "        learning_curves[i] = OrderedDict()\n",
    "        k = 0\n",
    "        skf = StratifiedKFold(n_splits=splits, random_state=i)\n",
    "        scores_tables_nn[i] = dict()\n",
    "        for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "            print(\"K: \\t{}\".format(k+1))\n",
    "            scores_tables_nn[i][k] = OrderedDict()\n",
    "            start = time.time()\n",
    "            # approx 3 epochs per second\n",
    "            LSMR = preprocess_data(df.loc[train_index])\n",
    "            W1, W2, W3, training_curve, classes = train_selective(df.loc[train_index], epochs=150, p_every=25)\n",
    "            _ = time.time()\n",
    "            trat = _-start\n",
    "            print(\"Took: {} for training\".format(trat))\n",
    "\n",
    "            _ = time.time()\n",
    "            preds_train, true_train, probs = predict_selective(df.loc[train_index], W1, W2, W3, classes=classes)\n",
    "            ll_train = log_loss(true_train,probs, labels=classes)\n",
    "            predtra = time.time()-_\n",
    "            print(\"Took: {} for predicting {} training instances\".format(predtra, len(train_index)))\n",
    "\n",
    "            test_data = preprocess_data(df.loc[test_index])\n",
    "            _ = time.time()\n",
    "            preds, true, probs = predict_selective(df.loc[test_index], W1, W2, W3, classes=classes)\n",
    "            ll_test = log_loss(true,probs, labels=classes)\n",
    "            tet = time.time()-_\n",
    "            print(\"Took: {} for predicting {} test instances\".format(tet, len(test_index)))\n",
    "\n",
    "            _ = time.time()\n",
    "            preds_tr, true_tr, probs = predict_selective(tronly_test_raw, W1, W2, W3, classes=classes)\n",
    "            ll_tr = log_loss(true_tr,probs, labels=classes)\n",
    "            trt = time.time()-_\n",
    "            print(\"Took: {} for predicting {} Turkish test instances\".format(trt, len(tronly_test)))\n",
    "\n",
    "            elapsed = time.time()-start\n",
    "\n",
    "            s = distance_accuracy(true, preds)\n",
    "            s_train = distance_accuracy(true_train, preds_train)\n",
    "            s_tr = distance_accuracy(true_tr, preds_tr)\n",
    "\n",
    "            f1_test = f1_score(true, preds, average='weighted')\n",
    "            f1_train = f1_score(true_train, preds_train, average='weighted')\n",
    "            f1_tronly = f1_score(true_tr, preds_tr, average='weighted')\n",
    "\n",
    "            mlp = MLPClassifier(random_state=i)\n",
    "            lr = LogisticRegression(random_state=i)\n",
    "            rf = RandomForestClassifier(random_state=i,n_jobs=-1)\n",
    "            train_mat = np.array(list(LSMR[\"rev_vec\"]))\n",
    "            test_mat = np.array(list(test_data[\"rev_vec\"]))\n",
    "            tronly_mat = np.array(list(tronly_test[\"rev_vec\"]))\n",
    "\n",
    "            evals = OrderedDict()\n",
    "            evals[\"Train\"] = s_train\n",
    "            evals[\"Test\"] = s\n",
    "            evals[\"Tr. Only\"] = s_tr\n",
    "            evals[\"Training Time\"] = trat\n",
    "            evals[\"Pred.Tra. Time\"] = predtra\n",
    "            evals[\"Testing Time\"] = tet\n",
    "            evals[\"Tr.Test Time\"] = trt\n",
    "            evals[\"F1 Test\"] = f1_test\n",
    "            evals[\"F1 Train\"] = f1_train\n",
    "            evals[\"F1 Tr. only\"] = f1_tronly\n",
    "            evals[\"Train_LL\"] = ll_train\n",
    "            evals[\"Test_LL\"] = ll_test\n",
    "            evals[\"Tr. Only_LL\"] = ll_tr\n",
    "            scores_tables_nn[i][k][\"DeepSelect\"] = evals\n",
    "\n",
    "            scores_tables_nn[i][k][\"LogisticRegression\"] = eval_models(\n",
    "                lr, train_mat, test_mat, tronly_mat, true_train, true, true_tr, True)\n",
    "            scores_tables_nn[i][k][\"MLP\"] = eval_models(\n",
    "                mlp, train_mat, test_mat, tronly_mat, true_train, true, true_tr, True)\n",
    "            scores_tables_nn[i][k][\"RandomForest\"] = eval_models(\n",
    "                rf, train_mat, test_mat, tronly_mat, true_train, true, true_tr, True)\n",
    "\n",
    "            print()\n",
    "            print(pd.DataFrame(scores_tables_nn[i][k]))\n",
    "            print(\"took:\", elapsed, \"seconds\\n\")\n",
    "            learning_curves[i][k] = training_curve\n",
    "            k += 1\n",
    "            print(\"*\"*10+\"\\n\")\n",
    "        print(\"Average scores for trial {}\".format(i))\n",
    "        print(get_trial_score(scores_tables_nn[i]))\n",
    "        print(\"-\"*30)\n",
    "    print(\"%%\"*20)\n",
    "    print(\"Average of {} trials\".format(NUM_TRIALS))\n",
    "    print(get_total_average(scores_tables_nn))\n",
    "    return scores_tables_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "K: \t1\n"
     ]
    }
   ],
   "source": [
    "scores_tables_nn = eval_selectivewaves_nn(df, tronly_test_raw)\n",
    "pickle.dump(scores_tables_nn, open(\"../results/incremental_tf_tables.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_weighted(true, preds):\n",
    "    return f1_score(true, preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_METRICES = {\"MAE-accuracy\":distance_accuracy, \"f1_score w.avg\":f1_score_weighted}\n",
    "\n",
    "def robustness_test(df_full, separate_test_size=100, metrics=DEFAULT_METRICES):\n",
    "    en_revs = df_full[df_full.Language==\"en\"]\n",
    "    tr_revs = df_full[df_full.Language==\"tr\"]\n",
    "    separate_test_size = min(len(tr_revs)+1, separate_test_size)\n",
    "    test = tr_revs[-separate_test_size:]\n",
    "    tr_revs = tr_revs[:-separate_test_size]\n",
    "    robustness = dict()  # {(num of en reviews, num of tr reviews): scores_dict}\n",
    "    for en_size in range(1,11):\n",
    "        for tr_size in range(1,11):\n",
    "            en_train = en_revs.sample(frac=en_size/10.0)\n",
    "            tr_train = tr_revs.sample(frac=tr_size/10.0)\n",
    "            start = time.time()\n",
    "            print(\"En: {}\\tTr: {}\".format(len(en_train),len(tr_train)))\n",
    "            train = pd.concat([en_train, tr_train]).reset_index(drop=True)\n",
    "            robustness_tables = dict()\n",
    "            print(\"Using first variation (Regressor and Classifier with score vectors)\")\n",
    "            LSMR, score_vect_dicts, training_curve = get_score_vects(\n",
    "                                            train, alpha=1e-5, iterations=50)\n",
    "            regressor, classifier = fit(LSMR, score_vect_dicts)\n",
    "            test_vecs = preprocess_data(test)\n",
    "            preds, true = predict(test_vecs, score_vect_dicts, regressor, classifier)\n",
    "            robustness_tables[\"DeepSelect (regclass)\"] = dict()\n",
    "            robustness_tables[\"DeepSelect\"] = dict()\n",
    "\n",
    "            print(\"Using second variation (average of outputs produced by each set of weight matrices)\")\n",
    "            W1, W2, W3, training_curve = train_selective(train, epochs=150, p_every=25)\n",
    "            preds_nn, true_nn, probs = predict_selective(test, W1, W2, W3)\n",
    "            ll = log_loss(true_nn,probs, labels=classes)\n",
    "            \n",
    "    #         f1_test_nn = f1_score(true_nn, preds_nn, average='weighted')\n",
    "        \n",
    "            for name, metric in metrics.items():\n",
    "                \n",
    "                s_regclass = metric(true, preds)\n",
    "                s_nn = metric(true_nn, preds_nn)\n",
    "    #         f1_test_regclass = f1_score(true, preds, average='weighted')\n",
    "                robustness_tables[\"DeepSelect (regclass)\"][name] = s_regclass\n",
    "                robustness_tables[\"DeepSelect\"][name] = s_nn\n",
    "            robustness_tables[\"DeepSelect (regclass)\"][\"log loss\"] = np.NAN\n",
    "            robustness_tables[\"DeepSelect\"][\"log loss\"] = ll\n",
    "\n",
    "            print(\"Using well-known algorithms: Logistic Regression, RandomForest and MLP\")\n",
    "            lr = LogisticRegression()\n",
    "            rf = RandomForestClassifier(n_jobs=-1)\n",
    "            mlp = MLPClassifier()\n",
    "\n",
    "            train_mat = np.array(list(LSMR[\"rev_vec\"]))\n",
    "            train_y = np.array(list(LSMR[\"Score\"]))\n",
    "            test_mat = np.array(list(test_vecs[\"rev_vec\"]))\n",
    "\n",
    "            for model_name, model in [(\"Logistic Regression\",lr),\n",
    "                                (\"RandomForest\", rf),\n",
    "                                (\"MLP\", mlp)]:\n",
    "                model.fit(train_mat, train_y)\n",
    "                robustness_tables.setdefault(model_name, dict())\n",
    "                robustness_tables[model_name][\"log loss\"] = np.NAN\n",
    "                for metric_name, metric in metrics.items():\n",
    "                    robustness_tables[model_name][metric_name] = metric(true, model.predict(test_mat))\n",
    "            robustness[(len(en_train),len(tr_train))] = robustness_tables\n",
    "            print(\"Took: {}\".format(time.time()-start))\n",
    "            print(\"-\"*50)\n",
    "    return robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "robustness = robustness_test(df_full)\n",
    "pickle.dump(robustness, open(\"../results/robustness.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grouping labels\n",
    "### label = 1   if review_score > 7\n",
    "### label = 0   if 7 >= review_score >= 4\n",
    "### label = -1  if review_score < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_labels(x):\n",
    "    if x>7:\n",
    "        return 1\n",
    "    elif x>=4:\n",
    "        return 0\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_trilabels = deepcopy(df_full)\n",
    "df_full_trilabels[\"Score\"] = df_full_trilabels.Score.apply(group_labels)\n",
    "df_full_trilabels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_trilabels.groupby(\"Score\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tronly_test_raw_trilabels = df_full_trilabels[-100:]\n",
    "tronly_test_trilabels = preprocess_data(tronly_test_raw_trilabels)\n",
    "df_trilabels = df_full_trilabels[:-100]\n",
    "tronly_test_trilabels[tronly_test_trilabels.Language==\"en\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_tables_tri = eval_selectivewaves_regclass(df_trilabels, tronly_test_raw_trilabels)\n",
    "pickle.dump(scores_tables_tri, open(\"../results/batch_no_tf_tables_tri.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tables_nn_tri = eval_selectivewaves_nn(df_trilabels, tronly_test_raw_trilabels)\n",
    "pickle.dump(scores_tables_nn_tri, open(\"../results/incremental_tf_tables_tri.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustness_tri = robustness_test(df_full_trilabels, accuracy_metric=classification_report)\n",
    "pickle.dump(robustness_tri, open(\"../results/robustness_tri.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO CHEAT LIKE A PRO\n",
    "# \"\"\"\n",
    "# def test_selective(df_test, W1, W2, W3):\n",
    "#     reset_graph()\n",
    "#     x = tf.placeholder(tf.float32, [None, 300])\n",
    "#     y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "#     w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "#     b1 = tf.Variable(tf.zeros([300]))\n",
    "#     b2 = tf.Variable(tf.zeros([300]))\n",
    "#     b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#     l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "#     l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "#     pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "    \n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#     instance_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#         # Testing the model\n",
    "#         LSMR_test = preprocess_data(df_test)\n",
    "#         X_test, y_test = get_test(LSMR_test)\n",
    "#         accuracy = 0.\n",
    "#         for i in range(len(X_test)):\n",
    "#             best_instance_accuracy = float(\"-inf\")\n",
    "#             for language, score, movie_id in W1:\n",
    "#                 w_1 = W1[(language, score, movie_id)]\n",
    "#                 w_2 = W2[(language, score)]\n",
    "#                 w_3 = W3[score]\n",
    "#                 a = instance_accuracy.eval({x: np.atleast_2d(X_test[i]), y: np.atleast_2d(y_test[i]),\n",
    "#                                    w1:w_1,\n",
    "#                                    w2:w_2,\n",
    "#                                    w3:w_3})\n",
    "#                 if a > best_instance_accuracy:\n",
    "#                     best_instance_accuracy = a\n",
    "#             accuracy += best_instance_accuracy\n",
    "\n",
    "#     return accuracy/len(X_test)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-layer NN > needs at least 3 days for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu is a must\n",
    "def train_deep(df_train, epochs=100, learning_rate=0.1, random_state=42):\n",
    "    LSMR_train = preprocess_data(df_train)\n",
    "    np.random.seed(random_state)\n",
    "    data_dict, L1, L2, L3 = get_data_dict(LSMR_train, get_L2and3=True)\n",
    "    init_weights = lambda layer, i, o: {k:2*np.random.random((i, o))-1 for k in layer}\n",
    "    W1 = init_weights(L1, 300, 300)  # (languge, score, movie_id)\n",
    "    W2 = init_weights(L2, 300, 300)  # (languge, score):\n",
    "    W3 = init_weights(L3, 300, 10)  # score:\n",
    "    \n",
    "    \n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.Variable(tf.zeros([300, 300]))\n",
    "    w2 = tf.Variable(tf.zeros([300, 300]))\n",
    "    w3 = tf.Variable(tf.zeros([300, 10]))\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    training_curve = dict()\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for e in range(epochs+1):\n",
    "                start = time.time()\n",
    "                avg_cost = 0.\n",
    "                for _, row in LSMR_train.iterrows():\n",
    "                    score = row[\"Score\"]\n",
    "                    y_ = np.zeros(10)\n",
    "                    y_[score-1] = 1\n",
    "                    y_ = np.atleast_2d(y_)\n",
    "                    x_ = np.atleast_2d(row[\"rev_vec\"])\n",
    "                    w_1, w_2, w_3 , _, c = sess.run([w1, w2, w3, optimizer, cost], feed_dict={x: x_,y: y_})               \n",
    "                    avg_cost += c\n",
    "                avg_cost /= len(LSMR_train)\n",
    "                training_curve[e] = (avg_cost, time.time()-start)\n",
    "                if e%10==0:\n",
    "                    print(\"Epoch {}: {}\".format(e, avg_cost))\n",
    "\n",
    "    return w_1, w_2, w_3, training_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_deep(df_test, w_1, w_2, w_3):\n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Testing the model\n",
    "            LSMR_test = preprocess_data(df_test)\n",
    "            X_test, y_test = get_test(LSMR_test)\n",
    "            return accuracy.eval({x: X_test,\n",
    "                                  y: y_test,\n",
    "                                  w1:w_1,w2:w_2,\n",
    "                                  w3:w_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 1\n",
    "scores_incremental = dict()\n",
    "learning_curves = dict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    scores_incremental[i] = dict()\n",
    "    learning_curves[i] = dict()\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        start = time.time()\n",
    "        w1, w2, w3, learning_curve = train_deep(df.loc[train_index], random_state=i, epochs=10000)\n",
    "        s = test_deep(df.loc[test_index], w1, w2, w3)\n",
    "        k += 1\n",
    "        print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "        print(\"took:\", time.time()-start)\n",
    "        scores_incremental[i][k] = s\n",
    "        learning_curves[i][k] = learning_curve\n",
    "    print(\"*\"*10)\n",
    "    try:\n",
    "        print(\"Trial {} avg score:\\t {}\".format(i+1, np.mean(list(scores_incremental[i].values()))))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
