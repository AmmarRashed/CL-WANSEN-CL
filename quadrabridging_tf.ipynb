{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import warnings, numpy as np, re, json, pandas as pd, pickle, unicodedata\n",
    "# try:\n",
    "#     import gnumpy as gpu\n",
    "# except ModuleNotFoundError:\n",
    "#     pass\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim, math, time\n",
    "import tensorflow as tf\n",
    "from gensim.models import doc2vec\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from KaggleWord2VecUtility import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>i love science fiction and i hate superheroes ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>the movie is absolutely incredible all the per...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>in a cinematic era dominated by reboots and mi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>movie review on rise of the planet of the apes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>during experiments to find a cure for alzheime...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language    Movie_ID                                             Review  \\\n",
       "0       en  -800777728  i love science fiction and i hate superheroes ...   \n",
       "1       en  -800777728  the movie is absolutely incredible all the per...   \n",
       "2       en -1018312192  in a cinematic era dominated by reboots and mi...   \n",
       "3       en -1018312192  movie review on rise of the planet of the apes...   \n",
       "4       en -1018312192  during experiments to find a cure for alzheime...   \n",
       "\n",
       "   Score  \n",
       "0      9  \n",
       "1     10  \n",
       "2      8  \n",
       "3      4  \n",
       "4      7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/movie_data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"tr\":\"tr2en\", \"en\":\"en2tr\"}\n",
    "en2tr = dict()\n",
    "tr2en = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../NLP_data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../NLP_data/wiki.tr/wiki.tr.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_translation_matrix(X,Y, iterations=5000, alpha=0.0001, alpha_change_rate=0.8):\n",
    "    W = np.random.random((300, 300))\n",
    "    for i in range(iterations+1):\n",
    "        gradient = np.zeros(300)\n",
    "        for score in range(len(X)):\n",
    "            error = X[score].dot(W) - Y[score]\n",
    "            gradient += alpha * np.gradient(error)\n",
    "        W += gradient\n",
    "        if i == 2000:\n",
    "            alpha /= 100\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            alpha *= alpha_change_rate\n",
    "            print(\"Mikolov distance: {}\".format(mikolov(X, Y, W)))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_stemmer = TurkishStemmer()\n",
    "def clean(text, language=\"en\", stem=True):\n",
    "    global turkish_stemmer\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "    \n",
    "    if language == \"tr\":\n",
    "        if stem:\n",
    "            text= ' '.join([turkish_stemmer.stem(w) for w in text.split()])\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r'[0-9]', '#', text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    return TextBlob(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def record_translations():\n",
    "#     for lang in data_dict:\n",
    "#         for score in data_dict[lang]:\n",
    "#             for movie in data_dict[lang][score]:\n",
    "#                 for review in data_dict[lang][score][movie]:\n",
    "#                     try:\n",
    "#                         blob = clean(review)\n",
    "#                         if review in globals()[d[lang]]:\n",
    "#                             ent = globals()[d[lang]][review]\n",
    "#                         else:\n",
    "#                             ent = str(blob.translate(to=d[lang][-2:]))\n",
    "#                             globals()[d[lang]][review] = ent\n",
    "#                     except:\n",
    "#                         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_translations()\n",
    "# pickle.dump(en2tr, open(\"datasets/en2tr\",\"wb\"))\n",
    "# pickle.dump(tr2en, open(\"datasets/tr2en\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en2tr = pickle.load(open(\"datasets/en2tr\",\"rb\"))\n",
    "tr2en = pickle.load(open(\"datasets/tr2en\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 300\n",
    "def vectorize(text, language, translate=False):\n",
    "    global VECTOR_SIZE, en2tr, tr2en\n",
    "    if translate:\n",
    "        if language==\"tr\":\n",
    "            try:\n",
    "                text = en2tr[text]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    translated = str(TextBlob(text).translate(to=language))\n",
    "                except:\n",
    "                    warnings.warn(\"Can't translate invalid English Review.\"+text[:10]+\"...\")\n",
    "                    return None\n",
    "                en2tr[text] = translated\n",
    "                tr2en[translated] = text\n",
    "                text = translated\n",
    "        else:\n",
    "            try:\n",
    "                text = tr2en[text]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    translated = str(TextBlob(text).translate(to=language))\n",
    "                except:\n",
    "                    warnings.warn(\"Can't translate invalid Turkish Review.\"+text[:10]+\"...\")\n",
    "                    return None\n",
    "                tr2en[text] = translated\n",
    "                en2tr[translated] = text\n",
    "                text = translated\n",
    "                \n",
    "    blob = clean(text, language)\n",
    "    vector = np.zeros(VECTOR_SIZE)\n",
    "    if len(blob.words) < 1:\n",
    "        return None\n",
    "\n",
    "    for word in blob.words:\n",
    "        try:\n",
    "            if language == \"en\":\n",
    "                vector += globals()[\"en_vects\"][word]\n",
    "            else:\n",
    "                vector += globals()[\"tr_vects\"][word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vector /= len(blob.words)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_size(data_dict, lang):\n",
    "    res = 0\n",
    "    for score in data_dict[lang]:\n",
    "        for movie in data_dict[lang][score]:\n",
    "                for tr_rev in data_dict[lang][score][movie]:\n",
    "                    res +=1\n",
    "    return res\n",
    "\n",
    "def get_X2_size(data_dict):\n",
    "    res = 0\n",
    "    for score in data_dict[\"tr\"]:\n",
    "        for movie in data_dict[\"tr\"][score]:\n",
    "            try:\n",
    "                for en_rev in data_dict[\"en\"][score][movie]:\n",
    "                    res += 1\n",
    "            except KeyError:  ## there are no english review for that movie with the same score\n",
    "                continue\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(df_train):\n",
    "    data_dict = dict()  # {(score, movie_id): {language: [reviews]}}\n",
    "    for _, row in df_train.iterrows():\n",
    "        lang = row[0]\n",
    "        movie_id = row[1]\n",
    "        review = row[2]\n",
    "        score = row[3]\n",
    "\n",
    "        data_dict.setdefault((score, movie_id), dict())\n",
    "        data_dict[(score, movie_id)].setdefault(lang, list())\n",
    "        data_dict[(score, movie_id)][lang].append(review)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n, num_classes=10):\n",
    "        s = np.zeros(num_classes)\n",
    "        s[n-1] = 1\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array(d):\n",
    "    return np.array([d[i] for i in sorted(d)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = \"en\"\n",
    "TARGET = \"tr\"\n",
    "def get_training_batch(data_dict):\n",
    "    \"\"\"\n",
    "    data_dict: # {(score, movie_id): {language: [reviews]}}\n",
    "    \"\"\"\n",
    "    X1_dict = dict()   # Turkish reviews\n",
    "    X2_dict = dict()   # English translation of the Turkish reviews\n",
    "    X3_dict = dict()   # Turkish translation of the English reviews\n",
    "    X4_dict = dict()   # English reviews\n",
    "    Y_dict = dict()    # score\n",
    "\n",
    "    for i, k in enumerate(data_dict):\n",
    "        # k (score, movie_id)\n",
    "        # data_dict[k] = {language: [reviews]}\n",
    "        try:\n",
    "            source_reviews = data_dict[k][SOURCE]\n",
    "            target_reviews = data_dict[k][TARGET]\n",
    "        except KeyError:\n",
    "            # There is no similar scores for that movie in both languages, alignment not possible\n",
    "            continue\n",
    "        for rev in range(max(len(source_reviews), len(target_reviews))):\n",
    "            X2_ = vectorize(target_reviews[rev%len(target_reviews)], language=SOURCE, translate=True)\n",
    "            X3_ = vectorize(source_reviews[rev%len(source_reviews)], language=TARGET, translate=True)\n",
    "            translation_error = False\n",
    "            for v in [X2_, X3_]:\n",
    "                if v is None:\n",
    "                    translation_error = True\n",
    "                    break\n",
    "            if translation_error: continue\n",
    "            X1_dict[i+rev] = vectorize(target_reviews[rev%len(target_reviews)], language=TARGET)\n",
    "            X2_dict[i+rev] = X2_\n",
    "            X3_dict[i+rev] = X3_\n",
    "            X4_dict[i+rev] = vectorize(source_reviews[rev%len(source_reviews)], language=SOURCE)\n",
    "            Y_dict[i+rev] = one_hot(k[0])\n",
    "    \n",
    "    X1, X2, X3, X4, Y = Parallel(n_jobs=-1)(delayed(dict2array)\n",
    "                                         (v)\n",
    "                                         for v in [X1_dict, X2_dict, X3_dict, X4_dict, Y_dict])\n",
    "    \n",
    "    return X1, X2, X3, X4, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(df_train, learning_rate=0.1, epochs=100):\n",
    "    data_dict = get_data_dict(df_train) # {(score, movie_id): {language: [reviews]}}\n",
    "    training_curve = dict()\n",
    "    X1 = tf.placeholder(tf.float32, [None, 300])\n",
    "    X2 = tf.placeholder(tf.float32, [None, 300])\n",
    "    X3 = tf.placeholder(tf.float32, [None, 300])\n",
    "    X4 = tf.placeholder(tf.float32, [None, 300])\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    W1 = tf.Variable(tf.zeros([300, 300]))\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    \n",
    "    W2 = tf.Variable(tf.zeros([300, 300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    \n",
    "    W3 = tf.Variable(tf.zeros([300, 300]))\n",
    "    b3 = tf.Variable(tf.zeros([300]))\n",
    "    \n",
    "    W4 = tf.Variable(tf.zeros([300, 10]))\n",
    "    b4 = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    \n",
    "    pred = tf.nn.softmax(tf.matmul(X4, W4) + b4) # Softmax\n",
    "    \n",
    "    \n",
    "    cost1 = tf.reduce_sum(tf.square(tf.nn.sigmoid(tf.matmul(X1, W1) + b1 - X2)))\n",
    "    cost2 = tf.reduce_sum(tf.square(tf.nn.sigmoid(tf.matmul(X2, W2) + b2 - X3)))\n",
    "    cost3 = tf.reduce_sum(tf.square(tf.sigmoid(tf.matmul(X3, W3) + b3 - X4)))\n",
    "    cost4 = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(pred), reduction_indices=1))\n",
    "    cost = cost1 + cost2 + cost3 + cost4\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    X1_, X2_, X3_, X4_, Y_ = get_training_batch(data_dict)\n",
    "    assert len(X1_) == len(X2_) == len(X3_) == len(X4_) == len(Y_)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(epochs):\n",
    "            avg_cost = 0.\n",
    "            for i in range(len(X1_)):\n",
    "                _, c, w1, w2, w3, w4 = sess.run([optimizer, cost, W1, W2, W3, W4],\n",
    "                                                feed_dict={X1: np.atleast_2d(X1_[i]),\n",
    "                                                           X2: np.atleast_2d(X2_[i]),\n",
    "                                                           X3: np.atleast_2d(X3_[i]),\n",
    "                                                           X4: np.atleast_2d(X4_[i]),\n",
    "                                                           Y: np.atleast_2d(Y_[i])})\n",
    "                avg_cost += c\n",
    "            avg_cost /= len(df_train)\n",
    "            training_curve[e] = avg_cost\n",
    "            if e%10==0:\n",
    "                print(\"Epoch {}: {}\".format(e, avg_cost/len(df_train)))\n",
    "\n",
    "    return w1, w2, w3, w4, training_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(LSMR, W1, W2, W3, W4):\n",
    "    reset_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w4 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([300]))\n",
    "    b4 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.matmul(x, w1) + b1\n",
    "    l3 = tf.matmul(l2, w2) + b2\n",
    "    l4 = tf.matmul(l3, w3) + b3\n",
    "    pred = tf.nn.softmax(tf.matmul(l4, w4) + b4)\n",
    "    \n",
    "    prediction = tf.argmax(pred, 1)\n",
    "    preds = np.zeros(len(LSMR))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        j = 0\n",
    "        for _, row in LSMR.iterrows():\n",
    "            v = vectorize(row[\"Review\"], row[\"Language\"])\n",
    "            preds[j] = prediction.eval({x: np.atleast_2d(v),\n",
    "                                        w1:W1, w2:W2, w3:W3, w4:W4})\n",
    "            j+=1\n",
    "    \n",
    "    \n",
    "    return preds, np.array(list(LSMR.Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_accuracy(y_true, y_predict):\n",
    "    res = 0\n",
    "    for i in range(len(y_true)):\n",
    "        res += abs(y_true[i]-y_predict[i])\n",
    "    return 1-res/(len(y_true)*len(set(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "X = df[\"Review\"]\n",
    "y = df[\"Language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review.film g zel...\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review. ok e lenc...\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review.m kemmmmmm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.007159850810486593\n",
      "K:\t1\n",
      "Score:\t0.5177777777777778\n",
      "took: 4.208765029907227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review.no i m you...\n",
      "Process ForkPoolWorker-35:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Process ForkPoolWorker-38:\n",
      "Process ForkPoolWorker-37:\n",
      "Process ForkPoolWorker-40:\n",
      "Process ForkPoolWorker-34:\n",
      "Process ForkPoolWorker-39:\n",
      "Process ForkPoolWorker-33:\n",
      "Process ForkPoolWorker-36:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/joblib/pool.py\", line 362, in get\n",
      "    return recv()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-336ee1224cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Language\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_curve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-cce009b66463>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(df_train, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mX1_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX3_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX4_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX4_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3f3e92ea7129>\u001b[0m in \u001b[0;36mget_training_batch\u001b[0;34m(data_dict)\u001b[0m\n\u001b[1;32m     37\u001b[0m     X1, X2, X3, X4, Y = Parallel(n_jobs=-1)(delayed(dict2array)\n\u001b[1;32m     38\u001b[0m                                          \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                          for v in [X1_dict, X2_dict, X3_dict, X4_dict, Y_dict])\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_terminate_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_terminate_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;34m\"\"\"Shutdown the process or thread pool\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiprocessingBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOBLIB_SPAWNED_PROCESS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# terminate does a join()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/joblib/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMemmapingPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    185\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'helping task handler/workers to finish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_help_stuff_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mresult_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_help_stuff_finish\u001b[0;34m(inqueue, task_handler, size)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# task_handler may be blocked trying to put items on inqueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'removing tasks from inqueue until task handler finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0minqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/local/lib/python3.5/dist-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 1\n",
    "scores = dict()\n",
    "learning_curves = dict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    scores[i] = dict()\n",
    "    learning_curves[i] = dict()\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        start = time.time()\n",
    "        W1, W2, W3, W4, learning_curve = fit(df.loc[train_index], epochs=2000)\n",
    "        preds, true = predict(df.loc[test_index], W1, W2, W3, W4)\n",
    "        \n",
    "        s = distance_accuracy(true, preds)\n",
    "        scores[i][k] = s\n",
    "        learning_curves[i][k] = learning_curve\n",
    "        k += 1\n",
    "        print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "        print(\"took:\", time.time()-start)\n",
    "        scores[i][k] = s\n",
    "        learning_curves[i][k] = learning_curve\n",
    "    print(\"*\"*10)\n",
    "    try:\n",
    "        print(\"Trial {} avg score:\\t {}\".format(i+1, np.mean(list(scores[i].values()))))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([scores, learning_curves], open(\"incremental_tf.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame(scores)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
