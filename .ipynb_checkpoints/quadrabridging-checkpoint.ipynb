{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, numpy as np, re, json, pandas as pd, pickle, unicodedata, textblob\n",
    "# try:\n",
    "#     import gnumpy as gpu\n",
    "# except ModuleNotFoundError:\n",
    "#     pass\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim, math\n",
    "from gensim.models import doc2vec\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from KaggleWord2VecUtility import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>i love science fiction and i hate superheroes ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>the movie is absolutely incredible all the per...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>in a cinematic era dominated by reboots and mi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>movie review on rise of the planet of the apes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>during experiments to find a cure for alzheime...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language    Movie_ID                                             Review  \\\n",
       "0       en  -800777728  i love science fiction and i hate superheroes ...   \n",
       "1       en  -800777728  the movie is absolutely incredible all the per...   \n",
       "2       en -1018312192  in a cinematic era dominated by reboots and mi...   \n",
       "3       en -1018312192  movie review on rise of the planet of the apes...   \n",
       "4       en -1018312192  during experiments to find a cure for alzheime...   \n",
       "\n",
       "   Score  \n",
       "0      9  \n",
       "1     10  \n",
       "2      8  \n",
       "3      4  \n",
       "4      7  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/movie_data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"tr\":\"tr2en\", \"en\":\"en2tr\"}\n",
    "en2tr = dict()\n",
    "tr2en = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../NLP_data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../NLP_data/wiki.tr/wiki.tr.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_translation_matrix(X,Y, iterations=5000, alpha=0.0001, alpha_change_rate=0.8):\n",
    "    W = np.random.random((300, 300))\n",
    "    for i in range(iterations+1):\n",
    "        gradient = np.zeros(300)\n",
    "        for score in range(len(X)):\n",
    "            error = X[score].dot(W) - Y[score]\n",
    "            gradient += alpha * np.gradient(error)\n",
    "        W += gradient\n",
    "        if i == 2000:\n",
    "            alpha /= 100\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            alpha *= alpha_change_rate\n",
    "            print(\"Mikolov distance: {}\".format(mikolov(X, Y, W)))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_stemmer = TurkishStemmer()\n",
    "def clean(text, language=\"en\", stem=True):\n",
    "    global turkish_stemmer\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "    \n",
    "    if language == \"tr\":\n",
    "        if stem:\n",
    "            text= ' '.join([turkish_stemmer.stem(w) for w in text.split()])\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r'[0-9]', '#', text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    return TextBlob(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def record_translations():\n",
    "#     for lang in data_dict:\n",
    "#         for score in data_dict[lang]:\n",
    "#             for movie in data_dict[lang][score]:\n",
    "#                 for review in data_dict[lang][score][movie]:\n",
    "#                     try:\n",
    "#                         blob = clean(review)\n",
    "#                         if review in globals()[d[lang]]:\n",
    "#                             ent = globals()[d[lang]][review]\n",
    "#                         else:\n",
    "#                             ent = str(blob.translate(to=d[lang][-2:]))\n",
    "#                             globals()[d[lang]][review] = ent\n",
    "#                     except:\n",
    "#                         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_translations()\n",
    "# pickle.dump(en2tr, open(\"datasets/en2tr\",\"wb\"))\n",
    "# pickle.dump(tr2en, open(\"datasets/tr2en\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "en2tr = pickle.load(open(\"datasets/en2tr\",\"rb\"))\n",
    "tr2en = pickle.load(open(\"datasets/tr2en\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 300\n",
    "def vectorize(text, language, translate=False):\n",
    "    global VECTOR_SIZE, en2tr, tr2en\n",
    "    if translate:\n",
    "        if language==\"tr\":\n",
    "            try:\n",
    "                text = en2tr[text]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    translated = str(TextBlob(text).translate(to=language))\n",
    "                except:\n",
    "                    warnings.warn(\"Can't translate invalid English Review.\"+text[:10]+\"...\")\n",
    "                    return None\n",
    "                en2tr[text] = translated\n",
    "                tr2en[translated] = text\n",
    "                text = translated\n",
    "        else:\n",
    "            try:\n",
    "                text = tr2en[text]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    translated = str(TextBlob(text).translate(to=language))\n",
    "                except:\n",
    "                    warnings.warn(\"Can't translate invalid Turkish Review.\"+text[:10]+\"...\")\n",
    "                    return None\n",
    "                tr2en[text] = translated\n",
    "                en2tr[translated] = text\n",
    "                text = translated\n",
    "                \n",
    "    blob = clean(text, language)\n",
    "    vector = np.zeros(VECTOR_SIZE)\n",
    "    if len(blob.words) < 1:\n",
    "        return None\n",
    "\n",
    "    for word in blob.words:\n",
    "        try:\n",
    "            if language == \"en\":\n",
    "                vector += globals()[\"en_vects\"][word]\n",
    "            else:\n",
    "                vector += globals()[\"tr_vects\"][word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vector /= len(blob.words)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_size(data_dict, lang):\n",
    "    res = 0\n",
    "    for score in data_dict[lang]:\n",
    "        for movie in data_dict[lang][score]:\n",
    "                for tr_rev in data_dict[lang][score][movie]:\n",
    "                    res +=1\n",
    "    return res\n",
    "\n",
    "def get_X2_size(data_dict):\n",
    "    res = 0\n",
    "    for score in data_dict[\"tr\"]:\n",
    "        for movie in data_dict[\"tr\"][score]:\n",
    "            try:\n",
    "                for en_rev in data_dict[\"en\"][score][movie]:\n",
    "                    res += 1\n",
    "            except KeyError:  ## there are no english review for that movie with the same score\n",
    "                continue\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(df, train_index):\n",
    "    data_dict = dict() #{language:{score: {movie_id: [rev1, rev2, ..., revn]}}}\n",
    "    for i in train_index:\n",
    "        row = df.loc[i]\n",
    "        lang = row[0]\n",
    "        movie_id = row[1]\n",
    "        review = row[2]\n",
    "        score = row[3]\n",
    "\n",
    "        data_dict.setdefault(lang, {})\n",
    "        data_dict[lang].setdefault(score, {})\n",
    "        data_dict[lang][score].setdefault(movie_id, [])\n",
    "        data_dict[lang][score][movie_id].append(review)\n",
    "        \n",
    "#     X1_size = get_X_size(data_dict, \"tr\")\n",
    "#     X1 = list(np.zeros((X1_size, 300)))\n",
    "#     Y1 = list(np.zeros((X1_size, 300)))\n",
    "\n",
    "    X1 = list()\n",
    "    Y1 = list()\n",
    "\n",
    "    \n",
    "#     X2_size = get_X2_size(data_dict)\n",
    "#     X2 = list(np.zeros((X2_size, 300)))\n",
    "#     Y2 = list(np.zeros((X2_size, 300)))\n",
    "    \n",
    "    X2 = list()\n",
    "    Y2 = list()\n",
    "    \n",
    "    \n",
    "#     X3_size = get_X_size(data_dict, \"en\")\n",
    "#     X3= list(np.zeros(((X3_size, 300))))\n",
    "#     Y3 = list(np.zeros(((X3_size, 300))))\n",
    "    X3 = list()\n",
    "    Y3 = list()\n",
    "    \n",
    "    y3 = list()\n",
    "\n",
    "    \n",
    "#     print(np.array(X1).shape, np.array(Y1).shape)\n",
    "#     print(np.array(X2).shape, np.array(Y2).shape)\n",
    "#     print(np.array(X3).shape, np.array(Y3).shape, np.array(y3).shape)\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = 0\n",
    "    for lang in data_dict:\n",
    "        for score in data_dict[lang]:\n",
    "            for movie in data_dict[lang][score]:\n",
    "                if lang == \"tr\":\n",
    "                    ## First Layer\n",
    "                    for tr_rev in data_dict[lang][score][movie]:\n",
    "                            y_ = vectorize(tr_rev, language=\"en\", translate=True)\n",
    "                            if y_ is None:\n",
    "                                continue\n",
    "                            X1.append(np.array(vectorize(tr_rev, language=\"tr\")))\n",
    "                            Y1.append(np.array(y_))\n",
    "#                             X1[i] = np.array(vectorize(tr_rev, language=\"tr\"))\n",
    "#                             Y1[i] = np.array(y_)\n",
    "                            i +=1 \n",
    "                        \n",
    "                    ### Second Layer\n",
    "                    try:\n",
    "                        for en_rev in data_dict[\"en\"][score][movie]:\n",
    "                            y_ = vectorize(en_rev, language=\"tr\", translate=True)\n",
    "                            if y_ is None:\n",
    "                                continue\n",
    "                            X2.append(np.array(vectorize(en_rev, language=\"en\")))\n",
    "                            Y2.append(np.array(y_))\n",
    "#                             X2[j] = np.array(vectorize(en_rev, language=\"en\"))\n",
    "#                             Y2[j] = np.array(y_)\n",
    "                            j += 1\n",
    "                    except KeyError:  ## there are no english review for that movie with the same score\n",
    "                        continue\n",
    "                else:\n",
    "                    ## Third Layer\n",
    "                    for en_rev in data_dict[lang][score][movie]:\n",
    "                        x_ = vectorize(en_rev, language=\"tr\", translate=True)\n",
    "                        if x_ is None:\n",
    "                            continue\n",
    "                        X3.append(np.array(x_))    \n",
    "                        Y3.append(np.array(vectorize(en_rev, language=\"en\")))\n",
    "                        y3.append(np.array([score]))\n",
    "#                         X3[k] = np.array(x_)\n",
    "#                         Y3[k] = np.array(vectorize(en_rev, language=\"en\"))\n",
    "#                         y3[k] = np.array([score])\n",
    "                        k+=1\n",
    "    \n",
    "    X1 = np.array(X1)\n",
    "    Y1 = np.array(Y1)\n",
    "    \n",
    "    X2 = np.array(X2)\n",
    "    Y2 = np.array(Y2)\n",
    "    \n",
    "    X3 = np.array(X2)\n",
    "    Y3 = np.array(Y2)\n",
    "    y3 = np.array(y3)\n",
    "    \n",
    "    print(X1.shape, Y1.shape)\n",
    "    print(X2.shape, Y2.shape)\n",
    "    print(X3.shape, Y3.shape, y3.shape)\n",
    "    print(\"-\"*50)\n",
    "    W1 = MLPRegressor(random_state=42)\n",
    "    W1.fit(X1, Y1)\n",
    "    \n",
    "    W2 = MLPRegressor(random_state=42)\n",
    "    W2.fit(X2, Y2)\n",
    "    \n",
    "    W3 = MLPRegressor(random_state=42)\n",
    "    W3.fit(X3, Y3)\n",
    "    \n",
    "    W4 = MLPClassifier(random_state=42)\n",
    "    W4.fit(Y3, y3)\n",
    "    return W1, W2, W3, W4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, test_index, W1, W2, W3, W4):\n",
    "    X = list(np.zeros(len(test_index), 300))\n",
    "    y = list(np.zeros(len(test_index)))\n",
    "    for i in range(len(test_index)):\n",
    "        row = df.loc[test_index[i]]\n",
    "        review = row[2]\n",
    "        score = row[3]\n",
    "        lang = row[0]\n",
    "        X[i] = vectorize(review, lang)\n",
    "        y[i] = score\n",
    "        \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    l1 =W1.predict(X)\n",
    "    l2 = W2.predict(l1)\n",
    "    l3 = W3.predict(l2)\n",
    "    pred_scores = W4.predict(l3)\n",
    "    return pred_scores, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_accuracy(y_true, y_predict):\n",
    "    res = 0\n",
    "    for i in range(len(y_true)):\n",
    "        res += abs(y_true[i]-y_predict[i])\n",
    "    return 1-res/(len(y_true)*len(set(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "X = df[\"Review\"]\n",
    "y = df[\"Language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review.puan a r a...\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review. ok e lenc...\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review.film g zel...\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review.tam bir g ...\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review.michael ke...\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:24: UserWarning: Can't translate invalid Turkish Review.m kemmmmmm...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(444, 300) (444, 300)\n",
      "(134, 300) (134, 300)\n",
      "(134, 300) (134, 300) (450, 1)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [134, 450]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-5dcdd83dcc36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-157-95aa46ab42f0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(df, train_index)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mW4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mW4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \"\"\"\n\u001b[1;32m    972\u001b[0m         return self._fit(X, y, incremental=(self.warm_start and\n\u001b[0;32m--> 973\u001b[0;31m                                             hasattr(self, \"classes_\")))\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    329\u001b[0m                              hidden_layer_sizes)\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 910\u001b[0;31m                          multi_output=True)\n\u001b[0m\u001b[1;32m    911\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [134, 450]"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    W1, W2, W3, W4 = fit(df, train_index)\n",
    "    preds, true = predict(df, test_index, W1, W2, W3, W4)\n",
    "    for metric in score_dict:\n",
    "        score_dict[metric] += globals()[metric](l_test, pred_scores)\n",
    "for metric in score_dict:\n",
    "    score_dict[metric] /= 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
