{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, numpy as np, re, json, pandas as pd, pickle, unicodedata, textblob\n",
    "# try:\n",
    "#     import gnumpy as gpu\n",
    "# except ModuleNotFoundError:\n",
    "#     pass\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim, math\n",
    "from gensim.models import doc2vec\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from KaggleWord2VecUtility import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>i love science fiction and i hate superheroes ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>the movie is absolutely incredible all the per...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>in a cinematic era dominated by reboots and mi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>movie review on rise of the planet of the apes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>during experiments to find a cure for alzheime...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language    Movie_ID                                             Review  \\\n",
       "0       en  -800777728  i love science fiction and i hate superheroes ...   \n",
       "1       en  -800777728  the movie is absolutely incredible all the per...   \n",
       "2       en -1018312192  in a cinematic era dominated by reboots and mi...   \n",
       "3       en -1018312192  movie review on rise of the planet of the apes...   \n",
       "4       en -1018312192  during experiments to find a cure for alzheime...   \n",
       "\n",
       "   Score  \n",
       "0      9  \n",
       "1     10  \n",
       "2      8  \n",
       "3      4  \n",
       "4      7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/movie_data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict() #{language:{score: {movie_id: [rev1, rev2, ..., revn]}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.iterrows():\n",
    "    lang = row[1][0]\n",
    "    movie_id = row[1][1]\n",
    "    review = row[1][2]\n",
    "    score = row[1][3]\n",
    "    \n",
    "    data_dict.setdefault(lang, {})\n",
    "    data_dict[lang].setdefault(score, {})\n",
    "    data_dict[lang][score].setdefault(movie_id, [])\n",
    "    data_dict[lang][score][movie_id].append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_dict, open(\"datasets/movie_reviews_dict.pckl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open(\"datasets/movie_reviews_dict.pckl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"wiki.tr/wiki.tr.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_translation_matrix(X,Y, iterations=5000, alpha=0.0001, alpha_change_rate=0.8):\n",
    "    W = np.random.random((300, 300))\n",
    "    for i in range(iterations+1):\n",
    "        gradient = np.zeros(300)\n",
    "        for score in range(len(X)):\n",
    "            error = X[score].dot(W) - Y[score]\n",
    "            gradient += alpha * np.gradient(error)\n",
    "        W += gradient\n",
    "        if i == 2000:\n",
    "            alpha /= 100\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            alpha *= alpha_change_rate\n",
    "            print(\"Mikolov distance: {}\".format(mikolov(X, Y, W)))\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, language=\"en\", stem=True):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "    \n",
    "    if language == \"tr\":\n",
    "        if stem:\n",
    "            text= ' '.join([self.turkish_stemmer.stem(w) for w in text.split()])\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r'[0-9]', '#', text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    return TextBlob(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"tr\":\"tr2en\", \"en\":\"en2tr\"}\n",
    "en2tr = dict()\n",
    "tr2en = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_translations():\n",
    "    for lang in data_dict:\n",
    "        for score in data_dict[lang]:\n",
    "            for movie in data_dict[lang][score]:\n",
    "                for review in data_dict[lang][score][movie]:\n",
    "                    try:\n",
    "                        blob = clean(review)\n",
    "                        if review in globals()[d[lang]]:\n",
    "                            ent = globals()[d[lang]][review]\n",
    "                        else:\n",
    "                            ent = str(blob.translate(to=d[lang][-2:]))\n",
    "                            globals()[d[lang]][review] = ent\n",
    "                    except:\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_translations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(en2tr, open(\"datasets/en2tr\",\"wb\"))\n",
    "pickle.dump(tr2en, open(\"datasets/tr2en\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.random(300)\n",
    "W2 = np.random.random(300)\n",
    "W3 = np.random.random(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_W(X, Y, W, iter=100, alpha=0.1):\n",
    "    for i in range(1, iter+1):  # We add this one so we can use i%10 == 0 in the last epoch\n",
    "        delta = 0\n",
    "        error = 0\n",
    "        for j in range(len(X)):\n",
    "            x = X[j]\n",
    "            y = Y[j]\n",
    "            y_prime = np.dot(x,W)\n",
    "            err = np.linalg.norm(y_prime - y)\n",
    "            error += err\n",
    "            delta += np.gradient(err)\n",
    "        if i %10 == 0:\n",
    "            print (\"Epoch %d:\"%i, error)\n",
    "        W += alpha * delta\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = dict()\n",
    "Y1 = dict()\n",
    "\n",
    "X2 = dict()\n",
    "Y2 = dict()\n",
    "\n",
    "X3 = dict()\n",
    "Y3 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text, language):\n",
    "    blob = clean(text, language)\n",
    "    vector = np.zeros(self.vector_size)\n",
    "    if len(blob.words) < 1:\n",
    "        return None\n",
    "\n",
    "    for word in blob.words:\n",
    "        try:\n",
    "            if language == \"en\":\n",
    "                vector += globals()[\"en_w2v\"][word]\n",
    "            else:\n",
    "                vector += globals()[\"tr_w2v\"][word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vector /= len(blob.words)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tr_doc in tr_docs:\n",
    "#     en_translation = tr2en[tr_doc]\n",
    "#     X1[len(X1)] = vectorize(tr_doc)\n",
    "#     Y1[len(Y1)] = vectorize(en_translation)\n",
    "# W1 = train_W(X1.values(), Y1.values(), W1)\n",
    "\n",
    "for score in data_dict[\"tr\"]:\n",
    "    for movie in data_dict[\"tr\"][score]:\n",
    "        for tr_rev in data_dict[\"tr\"][score][movie]:\n",
    "            X2[len(X2)] = vectorize(tr_rev)\n",
    "            try:\n",
    "                for en_rev in data_dict[\"en\"][score][movie]:\n",
    "                    Y2[len(Y2)] = vectorize(en2tr[en_rev])\n",
    "            except KeyError:\n",
    "                continue\n",
    "W2 = train_W(X2.values(), Y2.values(), W2)\n",
    "\n",
    "# for en_doc in en_docs:\n",
    "#     tr_translation = en2tr[en_doc]\n",
    "#     X3[len(X3)] = vectorize(tr_translation)\n",
    "#     Y3[len(Y3)] = vectorize(en_doc)\n",
    "# W3 = train_W(X3.values(), Y3.values(), W3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
