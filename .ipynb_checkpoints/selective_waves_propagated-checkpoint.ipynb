{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle, pandas as pd, re, numpy as np, ast, warnings\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import chain, starmap\n",
    "from itertools import product\n",
    "import unicodedata\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>i love science fiction and i hate superheroes ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>-800777728</td>\n",
       "      <td>the movie is absolutely incredible all the per...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>in a cinematic era dominated by reboots and mi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>movie review on rise of the planet of the apes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>-1018312192</td>\n",
       "      <td>during experiments to find a cure for alzheime...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language    Movie_ID                                             Review  \\\n",
       "0       en  -800777728  i love science fiction and i hate superheroes ...   \n",
       "1       en  -800777728  the movie is absolutely incredible all the per...   \n",
       "2       en -1018312192  in a cinematic era dominated by reboots and mi...   \n",
       "3       en -1018312192  movie review on rise of the planet of the apes...   \n",
       "4       en -1018312192  during experiments to find a cure for alzheime...   \n",
       "\n",
       "   Score  \n",
       "0      9  \n",
       "1     10  \n",
       "2      8  \n",
       "3      4  \n",
       "4      7  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/movie_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Language  Movie_ID  Review\n",
       "Score                            \n",
       "1            29        29      29\n",
       "2            21        21      21\n",
       "3            14        14      14\n",
       "4            23        23      23\n",
       "5            83        83      83\n",
       "6            43        43      43\n",
       "7            71        71      71\n",
       "8           207       207     207\n",
       "9           175       175     175\n",
       "10          334       334     334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Score\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../NLP_data/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_vects = gensim.models.KeyedVectors.load_word2vec_format(r\"../NLP_data/wiki.tr/wiki.tr.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_stemmer = TurkishStemmer()\n",
    "def clean(text, language=\"en\", stem=True):\n",
    "    global turkish_stemmer\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "    \n",
    "    if language == \"tr\":\n",
    "        if stem:\n",
    "            text= ' '.join([turkish_stemmer.stem(w) for w in text.split()])\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r'[0-9]', '#', text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" \", text)\n",
    "    text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    return TextBlob(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 300\n",
    "def vectorize(text, language):\n",
    "    global VECTOR_SIZE            \n",
    "    blob = clean(text, language)\n",
    "    vector = np.zeros(VECTOR_SIZE)\n",
    "    if len(blob.words) < 1:\n",
    "        return None\n",
    "\n",
    "    for word in blob.words:\n",
    "        try:\n",
    "            if language == \"en\":\n",
    "                vector += globals()[\"en_vects\"][word]\n",
    "            else:\n",
    "                vector += globals()[\"tr_vects\"][word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vector /= len(blob.words)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvec(x):\n",
    "    lang, rev = x.split(\":::::\")\n",
    "    return vectorize(rev, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMSR\n",
    "def preprocess_data(df, language_column=\"Language\", review_column=\"Review\"):\n",
    "    LMSR_df = df.copy()\n",
    "    LMSR_df[\"lang_rev\"] = LMSR_df[[language_column, review_column]].apply(lambda x: x[0]+\":::::\"+x[1], axis=1)\n",
    "    LMSR_df[\"rev_vec\"] = LMSR_df[\"lang_rev\"].apply(lambda x:getvec(x))\n",
    "    LMSR_df.drop([\"lang_rev\", \"Review\"], axis=1, inplace=True)\n",
    "    return LMSR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_accuracy(y_true, y_predict):\n",
    "    res = 0\n",
    "    for i in range(len(y_true)):\n",
    "        res += abs(y_true[i]-y_predict[i])\n",
    "    return 1-res/(len(y_true)*len(set(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XYy(LMSR):\n",
    "    X = np.zeros((len(LMSR), VECTOR_SIZE))\n",
    "    Y = np.zeros((len(LMSR), VECTOR_SIZE))\n",
    "    y = np.zeros((len(LMSR)))\n",
    "    i = 0\n",
    "    for rev in LMSR.iterrows():\n",
    "        score = rev[1][2]\n",
    "        rev_vec = rev[1][3]\n",
    "        score_vec = rev[1][4]\n",
    "\n",
    "        X[i] = rev_vec\n",
    "        Y[i] = score_vec\n",
    "        y[i] = score\n",
    "\n",
    "        i += 1\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derive=False):\n",
    "    if derive:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(df, get_L2and3=False):\n",
    "    data_dict = dict() #{language:{score: {movie_id: [rev1, rev2, ..., revn]}}}\n",
    "    L1 = dict()  # {(languge, score, movie_id): list of reviews with the same score with the same language}\n",
    "    L2 = dict()  # {(language, score): None}\n",
    "    L3 = dict()  # {score: None}\n",
    "    for _, row in df.iterrows():\n",
    "        lang = row[\"Language\"]\n",
    "        movie_id = row[\"Movie_ID\"]\n",
    "        score = row[\"Score\"]\n",
    "        review = row[\"rev_vec\"]\n",
    "\n",
    "        data_dict.setdefault(lang, {})\n",
    "        data_dict[lang].setdefault(score, {})\n",
    "        data_dict[lang][score].setdefault(movie_id, [])\n",
    "        data_dict[lang][score][movie_id].append(review)\n",
    "        \n",
    "        L1.setdefault((lang, score, movie_id), list())\n",
    "        L1[(lang, score, movie_id)].append(review)\n",
    "        if get_L2and3:    \n",
    "            L2[(lang, score)] = None\n",
    "            L3[score] = None\n",
    "    if get_L2and3:\n",
    "        return data_dict, L1, L2, L3\n",
    "    return data_dict, L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L2(LSM_R, data_dict):\n",
    "    L2 = dict()  # {(language, score): list of movies vectors}\n",
    "    for language in data_dict:\n",
    "        for score in data_dict[language]:\n",
    "            for movie_id in data_dict[language][score]:\n",
    "                L2.setdefault((language, score), list())\n",
    "                L2[(language, score)].append(LSM_R[(language, score, movie_id)])\n",
    "    return L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L3(LS_MR, data_dict):\n",
    "    L3 = dict()  # {score: vector of merged languages for that score}\n",
    "    for language in data_dict:\n",
    "        for score in data_dict[language]:\n",
    "            L3.setdefault(score, list())\n",
    "            L3[score].append(LS_MR[(language, score)])\n",
    "    return L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(L, W):\n",
    "    merged = dict()  # {item: vector of merged subitems}\n",
    "    for i, item in enumerate(sorted(L)):\n",
    "        for subitem in L[item]:\n",
    "            merged.setdefault(item, [np.zeros(VECTOR_SIZE),0])\n",
    "            merged[item][0] += sigmoid(subitem.dot(W[i]))\n",
    "            merged[item][1] += 1\n",
    "    for item in merged:\n",
    "        merged[item] = merged[item][0]/ merged[item][1]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(L, delta, W, alpha=0.1):\n",
    "    for i, k in enumerate(sorted(L)):\n",
    "        for l in L[k]:\n",
    "            W[i] += l.T.dot(delta[i]) *alpha\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_error(delta, W):\n",
    "    error = 0\n",
    "    for i in range(len(delta)):\n",
    "        error += delta[i].dot(W[i].T)\n",
    "    return error/len(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_delta(error, layer, size):\n",
    "    delta = np.zeros((size, VECTOR_SIZE))\n",
    "    j = 0\n",
    "    for i,k in enumerate(sorted(layer)):\n",
    "        for l in layer[k]:\n",
    "            delta[j] = error[i]*sigmoid(l, True)\n",
    "            j += 1\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_vects(df, iterations=100, alpha=0.1, random_state=42, W1=None, W2=None, W3=None, W4=None):\n",
    "    LSMR = preprocess_data(df)\n",
    "    data_dict, L1 = get_data_dict(LSMR)\n",
    "    y = softmax(list(LSMR.Score))\n",
    "#     np.random.seed(random_state)\n",
    "    learning_curve = dict()\n",
    "    for i in range(iterations+1):\n",
    "        # forward propagation\n",
    "        if W1 is None:\n",
    "            W1 = 2*np.random.random((len(L1), 300, 300))-1\n",
    "\n",
    "        LSM_R = merge(L1, W1)\n",
    "        L2 = get_L2(LSM_R, data_dict)\n",
    "        if W2 is None:\n",
    "            W2 = 2*np.random.random((len(L2), 300, 300))-1\n",
    "\n",
    "        LS_MR = merge(L2, W2)\n",
    "        L3 = get_L3(LS_MR, data_dict)\n",
    "        if W3 is None:\n",
    "            W3 = 2*np.random.random((len(L3), 300, 300))-1\n",
    "\n",
    "        score_vectors_dict = merge(L3, W3)\n",
    "        l4 = sigmoid(np.array([v for k, v in sorted(score_vectors_dict.items())]))\n",
    "        if W4 is None:\n",
    "            W4 = 2*np.random.random((300, len(LSMR)))-1\n",
    "        \n",
    "        l5 = softmax(l4.dot(W4))  # predicted scores\n",
    "        \n",
    "        # Calculate the error\n",
    "        l5_error = np.mean(np.dot(np.log(l5), y))\n",
    "        \n",
    "        # Back propagation\n",
    "        l5_delta = l5_error * sigmoid(l5, True)\n",
    "        W4 += l4.T.dot(l5_delta)*alpha\n",
    "        \n",
    "        l4_error = l5_delta.dot(W4.T)\n",
    "        l4_delta = l4_error * sigmoid(l4, True)\n",
    "        \n",
    "        W3 = update_weights(L3, l4_delta, W3, alpha)\n",
    "        \n",
    "        l3_error = get_layer_error(l4_delta, W3)\n",
    "        l3_delta = get_layer_delta(l3_error, L3, len(L2))\n",
    "        \n",
    "        W2 = update_weights(L2, l3_delta, W2, alpha)\n",
    "        \n",
    "        l2_error = get_layer_error(l3_delta, W2)\n",
    "        l2_delta = get_layer_delta(l2_error, L2, len(LSMR))\n",
    "        \n",
    "        W1 = update_weights(L1, l2_delta, W1, alpha)\n",
    "        learning_curve[i] = l5_error\n",
    "        if i%10 == 0:\n",
    "            print(\"Iteration {}:\\t{}\".format(i, np.abs(l5_error)))\n",
    "    return LSMR, score_vectors_dict, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(LSMR, score_vect_dicts,random_state=42, regressor=MLPRegressor(), classifier=MLPClassifier()):\n",
    "    LSMR[\"score_vec\"] = LSMR[\"Score\"].apply(lambda x: score_vect_dicts[x] if x in score_vect_dicts else np.NaN)\n",
    "    LSMR.dropna(inplace=True)\n",
    "    \n",
    "    X, Y, y = get_XYy(LSMR)\n",
    "    \n",
    "    regressor.random_state = random_state\n",
    "    classifier.random_state = random_state\n",
    "        \n",
    "    regressor.fit(X, Y)\n",
    "    classifier.fit(Y, y)\n",
    "    return regressor, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(LSMR, score_vect_dicts, regressor, classifier):\n",
    "    LSMR[\"score_vec\"] = LSMR[\"Score\"].apply(lambda x: score_vect_dicts[x] if x in score_vect_dicts else np.NaN)\n",
    "    LSMR.dropna(inplace=True)\n",
    "    \n",
    "    X, Y, y = get_XYy(LSMR)\n",
    "    \n",
    "    preds_score_vecs = regressor.predict(X)\n",
    "    pred_scores = classifier.predict(preds_score_vecs)\n",
    "    \n",
    "    return pred_scores, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "Iteration 0:\t23.05032027737502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10:\t12.013688806867378\n",
      "Iteration 20:\t10.744270890680427\n",
      "Iteration 30:\t10.114251226703924\n",
      "Iteration 40:\t9.75217132158491\n",
      "Iteration 50:\t9.52156371951053\n",
      "Iteration 60:\t9.37614311021223\n",
      "Iteration 70:\t9.280248022521429\n",
      "Iteration 80:\t9.218285276774253\n",
      "Iteration 90:\t9.18114695146247\n",
      "Iteration 100:\t9.161032448418315\n",
      "K:\t1\n",
      "Score:\t0.7822222222222223\n",
      "took: 30.43852734565735 seconds\n",
      "Iteration 0:\t21.13186922373753\n",
      "Iteration 10:\t11.35668325278657\n",
      "Iteration 20:\t10.51285456868467\n",
      "Iteration 30:\t10.45535150004107\n",
      "Iteration 40:\t9.89214040730134\n",
      "Iteration 50:\t9.597518842700218\n",
      "Iteration 60:\t9.419876479393006\n",
      "Iteration 70:\t9.306109132276688\n",
      "Iteration 80:\t9.229259279797853\n",
      "Iteration 90:\t9.178043524843314\n",
      "Iteration 100:\t9.14513334364513\n",
      "K:\t2\n",
      "Score:\t0.745\n",
      "took: 24.68630599975586 seconds\n",
      "Iteration 0:\t24.175998239788285\n",
      "Iteration 10:\t12.461546686531529\n",
      "Iteration 20:\t10.910005342716062\n",
      "Iteration 30:\t10.245256648082933\n",
      "Iteration 40:\t9.872941382060517\n",
      "Iteration 50:\t9.6527317486706\n",
      "Iteration 60:\t9.519403426918913\n",
      "Iteration 70:\t9.436222826515996\n",
      "Iteration 80:\t9.38418812596104\n",
      "Iteration 90:\t9.351524367147572\n",
      "Iteration 100:\t9.330220497050624\n",
      "K:\t3\n",
      "Score:\t0.7755555555555556\n",
      "took: 25.367411136627197 seconds\n",
      "Iteration 0:\t25.76522344102571\n",
      "Iteration 10:\t13.149132227192379\n",
      "Iteration 20:\t11.48207475256827\n",
      "Iteration 30:\t10.730282571420485\n",
      "Iteration 40:\t10.33336283016747\n",
      "Iteration 50:\t10.118756726802408\n",
      "Iteration 60:\t9.994490291405134\n",
      "Iteration 70:\t9.916059652161312\n",
      "Iteration 80:\t9.86464683443944\n",
      "Iteration 90:\t9.829076661820865\n",
      "Iteration 100:\t9.804100202639134\n",
      "K:\t4\n",
      "Score:\t0.783\n",
      "took: 25.417934894561768 seconds\n",
      "Iteration 0:\t22.59471241392317\n",
      "Iteration 10:\t12.162676183186267\n",
      "Iteration 20:\t10.75359832480135\n",
      "Iteration 30:\t10.14494434381767\n",
      "Iteration 40:\t9.819698639703493\n",
      "Iteration 50:\t9.622467644843287\n",
      "Iteration 60:\t9.499586320460692\n",
      "Iteration 70:\t9.419950817062277\n",
      "Iteration 80:\t9.365022770950073\n",
      "Iteration 90:\t9.32885999638294\n",
      "Iteration 100:\t9.307933485049498\n",
      "K:\t5\n",
      "Score:\t0.793\n",
      "took: 26.0831139087677 seconds\n",
      "Iteration 0:\t21.136001281987138\n",
      "Iteration 10:\t11.411096275186141\n",
      "Iteration 20:\t10.555946809800984\n",
      "Iteration 30:\t10.387934215077106\n",
      "Iteration 40:\t9.865708247823482\n",
      "Iteration 50:\t9.57760107409836\n",
      "Iteration 60:\t9.408755122213586\n",
      "Iteration 70:\t9.301426914501462\n",
      "Iteration 80:\t9.233036351642358\n",
      "Iteration 90:\t9.195098212087936\n",
      "Iteration 100:\t9.17533007265729\n",
      "K:\t6\n",
      "Score:\t0.778\n",
      "took: 25.29996609687805 seconds\n",
      "Iteration 0:\t24.05715668737055\n",
      "Iteration 10:\t12.20801440684642\n",
      "Iteration 20:\t10.757700989006416\n",
      "Iteration 30:\t10.06172694091533\n",
      "Iteration 40:\t9.672632859725272\n",
      "Iteration 50:\t9.457532789905768\n",
      "Iteration 60:\t9.334073664495609\n",
      "Iteration 70:\t9.259085012147471\n",
      "Iteration 80:\t9.209798336284814\n",
      "Iteration 90:\t9.179168974159362\n",
      "Iteration 100:\t9.162993901868749\n",
      "K:\t7\n",
      "Score:\t0.7922222222222222\n",
      "took: 29.299280166625977 seconds\n",
      "Iteration 0:\t24.380187778997403\n",
      "Iteration 10:\t12.186699801762751\n",
      "Iteration 20:\t10.737615940410379\n",
      "Iteration 30:\t10.091389619349787\n",
      "Iteration 40:\t9.742819898072426\n",
      "Iteration 50:\t9.532919885627999\n",
      "Iteration 60:\t9.402956308286699\n",
      "Iteration 70:\t9.321793589579753\n",
      "Iteration 80:\t9.270130731578627\n",
      "Iteration 90:\t9.238613414042124\n",
      "Iteration 100:\t9.219039820594773\n",
      "K:\t8\n",
      "Score:\t0.7975\n",
      "took: 25.377017974853516 seconds\n",
      "Iteration 0:\t22.49075962619308\n",
      "Iteration 10:\t11.946818622606568\n",
      "Iteration 20:\t10.794337441158786\n",
      "Iteration 30:\t10.062309064635965\n",
      "Iteration 40:\t9.66097418638548\n",
      "Iteration 50:\t9.428652964969647\n",
      "Iteration 60:\t9.295812796817739\n",
      "Iteration 70:\t9.219156157290026\n",
      "Iteration 80:\t9.17447642852335\n",
      "Iteration 90:\t9.151592601933881\n",
      "Iteration 100:\t9.14061078033861\n",
      "K:\t9\n",
      "Score:\t0.772\n",
      "took: 25.12312650680542 seconds\n",
      "Iteration 0:\t24.85573239689249\n",
      "Iteration 10:\t12.132195007797074\n",
      "Iteration 20:\t10.751963687532903\n",
      "Iteration 30:\t10.103321068752539\n",
      "Iteration 40:\t9.72710180819342\n",
      "Iteration 50:\t9.495162392147899\n",
      "Iteration 60:\t9.347135359894526\n",
      "Iteration 70:\t9.254273440661464\n",
      "Iteration 80:\t9.199508257438216\n",
      "Iteration 90:\t9.167704935849637\n",
      "Iteration 100:\t9.150236634541082\n",
      "K:\t10\n",
      "Score:\t0.8022222222222222\n",
      "took: 24.48548460006714 seconds\n",
      "**********\n",
      "Trial 1 avg score:\t 0.7820722222222222\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 1\n",
    "scores = dict()\n",
    "learning_curve = dict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    scores[i] = dict()\n",
    "    learning_curves[i] = dict()\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        start = time.time()\n",
    "        LSMR, score_vect_dicts, training_curve = get_score_vects(df.loc[train_index], random_state=i)\n",
    "        regressor, classifier = fit(LSMR, score_vect_dicts, random_state=i)\n",
    "        preds, true = predict(preprocess_data(df.loc[test_index]), score_vect_dicts, regressor, classifier)\n",
    "        s = distance_accuracy(true, preds)\n",
    "        k += 1\n",
    "        print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "        print(\"took:\", time.time()-start, \"seconds\")\n",
    "        scores[i][k] = s\n",
    "        learning_curves[i][k] = training_curve\n",
    "    print(\"*\"*10)\n",
    "    try:\n",
    "        print(\"Trial {} avg score:\\t {}\".format(i+1, np.mean(list(scores[i].values()))))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([scores, learning_curves], open(\"batch_no_tf.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = pd.DataFrame(list(scores))\n",
    "# stats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(LSMR):\n",
    "    X = dict()\n",
    "    y = dict()\n",
    "    for _, row in LSMR.iterrows():\n",
    "        score = row[\"Score\"]\n",
    "        y_ = np.zeros(10)\n",
    "        y_[score-1] = 1\n",
    "        y[len(y)] = y_\n",
    "        X[len(X)] = row[\"rev_vec\"]\n",
    "    return np.array(list(X.values())), np.array(list(y.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(df_test, w_1, w_2, w_3):\n",
    "#     reset_graph()\n",
    "#     x = tf.placeholder(tf.float32, [None, 300])\n",
    "#     y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "#     w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "#     b1 = tf.Variable(tf.zeros([300]))\n",
    "#     b2 = tf.Variable(tf.zeros([300]))\n",
    "#     b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#     l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "#     l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "#     pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "    \n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#         # Testing the model\n",
    "#         LSMR_test = preprocess_data(df_test)\n",
    "#         X_test, y_test = get_test(LSMR_test)\n",
    "#         return accuracy.eval({x: X_test,\n",
    "#                               y: y_test,\n",
    "#                               w1:w_1,w2:w_2,\n",
    "#                               w3:w_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_TRIALS = 5\n",
    "# scores = list(np.zeros(NUM_TRIALS))\n",
    "# for i in range(NUM_TRIALS):\n",
    "#     print(\"Trial:\\t{}\".format(i+1))\n",
    "#     score_dict = {\"distance_accuracy\":0}\n",
    "#     k = 0\n",
    "#     skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "#     for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "#         w1, w2, w3 = train(df.loc[train_index], random_state=i, epochs=50000)\n",
    "#         s = test(df.loc[test_index], w1, w2, w3)\n",
    "#         score_dict[\"distance_accuracy\"] += s\n",
    "#         k += 1\n",
    "#         print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "#     score_dict[\"distance_accuracy\"] /= 10.0\n",
    "#     scores[i] = score_dict[\"distance_accuracy\"]\n",
    "#     print(\"*\"*10)\n",
    "#     print(\"Trial{} avg score:\\t {}\".format(i, score_dict[\"distance_accuracy\"]))\n",
    "#     print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_selective(df_train,epochs=100, learning_rate = 0.1, random_state=42):\n",
    "    LSMR_train = preprocess_data(df_train)\n",
    "    np.random.seed(random_state)\n",
    "    data_dict, L1, L2, L3 = get_data_dict(LSMR_train, get_L2and3=True)\n",
    "    init_weights = lambda layer, i, o: {k:2*np.random.random((i, o))-1 for k in layer}\n",
    "    W1 = init_weights(L1, 300, 300)  # (languge, score, movie_id)\n",
    "    W2 = init_weights(L2, 300, 300)  # (languge, score):\n",
    "    W3 = init_weights(L3, 300, 10)  # score:\n",
    "    \n",
    "    \n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "    y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    training_curve = dict()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(epochs+1):\n",
    "            avg_cost = 0.\n",
    "            for _, row in LSMR_train.iterrows():\n",
    "                lang = row[\"Language\"]\n",
    "                movie_id = row[\"Movie_ID\"]\n",
    "                score = row[\"Score\"]\n",
    "                y_ = np.zeros(10)\n",
    "                y_[score-1] = 1\n",
    "                y_ = np.atleast_2d(y_)\n",
    "                x_ = np.atleast_2d(row[\"rev_vec\"])\n",
    "                w1_,w2_,w3_,_, c = sess.run([w1, w2, w3, optimizer, cost],\n",
    "                                         feed_dict={x: x_,\n",
    "                                                    y: y_,\n",
    "                                                    w1:W1[(lang, score, movie_id)],\n",
    "                                                    w2:W2[(lang, score)],\n",
    "                                                    w3:W3[score]})\n",
    "                W1[(lang, score, movie_id)] = w1_\n",
    "                W2[(lang, score)] = w2_\n",
    "                W3[score] = w3_\n",
    "                \n",
    "                avg_cost += c\n",
    "            training_curve[e] = avg_cost\n",
    "            if e%10==0:\n",
    "                print(\"Epoch {}: {}\".format(e, avg_cost/len(LSMR_train)))\n",
    "        \n",
    "        return W1, W2, W3, training_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(array):\n",
    "    indx = None\n",
    "    max_ = float(\"-inf\")\n",
    "    for i, e in enumerate(array):\n",
    "        if e > max_:\n",
    "            max_ = e\n",
    "            indx = i\n",
    "    return indx, max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_selective(df, W1, W2, W3):\n",
    "    LSMR = preprocess_data(df)\n",
    "    reset_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 300])\n",
    "\n",
    "    w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "    w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "    b1 = tf.Variable(tf.zeros([300]))\n",
    "    b2 = tf.Variable(tf.zeros([300]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "    l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "    pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction = tf.argmax(pred, 1)\n",
    "    preds = np.zeros(len(LSMR))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        j = 0\n",
    "        for _, row in LSMR.iterrows():\n",
    "            v = row[\"rev_vec\"]\n",
    "            predicted_scores = np.zeros(len(W1))\n",
    "            for i, info in enumerate(W1):\n",
    "                language, score, movie_id = info\n",
    "                w_1 = W1[(language, score, movie_id)]\n",
    "                w_2 = W2[(language, score)]\n",
    "                w_3 = W3[score]\n",
    "                \n",
    "                predicted_scores[i] = prediction.eval({x: np.atleast_2d(v),\n",
    "                                                       w1:w_1,w2:w_2,w3:w_3})\n",
    "                \n",
    "            max_index, probability = get_max_index(softmax(predicted_scores))\n",
    "            predicted_score = predicted_scores[max_index]\n",
    "            \n",
    "            preds[j] = predicted_score\n",
    "            j+=1\n",
    "    \n",
    "    \n",
    "    return preds, np.array(list(LSMR.Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial:\t1\n",
      "Epoch 0: 2.9755149090860424\n",
      "Epoch 10: 0.03301395801265951\n",
      "Epoch 20: 0.009102185252193319\n",
      "Epoch 30: 0.005444824386233652\n",
      "Epoch 40: 0.003938626393603752\n",
      "Epoch 50: 0.003107796458510721\n",
      "Epoch 60: 0.002577785003723976\n",
      "Epoch 70: 0.0022086838019789766\n",
      "Epoch 80: 0.0019360923293665008\n",
      "Epoch 90: 0.0017260877580851128\n",
      "Epoch 100: 0.0015590692933245818\n",
      "Epoch 110: 0.0014229088037864637\n",
      "Epoch 120: 0.0013096479197252443\n",
      "Epoch 130: 0.0012138942475146703\n",
      "Epoch 140: 0.0011318201913728546\n",
      "Epoch 150: 0.0010606514541244552\n",
      "Epoch 160: 0.0009983230416030942\n",
      "Epoch 170: 0.0009432612553825948\n",
      "Epoch 180: 0.000894240514087667\n",
      "Epoch 190: 0.0008503051894033181\n",
      "Epoch 200: 0.0008106973505202102\n",
      "Epoch 210: 0.0007747959017999367\n",
      "Epoch 220: 0.0007420984184180826\n",
      "Epoch 230: 0.0007121829090133108\n",
      "Epoch 240: 0.0006847060506912486\n",
      "Epoch 250: 0.000659379128109347\n",
      "Epoch 260: 0.0006359545544037499\n",
      "Epoch 270: 0.0006142189659869423\n",
      "Epoch 280: 0.0005939980175703062\n",
      "Epoch 290: 0.0005751360640951791\n",
      "Epoch 300: 0.0005574943513098181\n",
      "Epoch 310: 0.0005409593071902287\n",
      "Epoch 320: 0.0005254317589344737\n",
      "Epoch 330: 0.0005108144589247685\n",
      "Epoch 340: 0.0004970316169550415\n",
      "Epoch 350: 0.00048401166768605386\n",
      "Epoch 360: 0.00047169356875637427\n",
      "Epoch 370: 0.00046002052496956164\n",
      "Epoch 380: 0.0004489378180401597\n",
      "Epoch 390: 0.0004384107524130817\n",
      "Epoch 400: 0.00042838847307905323\n",
      "Epoch 410: 0.00041884128706903487\n",
      "Epoch 420: 0.0004097333003997821\n",
      "Epoch 430: 0.00040103362267340494\n",
      "Epoch 440: 0.00039271619521748057\n",
      "Epoch 450: 0.00038475570541063715\n",
      "Epoch 460: 0.00037712906613017417\n",
      "Epoch 470: 0.00036981344268945625\n",
      "Epoch 480: 0.00036279358098719665\n",
      "Epoch 490: 0.00035604660539434007\n",
      "Epoch 500: 0.00034956052225702984\n",
      "K:\t1\n",
      "Score:\t0.8311111111111111\n",
      "took: 306.7269244194031\n",
      "Epoch 0: 1.9232659654853088\n",
      "Epoch 10: 0.010127266519697097\n",
      "Epoch 20: 0.004436774661494951\n",
      "Epoch 30: 0.0029274397332653733\n",
      "Epoch 40: 0.0022122628677962554\n",
      "Epoch 50: 0.0017906625088966483\n",
      "Epoch 60: 0.0015109276848846143\n",
      "Epoch 70: 0.001310944722348495\n",
      "Epoch 80: 0.0011604180473048901\n",
      "Epoch 90: 0.0010427494853469928\n",
      "Epoch 100: 0.0009480740189412826\n",
      "Epoch 110: 0.0008701410802343743\n",
      "Epoch 120: 0.0008047971639838162\n",
      "Epoch 130: 0.0007491575551898993\n",
      "Epoch 140: 0.0007011831836918406\n",
      "Epoch 150: 0.0006593514285283833\n",
      "Epoch 160: 0.0006225369751154138\n",
      "Epoch 170: 0.0005898692464245098\n",
      "Epoch 180: 0.000560671540332799\n",
      "Epoch 190: 0.000534401706575554\n",
      "Epoch 200: 0.0005106380087647722\n",
      "Epoch 210: 0.0004890355791282117\n",
      "Epoch 220: 0.0004692908737581547\n",
      "Epoch 230: 0.0004511838244502542\n",
      "Epoch 240: 0.00043450558724373423\n",
      "Epoch 250: 0.00041909622119981626\n",
      "Epoch 260: 0.0004048137318744062\n",
      "Epoch 270: 0.0003915379747050767\n",
      "Epoch 280: 0.00037915137922280137\n",
      "Epoch 290: 0.00036757758571209504\n",
      "Epoch 300: 0.0003567282591086496\n",
      "Epoch 310: 0.0003465458655062371\n",
      "Epoch 320: 0.00033696654145166525\n",
      "Epoch 330: 0.0003279315090705697\n",
      "Epoch 340: 0.00031939896836825535\n",
      "Epoch 350: 0.0003113270304379196\n",
      "Epoch 360: 0.0003036773166980172\n",
      "Epoch 370: 0.00029641805748838125\n",
      "Epoch 380: 0.0002895225677836935\n",
      "Epoch 390: 0.00028296312847617047\n",
      "Epoch 400: 0.00027670878985696214\n",
      "Epoch 410: 0.0002707426487753158\n",
      "Epoch 420: 0.00026504715710517406\n",
      "Epoch 430: 0.00025959595491700484\n",
      "Epoch 440: 0.00025437905223807573\n",
      "Epoch 450: 0.0002493807134446039\n",
      "Epoch 460: 0.0002445886286624146\n",
      "Epoch 470: 0.00023998508922870896\n",
      "Epoch 480: 0.00023556898864800108\n",
      "Epoch 490: 0.0002313175178151836\n",
      "Epoch 500: 0.0002272292551338574\n",
      "K:\t2\n",
      "Score:\t0.789\n",
      "took: 298.94539427757263\n",
      "Epoch 0: 1.4014802528536772\n",
      "Epoch 10: 0.01079084289042132\n",
      "Epoch 20: 0.004457719902259719\n",
      "Epoch 30: 0.002890649203036825\n",
      "Epoch 40: 0.0021651885019203596\n",
      "Epoch 50: 0.0017422374174702782\n",
      "Epoch 60: 0.0014635299400754952\n",
      "Epoch 70: 0.001265263910989347\n",
      "Epoch 80: 0.0011165859339291096\n",
      "Epoch 90: 0.0010007417983766548\n",
      "Epoch 100: 0.0009077802053134418\n",
      "Epoch 110: 0.0008314466917900947\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-55eabd30e98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Language\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_curve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_selective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_selective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-81eb4c80c9c0>\u001b[0m in \u001b[0;36mtrain_selective\u001b[0;34m(df_train, epochs, learning_rate, random_state)\u001b[0m\n\u001b[1;32m     46\u001b[0m                                                     \u001b[0mw1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                     \u001b[0mw2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                                                     w3:W3[score]})\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mW1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw1_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mW2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 1\n",
    "scores_incremental = dict()\n",
    "learning_curves = dict()\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"Trial:\\t{}\".format(i+1))\n",
    "    scores_incremental[i] = dict()\n",
    "    learning_curves[i] = dict()\n",
    "    k = 0\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=i)\n",
    "    for train_index, test_index in skf.split(df[\"Review\"], df[\"Language\"]):\n",
    "        start = time.time()\n",
    "        # approximately 100 epochs per minute\n",
    "        W1, W2, W3, training_curve = train_selective(df.loc[train_index], epochs=5000)\n",
    "        \n",
    "        preds, true = predict_selective(df.loc[test_index], W1, W2, W3)\n",
    "        s = distance_accuracy(true, preds)\n",
    "        scores_incremental[i][k] = s\n",
    "        learning_curves[i][k] = training_curve\n",
    "        k += 1\n",
    "        print(\"K:\\t{}\\nScore:\\t{}\".format(k, s))\n",
    "        print(\"took:\", time.time()-start)\n",
    "        scores_incremental[i][k] = s\n",
    "        learning_curves[i][k] = training_curve\n",
    "    print(\"*\"*10)\n",
    "    try:\n",
    "        print(\"Trial {} avg score:\\t {}\".format(i+1, np.mean(list(scores_incremental[i].values()))))\n",
    "    except:\n",
    "        continue\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([scores_incremental, learning_curves], open(\"incremental_tf.results\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO CHEAT LIKE A PRO\n",
    "# \"\"\"\n",
    "# def test_selective(df_test, W1, W2, W3):\n",
    "#     reset_graph()\n",
    "#     x = tf.placeholder(tf.float32, [None, 300])\n",
    "#     y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "#     w1 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w2 = tf.placeholder(tf.float32, [300, 300])\n",
    "#     w3 = tf.placeholder(tf.float32, [300, 10])\n",
    "\n",
    "#     b1 = tf.Variable(tf.zeros([300]))\n",
    "#     b2 = tf.Variable(tf.zeros([300]))\n",
    "#     b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#     l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "#     l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "#     pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "    \n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "#     instance_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#         # Testing the model\n",
    "#         LSMR_test = preprocess_data(df_test)\n",
    "#         X_test, y_test = get_test(LSMR_test)\n",
    "#         accuracy = 0.\n",
    "#         for i in range(len(X_test)):\n",
    "#             best_instance_accuracy = float(\"-inf\")\n",
    "#             for language, score, movie_id in W1:\n",
    "#                 w_1 = W1[(language, score, movie_id)]\n",
    "#                 w_2 = W2[(language, score)]\n",
    "#                 w_3 = W3[score]\n",
    "#                 a = instance_accuracy.eval({x: np.atleast_2d(X_test[i]), y: np.atleast_2d(y_test[i]),\n",
    "#                                    w1:w_1,\n",
    "#                                    w2:w_2,\n",
    "#                                    w3:w_3})\n",
    "#                 if a > best_instance_accuracy:\n",
    "#                     best_instance_accuracy = a\n",
    "#             accuracy += best_instance_accuracy\n",
    "\n",
    "#     return accuracy/len(X_test)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = pd.DataFrame(list(scores))\n",
    "# stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs a gpu\n",
    "# def train(df_train, epochs=100, learning_rate=0.1, random_state=42):\n",
    "#     LSMR_train = preprocess_data(df_train)\n",
    "#     np.random.seed(random_state)\n",
    "#     data_dict, L1, L2, L3 = get_data_dict(LSMR_train, get_L2and3=True)\n",
    "#     init_weights = lambda layer, i, o: {k:2*np.random.random((i, o))-1 for k in layer}\n",
    "#     W1 = init_weights(L1, 300, 300)  # (languge, score, movie_id)\n",
    "#     W2 = init_weights(L2, 300, 300)  # (languge, score):\n",
    "#     W3 = init_weights(L3, 300, 10)  # score:\n",
    "    \n",
    "    \n",
    "#     reset_graph()\n",
    "#     x = tf.placeholder(tf.float32, [None, 300])\n",
    "#     y = tf.placeholder(tf.float32, [None, 10]) # 1-10 => 10 classes\n",
    "\n",
    "#     w1 = tf.Variable(tf.zeros([300, 300]))\n",
    "#     w2 = tf.Variable(tf.zeros([300, 300]))\n",
    "#     w3 = tf.Variable(tf.zeros([300, 10]))\n",
    "\n",
    "#     b1 = tf.Variable(tf.zeros([300]))\n",
    "#     b2 = tf.Variable(tf.zeros([300]))\n",
    "#     b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#     l2 = tf.nn.sigmoid(tf.matmul(x, w1) + b1)\n",
    "#     l3 = tf.nn.sigmoid(tf.matmul(l2, w2) + b2)\n",
    "#     pred = tf.nn.softmax(tf.matmul(l3, w3) + b3)\n",
    "\n",
    "\n",
    "#     cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "#     training_curve = dict()\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         for e in range(epochs+1):\n",
    "#             start = time.time()\n",
    "#             avg_cost = 0.\n",
    "#             for _, row in LSMR_train.iterrows():\n",
    "#                 score = row[\"Score\"]\n",
    "#                 y_ = np.zeros(10)\n",
    "#                 y_[score-1] = 1\n",
    "#                 y_ = np.atleast_2d(y_)\n",
    "#                 x_ = np.atleast_2d(row[\"rev_vec\"])\n",
    "#                 w_1, w_2, w_3 , _, c = sess.run([w1, w2, w3, optimizer, cost], feed_dict={x: x_,y: y_})               \n",
    "#                 avg_cost += c\n",
    "#             avg_cost /= len(LSMR_train)\n",
    "#             training_curve[e] = (avg_cost, time.time()-start)\n",
    "#             if e%100==0:\n",
    "#                 print(\"Epoch {}: {}\".format(e, avg_cost))\n",
    "        \n",
    "#         return w_1, w_2, w_3, training_curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
